{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-NP1bmcZmub"
   },
   "source": [
    "# AM207 Final Project\n",
    "\n",
    "#### Team: ShiLe Wong, Xin Zeng (interested in research), Yujie Cai, Jiahui Tang\n",
    "\n",
    "#### Paper: Risk score learning for COVID-19 contact tracing apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook contents\n",
    "\n",
    "â›” PLEASE DO NOT RERUN THE EXPERIMENT PART (II.4) SINCE IT TAKES EXTREMELY LONG TIME\n",
    "\n",
    "If you need to return to the content page, kindly press `return to top` in each section.\n",
    "\n",
    "[I. Introduction](#I.-Introduction)\n",
    "\n",
    " - [Problem Statement](#Problem-Statement)\n",
    " \n",
    " - [Literature](#Literature)\n",
    " \n",
    "[II. Methods](#II.-Methods)\n",
    "\n",
    " - [1. High Level Technical Content](#1.-High-Level-Technical-Content)\n",
    " \n",
    " - [2. Detailed Technical Content](#2.-Detailed-Technical-Content)\n",
    " \n",
    "  - [2.1 Synthetic data generator](#2.1-Synthetic-data-generator)\n",
    "  \n",
    "  - [2.2 Machine learning technique used](#2.2-Machine-learning-technique-used)\n",
    "  \n",
    " - [3. Implementation](#3.-Implementation)\n",
    " \n",
    "  - [A. Simulation of Toy Dataset](#A.-Simulation-of-Toy-Dataset)\n",
    "  \n",
    "  - [B. Bagging Simulator](#B.-Bagging-Simulator)\n",
    "  \n",
    "  - [C. Training the Model](#C.-Training-the-Model)\n",
    "  \n",
    "       - [Analytical Gradient](#Analytical-Gradient)\n",
    "   \n",
    "       - [Training Code](#Training-Code)\n",
    "       \n",
    " - [4. Model and Experiments](#4.-Model-and-Experiments)\n",
    "  \n",
    "  - [1. Varying maximum bag size](#1.-Varying-maximum-bag-size)\n",
    "   \n",
    "  - [2. Varying censoring probability](#2.-Varying-censoring-probability)\n",
    "  \n",
    "  - [3. Varying Positive Bag Construction](#3.-Varying-Positive-Bag-Construction)\n",
    "  \n",
    "  - [4. Robustness to Model Mismatch](#4.-Robustness-to-Model-Mismatch)\n",
    "  \n",
    "  - [5. Reordering of data](#5.-Reordering-of-data)\n",
    "  \n",
    "  - [6. Examples of failure mode](#6.-Examples-of-failure-mode)\n",
    "  \n",
    "[III. Discussion](#III.-Discussion)\n",
    "\n",
    " - [1. Evaluation](#1.-Evaluation)\n",
    " \n",
    " - [2. Future Work and Potential Modifications](#2.-Future-Work-and-Potential-Modifications)\n",
    " \n",
    " - [3. Broader Impact](#3.-Broader-Impact)\n",
    " \n",
    "[References](#References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "050JVE1isEoA"
   },
   "source": [
    "# I. Introduction\n",
    "\n",
    "* Problem statement - what is the problem the paper aims to solve?\n",
    "* Context/scope - why is this problem important or interesting?\n",
    "* Existing work - what has been done in literature?\n",
    "* Contribution - what is gap in literature that the paper is trying to fill? What is the unique contribution\n",
    "\n",
    "\n",
    "### **Problem Statement**\n",
    "\n",
    "During the COVID-19 Pandemic, contact tracing came to the forefront of digital tools to help combat the spread of the virus. Part of the work in contact tracing is to accurately determine the risk of infection of an individual given their exposure to other individuals who have tested positive, so as to alert the individual should the risk be significant. This paper aims to solve the problem of accurately determining the probability of infection based on features of interpersonal interactions (e.g. duration and distance of interaction) and the contagiousness of the infected case.\n",
    "\n",
    "This is particularly important since contact tracing is a key tool used by many places to contain the spread of the virus. When an individual is alerted of their risk of infection in a timely manner, they can be advised to isolate and thereby reduce the chances of them spreading the virus to other people should they be infected. Hence, accurately determining the risk of infection and using the information to advise individuals of relevant protocols can determine the efficacy of contact tracing as a tool, and more seriously, the extent of virus and pandemic progression.\n",
    "\n",
    "### **Literature**\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "The paper references a few examples of work that has been done in contact tracing, and specifically in using data to train models for risk scoring. First, the paper centers on the risk score model for the Google/Apple Exposure Notification (GAEN) system [1] for contact tracing, which has predefined relationships of specified parameters that come from data which can be collected through mobile applications. The associated parameters are the main focus of the machine learning optimization problem covered in the paper, and that we hope to recreate.\n",
    "\n",
    "Second, the paper highlights that many current risk score models or contact tracing apps assume a known model with fixed parameters. There is a publicly available resource put together by domain experts that suggest values for the parameters. [2] A couple of other papers cited also evaluate the effectiveness of contact tracing apps that assume a fixed, known model. [3,4] The authors find this landscape to present a gap in that these models are not dynamic or flexible enough to adjust to or reflect what the data might tell us. Having fixed parameters beforehand without accounting for what is happening in real life (as seen from data), even if they were determined through expert guidance, may still be biased or inaccurate. Hence, they suggest that their own model of training to fit data could perform better at estimating an individual's risk of infection, and compare their results to that of a Swiss model that uses fixed thresholds. [5]\n",
    "\n",
    "Finally, they acknowledge one paper that attempts to use data to train a model that optimizes for the risk parameters. [6] However, there were two gaps: (1) this earlier paper did not account for the infectiousness of the index case; and (2) this earlier paper performed supervised learning. For the first gap, by not accounting for the infectiousness, the model is potentially missing out on additional useful data that could inform the risk score, since the infectiousness of a patient you come across could greatly affect whether or not you would catch the virus from them. Moreover, the infectiousness is data that is captured by the GAEN app, so having a model that can utilize all the relevant data points collected by the app can maximize its efficacy and allow the model to be deployed directly. The second gap relates to the real-life applicability of the model. This current paper suggests that they have a better solution since their model takes into account imperfect information, for example, by not having complete infection labels for every single exposure, but an overall label for a bag of exposures that is prone to false negatives because of censoring. This is also potentially more realistic in a real-life situation where information collection is likely imperfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr9RqWS7MeCn"
   },
   "source": [
    "# II. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J3u07NPMpmx"
   },
   "source": [
    "## 1. High Level Technical Content \n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "Due to privacy issues, the authors of the paper could not access real data collected by the covid tracing app. Therefore, the first contribution they made was to propose a mechanism to generate synthetic data, which we will provide detailed documentation of in II.2. Since the focus of our project is on understanding and testing their model for learning risk score parameters, we have decided to use the same synthetic data generator to replicate and test the experiments. \n",
    "\n",
    "Second, with a dataset in hand, the authors performed several comparative experiments to prove that their algorithm is effective in estimating parameters of risk score models. They do this by doing a semi-supervised machine learning, where observations are clustered or \"bagged\" and their labels are aggregated (and in some cases, partially censored). The observation features are then fed into a machine learning model for the model to learn to predict risk scores that will maximize the log-likelihood of the corresponding bag labels. Producing risk score estimations involves optimizing for parameters such as attenuation thresholds and weights and contagiousness weights, which was performed using the `jax` package implementation of stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST0dsud-ta6k"
   },
   "source": [
    "## 2. Detailed Technical Content\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "### 2.1 Synthetic data generator:\n",
    "\n",
    "\n",
    "Although data generation is not the main objective of this project, it is worth pointing out that the authors have done a tremendous job putting together a sensible physical model using scientific literature, and coming up with approximation tools to simulate the real life scenario, i.e. the transmission mechanism of COVID-19. We will elaborate on how the transmission is modelled in the physical world and analogous features collected on the mobile application, since this is what defines the generative model and is thus used to inform the eventual optimization.\n",
    "\n",
    "A bottom-up approach can be used to describe the transmission mechanism. That is, we first figure out the risk of infection from a single exposure, then we combine it with the social network effect to determine how aggregated exposures may have an effect on a person's overall infection outcome.\n",
    "\n",
    "#### **2.1.1 In the real world: Biological Model of Risk Scoring**\n",
    "\n",
    "\n",
    "Suppose a person is exposed to another person who has been infected. The hazard score of that event is defined to be a function of three arguments: duration of exposure $\\tau_n$, the distance of exposure $d_n$, and the time of sympton onset since the exposure $\\sigma_n$ (of the infected person), which is a proxy for the infectiousness of the index case. Namely, \n",
    "\n",
    "$$s_n = f(\\tau_n, d_n, \\sigma_n; \\phi) = \\tau_n \\times f_{dist}(d_n; \\phi) \\times f_{inf}(\\sigma_n; \\phi) \\quad \\quad (1)$$\n",
    "\n",
    "where $\\phi$ are parameters of the simulator. Notice the exact functional form of $f_{dist}$ and $f_{inf}$ follow fluid physics law as well as empirical conventions[1,2], which we choose not to write in details here given this is not our core implementation and innovation. \n",
    "\n",
    "\n",
    "#### **2.1.2 On a mobile application: Translating the biological model to a computational model**\n",
    "\n",
    "> ##### **From Hazard Score to Risk Score**\n",
    "\n",
    "Since all training features (essentially the model inputs) will be recorded by a mobile device, the data generator is modelled after the device used for recording. For this paper, the authors decide to follow the GAEN system as mentioned above in the Introduction. Specifically, the distance function $f_{dist}$ is approximated by bluetooth attentuation $f_{ble}$, and the infectiousness calculation function $f_{inf}$ is approximated by a quantized version of symptom onset time $f_{con}$. Rewriting equation (1) from above, we get the risk score model:\n",
    "\n",
    "$$r_n = f(\\tau_n, a_n, c_n; \\psi) = \\tau_n \\times f_{ble}(a_n; \\psi) \\times f_{con}(c_n; \\psi)  \\quad \\quad (2)$$\n",
    "\n",
    "where $\\psi$ are parameters of the risk score model. The inputs to the model that are proxied by digital features are elaborated on below.\n",
    "\n",
    "> ##### **Distance as proxied by Bluetooth Attenuation**\n",
    "\n",
    "The bluetooth attenuation function assigns different weights to different attenuation thresholds, and all the weights and thresholds are parameters to optimize for. To put it simply, the farther apart the devices, the less weight the attenuation function assigns. The actual math shows a relationship that maps bluetooth attenuation ($a_n$) to 4 categories defined by 3 attentuation threshold angles, $\\theta_b^\\text{ble}$ (distance), with each category having an output of one weight. \n",
    "\n",
    "\\begin{equation*}\n",
    "f_{\\text{ble}}(a_n; \\psi)=\\begin{cases}\n",
    "          w_1^\\text{ble} \\quad &\\text{if} \\, a_n \\leq \\theta_1^\\text{ble} \\\\\n",
    "          w_2^\\text{ble} \\quad &\\text{if} \\, \\theta_1^\\text{ble} < a_n \\leq \\theta_2^\\text{ble} \\\\\n",
    "          w_3^\\text{ble} \\quad &\\text{if} \\, \\theta_2^\\text{ble} < a_n \\leq \\theta_3^\\text{ble} \\\\\n",
    "          w_4^\\text{ble} \\quad &\\text{if} \\, \\theta_3^\\text{ble} < a_n \\\\\n",
    "     \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Two modifications to this mapping function that is relevant for our work is (1) the assumption of monotocity of the weights; and (2) transforming this discretized categorization into a continuous function. These two modifications directly influence our mathematical work and interpretation.\n",
    "\n",
    "(1) Assumption of Monotonicity\n",
    "\n",
    "Biologically, it is reasonable to believe that the risk of infection only gets higher when the distance between an individual and infected case gets closer. Hence, to ensure this monotonic relationship is followed in the computational model, the authors opted to reparameterize the weights as follows:\n",
    "\n",
    "$$w^{\\text{ble}} = [w_1^{\\text{ble}}, w_2^{\\text{ble}} = w_1^{\\text{ble}} + \\Delta_2^{\\text{ble}}, w_3^{\\text{ble}} = w_2^{\\text{ble}} + \\Delta_3^{\\text{ble}}, w_4^{\\text{ble}} = w_3^{\\text{ble}} + \\Delta_4^{\\text{ble}}]$$\n",
    " \n",
    "Hence, we will optimize over $(w^{\\text{ble}}_1, \\Delta_2^{\\text{ble}},\\Delta_3^{\\text{ble}},\\Delta_4^{\\text{ble}})$ instead of $(w_1^{\\text{ble}}, w_2^{\\text{ble}}, w_3^{\\text{ble}}, w_4^{\\text{ble}})$. We will have a function that maps the weights to the parameters we optimize, and vice versa.\n",
    "\n",
    "(2) Discrete to continuous mapping\n",
    "\n",
    "The approach as represented by the function right now is known as hard binning. However, this means that the function is not continuous with respects to the attenuation thresholds. The motivation to ensure the function is continuous with respects to the thresholds is so that the function can be easily differentiated with respects to the thresholds and thus the gradient can be easily calculated too. Hence, the authors opted to replace this hard binning method with a soft thresholding approximation, by converting the discrete mapping to one that utilizes the sigmoid function, $\\sigma_\\tau$. In the equations below, $I \\left(\\theta^{\\text{ble}}_{b-1} < a_{nk} \\leq \\theta^{\\text{ble}}_{b}\\right)$ represents the output from the discrete function presented above.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\tau_{nb} &= \\sum_k \\tau_{nk} I \\left(\\theta^{\\text{ble}}_{b-1} < a_{nk} \\leq \\theta^{\\text{ble}}_{b}\\right)\\\\\n",
    "& \\approx \\sum_k \\tau_{nk} \\sigma_\\tau (a_{nk} - \\theta^{\\text{ble}}_{b-1}) \\sigma_\\tau(\\theta^{\\text{ble}}_{b}-a_{nk})\\\\\n",
    "& \\approx \\sum_k \\tau_{nk} \\left[\\frac{1}{1+\\exp(-t(a_{nk} - \\theta^{\\text{ble}}_{b-1}))}\\right] \\left[\\frac{1}{1+\\exp(-t(\\theta^{\\text{ble}}_{b}-a_{nk}))}\\right]\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "> ##### **Contagiousness of Index Case**\n",
    "\n",
    "The days since symptom onset is calculated by health authorities and directly mapped to 3 contagiousness levels: none ($c_n = 1$), standard ($c_n = 2$), and high ($c_n = 3$). The eventual contagiousness level is then used in the risk scoring instead of having the calculated days be directly accessible by Google/Apple in view of privacy concerns. Again, zero weight is assigned to the none level, and two non-zero weights are assigned to the other two levels. \n",
    "\n",
    "\\begin{equation*}\n",
    "f_{\\text{con}}(c_n; \\psi)=\\begin{cases}\n",
    "          0 \\quad &\\text{if} \\, c_n = 1 \\\\\n",
    "          w_2^{\\text{con}} \\quad &\\text{if} \\, c_n = 2 \\\\\n",
    "          w_3^{\\text{con}} \\quad &\\text{if} \\, c_n = 3 \\\\\n",
    "     \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Again, there is an assumption of monotonicity that is used to reparameterize the weights because there is a biological basis for believing that the risk of infection will only increase with increasing contagiousness.\n",
    "\n",
    "$$w^{\\text{con}} = [w_2^{\\text{con}}, w_2^{\\text{con}} + \\Delta^\\text{con}]$$\n",
    "\n",
    "We will optimize over $(w_2^{\\text{con}}, \\Delta^\\text{con})$ and similarly have a function that maps between these parameters and the weights in the function.\n",
    "\n",
    "> ##### **Mapping Risk Score to Probability of Infection**\n",
    "\n",
    "Finally, to map the risk score into probability space, which is constrained to the bounds $[0,1]$, the authors use the standard exponential does response model [3]. Below is the form that uses the risk score model (instead of the hazard score model), which takes into account the digital features collected to determine the estimated probability of infection. \n",
    "\n",
    "$$q_n = \\mathbb{P}(y_n = 1|\\tilde x_n; \\psi) = 1 - e^{-\\mu r_n}$$\n",
    "\n",
    "Here $\\tilde x_n = (\\tau_n, a_n, c_n)$, $y_n = 1$ (infected) or $0$ (not infected).\n",
    "\n",
    "#### **2.1.3 Multiple exposures**\n",
    "\n",
    "As an extension of the individual exposure modelling described above, the authors look at combining the risk from multiple exposures to reflect a single individual's experience, First, the authors recognize that different exposure events means different lengths of time spent in different distances to infected cases. This is represented by calculating the amount of time spent in each attenuation bucket (which is then transformed to the continuous sigmoid):\n",
    "\n",
    "$$\\underbrace{\\tau_{n b}}_{\\text{duration in bucket } b}= \\sum_{k=1}^{K_{n}} \\underbrace{\\tau_{n k}}_{\\text {duration}} \\times \\underbrace{\\mathbb{I}\\left(\\theta_{b-1}^{\\text {ble }}< a_{nk} \\leq \\theta_{b}^{\\text {ble}}\\right)}_{\\text{attenuation is in bucket } b}$$\n",
    "\n",
    "where $K_n$ is the number of exposures within the n'th exposure window. We then are able to compute the overall risk for the exposure:\n",
    "\n",
    "$$\\underbrace{r_{n}}_{\\text {risk score }}=\\underbrace{\\left[\\sum_{b=1}^{N_{B}} \\tau_{n b} w_{b}^{\\text {ble }}\\right]}_{\\text {weighted exposure minutes }} \\times \\underbrace{\\left[\\sum_{\\ell=1}^{N_{C}} \\mathbb{I}\\left(c_{n, \\ell}\\right) w_{\\ell}^{\\text {con }}\\right]}_{\\text {weighted contagiousness level }}$$\n",
    "\n",
    "If we have multiple exposures for a user, we sum the risk scores to get $R_j = \\sum_{n \\in E_j} r_n$, which then translates into an overall probability of infection:\n",
    "\n",
    "$$Q_j = 1 - e^{-\\mu \\cdot R_j}$$\n",
    "\n",
    "#### **2.1.4 Bagging of exposure observations**\n",
    "\n",
    "The individual exposure events are first randomly generated. Then, the exposure events are randomly grouped or bagged to represent the experience of an individual, and the probability of infection is calculated using the above aggregation. Each exposure event can affect multiple individuals, in a concept coined as the social network effect in the paper. However, this is not crucial to our calculations since it is taken care of in the bagging step of the model instead of in optimization. The probability of infection is then used to generate the ground truth label of whether a person is infected or not.\n",
    "\n",
    "Having generated the data, the authors attempt to simulate the situation on the other side, which is in receiving the data and optimizing a model for the parameters. The same model is assumed for optimization, and the bagging of exposure events as described in the section above adds complexity in terms of noise and the potential for \"censoring\", where determinative exposure events (when an exposure significantly contributes to a person's risk but is not recorded) is not captured.\n",
    "\n",
    "\n",
    "### 2.2 Machine learning technique used\n",
    "\n",
    " + Multi-instance learning, Stochastic Gradient Descent\n",
    "\n",
    "The paper is dealing with a multi-instance (MI) learning problem, which belongs to supervised learning. Where MI learning differs from the traditional scenario is in the nature of the learning examples. In MI learning each example is represented by a bag of feature vectors. Classification labels are only provided for entire bags, and the task is to learn a model that predicts the classification labels for unseen future bags. In our case, we have only one, binary label (a user gets infected or not) for a \"bag\" of exposure events. Potentially this type of problem gets more challenging to tackle as the bag getting larger or the feature in the bag is noisy, which correspond to more exposures and censoring recorded on the app. \n",
    "\n",
    "\n",
    "As mentioned above, we have ten parameters of the infectious risk model to learn. To recap, two weights parameters $(\\Delta_{2:3}^{\\text{con}})$ come from the contageousness measure; three threshold angles $(\\theta_{1:3}^{\\text{ble}})$ and four weights $(w_1^{\\text{ble}} \\text{and} \\Delta_{2:4}^{\\text{ble}})$ from bluetooth attentuation; and $\\mu$, a constant in the exponential model. The objective is to maximize the log-likelihood (or minimizing the binary cross entropy) of th e following fuction, where $Q_j$ incorporates these parameters:\n",
    "\n",
    "$$\\mathcal{L}(\\psi) = -\\sum_{j=1}^J Y_j \\log Q_j + (1-Y_j)\\log (1-Q_j)$$\n",
    "\n",
    "$Y_j \\in \\{0,1\\}$ is the infection label for user $j$. \n",
    "\n",
    "The exact techniques used by original authors are not complicated though, since relatively simple algorithm such as stochastic gradient descent (SGD) has proven the effectivness through their experiments. Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the optimal point of function. Adding stochastic component can let the gradient descent to start at random point for each iteration. Some biggest advantages are to reduce the computations enormously and to avoid the algorithm stuck at undesired optima. However, we need to be careful on choosing a proper learning rate, as SGD is sensitive to it. \n",
    "\n",
    "In the original experiment, the authors fit the model using 1000 iterations with batch size of 100. After the fitting, they choose AUC as metrics to discusses the performance with increasing problem complexity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGuLSvRkunJA"
   },
   "source": [
    "## 3. Implementation\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "There are 3 parts to the implementation: (A) Data Simulation and Generation, (B) Bagging Simulation, and (C) Learning Model Training. As mentioned, we have decided to use the original data simulation and generation code, as well as most of the bagging simulation, since the focus of our project is on testing the learning model. We have modified the original code to simplify processes where possible and to use simpler python functions as a way to test the robustness of the algorithm when reduced to less fancy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OBuQ3B7W9v0E"
   },
   "outputs": [],
   "source": [
    "# it is advisory to install this version of jax package, and to run the notebook on intel-chip computer\n",
    "# !pip install jax==0.2.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NFqb7otlpqW_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.stats import nbinom\n",
    "from functools import partial\n",
    "import scipy.stats\n",
    "from scipy.stats import bernoulli\n",
    "from dataclasses import dataclass, asdict\n",
    "import sklearn.metrics as metrics\n",
    "from textwrap import wrap\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFg70TF-pixZ"
   },
   "source": [
    "### A. Simulation of Toy Dataset\n",
    "\n",
    "The end goal of the simulation is to create a set of data points that have the features used to train the model (e.g. bluetooth attenuation, infectiousness, etc.), the labels for the data points (whether the exposure led to an infection) and the probability of infection, as calculated using the generative model and parameters. The code first includes a few helper functions for calculating different components of the risk score. Then we put it in a dictionary and calculate the probabilities and labels for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yljYIvF5obEC",
    "outputId": "0880e4a4-16ff-4603-f31a-c705b1ce4aef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making grid of 80 distances x 20 durations x 21 onsets = 33600 points\n",
      "Approx: [[[1.         1.         1.         ... 0.97538178 0.92937435 0.92675691]]\n",
      "\n",
      " [[0.49996063 0.49998181 0.46092688 ... 0.49969697 0.49750601 0.49731772]]\n",
      "\n",
      " [[0.67110338 0.66968263 0.81008083 ... 0.65435507 0.63129513 0.62997964]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.63518322 0.63414931 0.72087815 ... 0.62280329 0.60509304 0.60406036]]\n",
      "\n",
      " [[0.63539429 0.63435626 0.72199235 ... 0.62296994 0.60521186 0.60417686]]\n",
      "\n",
      " [[0.63536768 0.63433023 0.72181414 ... 0.62294962 0.60519806 0.60416337]]]\n",
      "Exact: [0.63537038 0.63433287 0.72183658 ... 0.62295162 0.60519936 0.60416464]\n",
      "(33600, 3) (33600,)\n"
     ]
    }
   ],
   "source": [
    "### Original Data Generating Function\n",
    "\n",
    "@dataclass\n",
    "class BleParams:\n",
    "  slope: float = 0.21\n",
    "  intercept: float = 3.92\n",
    "  sigma: float = np.sqrt(0.33)\n",
    "  tx: float = 0.0\n",
    "  correction: float = 2.398\n",
    "  name: str = 'briers-lognormal'\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "  ble_params: BleParams = BleParams()  # may want to change sigma\n",
    "  distance_fun: str = 'quadratic'  # quadratic  or sigmoid\n",
    "  distance_Dmin: float = 1.0\n",
    "  distance_slope: float = 2.0\n",
    "  distance_inflection: float = 2.0\n",
    "  infectiousness_fun: str = 'skew-logistic'  # gaussian or skew-logistic\n",
    "  beta: float = 1e-1  # transmission rate (the simulation Colab uses 1e-3)\n",
    "\n",
    "@dataclass\n",
    "class RiskConfig:\n",
    "    ble_thresholds: jnp.array  = jnp.array([])\n",
    "    ble_weights: jnp.array  = jnp.array([])\n",
    "    inf_levels: jnp.array  = jnp.array([])\n",
    "    inf_weights: jnp.array  = jnp.array([])\n",
    "    name: str = ''\n",
    "    beta: float = 3.1 * 1e-6  # Wilson table 1\n",
    "\n",
    "\n",
    "def incubation_dist(t):\n",
    "  mu = 1.621\n",
    "  sig = 0.418\n",
    "  rv = scipy.stats.lognorm(sig, scale=np.exp(mu))\n",
    "  return rv.pdf(t)\n",
    "\n",
    "# Symptom days to infectiousness\n",
    "def skew_logistic_scaled(x, alpha, mu, sigma):\n",
    "  return scipy.stats.genlogistic.pdf(x, alpha, loc=mu, scale=sigma)\n",
    "\n",
    "def ptost_conditional(ts, incubation):\n",
    "  mu = -4\n",
    "  sigma = 1.85\n",
    "  alpha = 5.85\n",
    "  tau = 5.42\n",
    "  fpos = skew_logistic_scaled(ts, alpha, mu, sigma)\n",
    "  fneg = skew_logistic_scaled(ts*tau/incubation, alpha, mu, sigma)\n",
    "  ps = fpos\n",
    "  neg = jnp.where(ts < 0)\n",
    "  ps[neg] = fneg[neg]\n",
    "  ps = ps/np.max(ps)\n",
    "  return ps\n",
    "\n",
    "def ptost_uncond(tost_times):\n",
    "  incub_times = np.arange(1, 14, 1)\n",
    "  incub_probs = incubation_dist(incub_times) \n",
    "  tost_probs = np.zeros_like(tost_times, dtype=float)\n",
    "  for k, incub in enumerate(incub_times):\n",
    "    ps = ptost_conditional(tost_times, incub)\n",
    "    tost_probs += incub_probs[k] * ps\n",
    "  return tost_probs\n",
    "\n",
    "infectiousness_curve_times = np.arange(-14, 14+1, 0.1)\n",
    "infectiousness_curve_vals = ptost_uncond(infectiousness_curve_times)\n",
    "\n",
    "def infectiousness_skew_logistic(delta):\n",
    "  return np.interp(delta, infectiousness_curve_times, infectiousness_curve_vals)\n",
    "\n",
    "# Distance to dose\n",
    "def transmission_vs_distance_quadratic(d, Dmin=1):  \n",
    "  m = np.power(Dmin,2)/np.power(d, 2)\n",
    "  return np.minimum(1, m)\n",
    "\n",
    "# Generating input data grid\n",
    "def uniform_input_data_grid(max_dur = 60, max_dist = 5, ngrid_dist = 20, ngrid_dur=20, min_dist=0.1, min_dur=5):\n",
    "  distances = np.linspace(min_dist, max_dist, ngrid_dist) # meters\n",
    "  durations = np.linspace(min_dur, max_dur, ngrid_dur) # minutes\n",
    "  symptoms = np.arange(-10, 10+0.001, dtype=int) # must be int\n",
    "  return distances, durations, symptoms\n",
    "\n",
    "# Generating input data and parameters\n",
    "def make_input_data(sigma=0.1, nsamples=20, distances=None, durations=None, symptoms=None):\n",
    "  if distances is None:\n",
    "    distances, durations, symptoms = uniform_input_data_grid()\n",
    "  ble_params = BleParams()\n",
    "  attens = dist_to_atten(distances, ble_params)\n",
    "   \n",
    "  # Make 3d cross product of the three 1d inputs\n",
    "  vals = itertools.product(distances, durations, symptoms)\n",
    "  X = np.vstack([np.array(v) for v in vals])\n",
    "  distance_grid = X[:,0]\n",
    "  duration_grid = X[:,1]\n",
    "  symptom_grid = np.array(X[:,2], dtype=int)\n",
    "  n = len(distance_grid)\n",
    "  print('Making grid of {} distances x {} durations x {} onsets = {} points'.format(\n",
    "      len(distances), len(durations), len(symptoms), n))\n",
    "\n",
    "  # noise-free atteniations  \n",
    "  atten_grid = dist_to_atten(distance_grid, ble_params) \n",
    "\n",
    "  # noisy samples\n",
    "  atten_grid_samples = []\n",
    "  for n in range(nsamples):\n",
    "    sample = dist_to_atten_sample(distance_grid, ble_params, sigma)\n",
    "    atten_grid_samples.append(sample)\n",
    "\n",
    "  \n",
    "  # Make 2d matrix for surface plotting functions (for fixed duration)\n",
    "  grid_2d_matrix = np.meshgrid(symptoms, attens)\n",
    "  symptom_grid_2d_matrix, atten_grid_2d_matrix  = grid_2d_matrix\n",
    "  vals = [z for z in zip(*(x.flat for x in grid_2d_matrix))]\n",
    "  X = np.vstack([np.array(v) for v in vals])\n",
    "  symptom_grid_2d = np.array(X[:,0], dtype=int)\n",
    "  atten_grid_2d = X[:,1]\n",
    "\n",
    "  data = {\n",
    "          'distance_grid': distance_grid, 'atten_grid': atten_grid, \n",
    "          'duration_grid': duration_grid, 'symptom_grid': symptom_grid,\n",
    "          'symptom_grid_2d_matrix': symptom_grid_2d_matrix,\n",
    "          'atten_grid_2d_matrix': atten_grid_2d_matrix,\n",
    "          'symptom_grid_2d': symptom_grid_2d,\n",
    "          'atten_grid_2d': atten_grid_2d,\n",
    "          'atten_grid_samples': atten_grid_samples,\n",
    "          'noise_level': sigma, 'nsamples': nsamples}\n",
    "  return data\n",
    "\n",
    "\n",
    "# Transmission model\n",
    "def hazard_fun_batch(attenuations, durations, symptom_days, params, distances=None):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params = ModelParams() object.\n",
    "  \"\"\"\n",
    "  if distances is None:\n",
    "    distances = atten_to_dist(attenuations, params.ble_params)\n",
    "  if params.distance_fun == 'quadratic':\n",
    "    fd = transmission_vs_distance_quadratic(distances, params.distance_Dmin)\n",
    "  elif params.distance_fun == 'sigmoid':\n",
    "    fd = transmission_vs_distance_sigmoid(distances, params.distance_slope, params.distance_inflection)\n",
    "  elif params.distance_fun == 'spline':\n",
    "    fd = transmission_vs_distance_spline(distances)\n",
    "  if params.infectiousness_fun == 'gaussian':\n",
    "    finf = infectiousness_gaussian(symptom_days) \n",
    "  elif params.infectiousness_fun == 'skew-logistic':\n",
    "    finf = infectiousness_skew_logistic(symptom_days)\n",
    "  doses  = durations * fd * finf \n",
    "  return doses\n",
    "\n",
    "# calculate probability of infection, with option to do an approx with Taylor series\n",
    "def prob_infection_batch(attenuations, durations, symptom_days, params, approx='taylor', exp_taylor_terms=np.inf, temp=1, distances=None):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params = ModelParams() object.\n",
    "  \"\"\"\n",
    "  doses = hazard_fun_batch(attenuations, durations, symptom_days, params, distances)\n",
    "  prob_infect_exact = 1-np.exp(-params.beta * doses)\n",
    "  if not (isinstance(temp, list) or isinstance(temp, np.ndarray)):\n",
    "    temp = list(temp)\n",
    "\n",
    "  if approx == 'none' or (approx == 'taylor' and exp_taylor_terms == np.inf) or (approx == 'tempered' and len(temp)==1 and temp[0] == 1):\n",
    "    prob_infect_approx = prob_infect_exact\n",
    "  elif approx == 'taylor' and exp_taylor_terms < np.inf:    \n",
    "    prob_infect_approx = []\n",
    "    x = -params.beta * doses\n",
    "    prob_infect = np.zeros(len(doses))\n",
    "    for k in range(1,exp_taylor_terms+1):\n",
    "      prob_infect -= x**k / math.factorial(k)\n",
    "      prob_infect_approx.append(np.minimum(np.maximum(prob_infect, 0), 1))\n",
    "  elif approx == 'tempered':\n",
    "    prob_infect_approx = []\n",
    "    x = -params.beta * doses\n",
    "    for t in temp:\n",
    "      prob_infect = 1 - np.power(np.maximum(1 + (1-t)*x,0), 1/(1-t))\n",
    "      prob_infect_approx.append(prob_infect)\n",
    "  else:\n",
    "    raise ValueError(\"Unknnown approximation type: {}\".format(approx))\n",
    "  return prob_infect_exact, prob_infect_approx\n",
    "\n",
    "\n",
    "def prob_infection_grid(data, params, approx='none', exp_taylor_terms=np.inf, temp=1):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    data = dictionary obtained from make_input_data().\n",
    "    params = ModelParams() object.\n",
    "    approx: Approximation for the exponential function for simulating model mismatch (none/taylor/tempered)\n",
    "    exp_taylor_terms: Number of approximation terms to use for Taylor approximation\n",
    "    temp: Array of temperature parameters for 'tempered' approximation \n",
    "  \"\"\"\n",
    "  ps = prob_infection_batch(data['atten_grid'], data['duration_grid'], data['symptom_grid'], params, approx, exp_taylor_terms, temp)\n",
    "  return ps\n",
    "\n",
    "def risk_score_grid(data, config):\n",
    "  rs = risk_score_batch(data['atten_grid'], data['duration_grid'], data['symptom_grid'], config)\n",
    "  return rs\n",
    "\n",
    "def prob_risk_score_grid(data, config):\n",
    "  qs = prob_risk_score_batch(data['atten_grid'], data['duration_grid'], data['symptom_grid'], config)\n",
    "  return qs\n",
    "\n",
    "def risk_score_grid_sample(data, config, ndx):\n",
    "  rs = risk_score_batch(data['atten_grid_samples'][ndx], data['duration_grid'], data['symptom_grid'], config)\n",
    "  return rs\n",
    "\n",
    "def atten_to_rssi(atten, ble_params):\n",
    "  return ble_params.tx  - (atten + ble_params.correction)\n",
    "\n",
    "def rssi_to_atten(rssi, ble_params):\n",
    "  return ble_params.tx - (rssi + ble_params.correction)\n",
    "\n",
    "def atten_to_dist(atten, ble_params):\n",
    "  rssi = ble_params.tx  - (atten + ble_params.correction)\n",
    "  return np.exp((np.log(-rssi) - ble_params.intercept)/ble_params.slope)\n",
    "\n",
    "def dist_to_atten(distance, ble_params):\n",
    "  mu = ble_params.intercept + ble_params.slope * np.log(distance)\n",
    "  rssi = -np.exp(mu)\n",
    "  atten = ble_params.tx  - (rssi + ble_params.correction)\n",
    "  return atten\n",
    "\n",
    "# This will be used in the future to add noise\n",
    "def dist_to_atten_sample(distances, ble_params, sigma):\n",
    "  # if ble_params.sigma == 0:\n",
    "  if sigma == 0:\n",
    "    return dist_to_atten(distances, ble_params)\n",
    "  N = len(distances)\n",
    "  mus = ble_params.intercept + ble_params.slope * np.log(distances)\n",
    "  # sigs = ble_params.sigma\n",
    "  sigs = sigma\n",
    "  rssi = -scipy.stats.lognorm(s=sigs, scale=np.exp(mus)).rvs()\n",
    "  atten = ble_params.tx  - (rssi + ble_params.correction)\n",
    "  return atten\n",
    "\n",
    "def make_infectiousness_params_v2():\n",
    "  # Derived from Arizona by averaging some bins\n",
    "  inf_pre = np.zeros((9), dtype=int)\n",
    "  inf_post = np.zeros((5), dtype=int)\n",
    "  inf_mid6 = np.array([1, 3, 4, 5, 6, 6, 6, 6, 5, 4, 3, 2, 2, 1, 1])\n",
    "  inf_mid = np.ones_like(inf_mid6)\n",
    "  ndx = (inf_mid6 >= 5)\n",
    "  inf_mid[ndx] = 2\n",
    "  #inf_mid = np.array([1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1])\n",
    "  inf_levels = np.concatenate((inf_pre, inf_mid, inf_post))\n",
    "  inf_weights = np.array([0,  10**1.6, 10**2])/100\n",
    "  return inf_levels, inf_weights\n",
    "\n",
    "def days_to_inf_levels(symptom_days):\n",
    "  symptom_days = np.atleast_1d(symptom_days)  # turn into array\n",
    "  inf_levels, _ = make_infectiousness_params_v2()\n",
    "  return inf_levels[symptom_days + 14]\n",
    "\n",
    "# initializing input data\n",
    "distances, durations, symptoms = uniform_input_data_grid(max_dur = 60,  max_dist = 5, ngrid_dist = 80, ngrid_dur=20, min_dist=0.1, min_dur=5)\n",
    "data = make_input_data(distances=distances, durations=durations, symptoms=symptoms)\n",
    "\n",
    "# initializing parameters\n",
    "ble_params = BleParams()\n",
    "rssi = atten_to_rssi(data['atten_grid'], ble_params)\n",
    "duration = data['duration_grid']\n",
    "symptom_day = data['symptom_grid']\n",
    "infectiousness = days_to_inf_levels(symptom_day)\n",
    "\n",
    "params = ModelParams()\n",
    "\n",
    "# Create model inputs\n",
    "X = np.concatenate([rssi[:,None], duration[:,None], infectiousness[:,None]], axis=1)\n",
    "approx_type = 'taylor'\n",
    "exp_taylor_terms = 8\n",
    "exp_temperature = np.arange(0.5,1.,0.09)\n",
    "\n",
    "# calculating probability of infection\n",
    "prob_infect_exact, prob_infect_approx = prob_infection_grid(data, params, approx=approx_type, exp_taylor_terms=exp_taylor_terms, temp=exp_temperature)\n",
    "prob_infect_approx = np.array(prob_infect_approx)\n",
    "# manually inspect the approximation error (higher error for larger values -- away from 0)\n",
    "ind = np.where(prob_infect_exact>0.6)\n",
    "print('Approx:', prob_infect_approx[:,ind])\n",
    "print('Exact:', prob_infect_exact[ind])\n",
    "\n",
    "\n",
    "print (X.shape, prob_infect_exact.shape)\n",
    "\n",
    "\n",
    "simdata = dict()\n",
    "simdata['X'] = X\n",
    "simdata['prob_infect_exact'] = prob_infect_exact\n",
    "simdata['prob_infect_approx'] = prob_infect_approx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mwvQvVksPIm",
    "outputId": "36424a92-67c4-450b-8e96-e29a52f1c1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n"
     ]
    }
   ],
   "source": [
    "X_epi = simdata['X']  \n",
    "probabilities_true_epi = simdata['prob_infect_exact']\n",
    "probabilities_true_epi_approx = simdata['prob_infect_approx'].T  # N x num_approx_steps\n",
    "N = len(probabilities_true_epi)\n",
    "perm = np.random.permutation(N)\n",
    "X_epi = X_epi[perm, :]\n",
    "probabilities_true_epi = probabilities_true_epi[perm]\n",
    "probabilities_true_epi_approx = np.clip(probabilities_true_epi_approx[perm,:], 1e-5, 1-1e-5)\n",
    "  \n",
    "# generating labels for the exposure events\n",
    "def sample_labels(probabilities):\n",
    "  Y = bernoulli.rvs(probabilities)\n",
    "  N_pos = np.sum(Y)\n",
    "  N_neg = np.sum(1-Y)\n",
    "  print(\"total: {}, positives: {}, negatives: {}\".format(N, N_pos, N_neg))\n",
    "  pos_neg_ratio= float(N_neg) / N_pos\n",
    "  return Y, N_pos, N_neg, pos_neg_ratio\n",
    "\n",
    "Y_epi, N_pos, N_neg, pos_neg_ratio = sample_labels(probabilities_true_epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQfNg1QVsSVj",
    "outputId": "74beb596-ec18-4836-f22e-2bd5c7c24b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[-31.07666234,   5.        ,   0.        ],\n",
       "        [-31.07666234,   5.        ,   0.        ],\n",
       "        [-31.07666234,   5.        ,   0.        ],\n",
       "        ...,\n",
       "        [-70.66723028,  60.        ,   1.        ],\n",
       "        [-70.66723028,  60.        ,   1.        ],\n",
       "        [-70.66723028,  60.        ,   0.        ]]),\n",
       " 'prob_infect_exact': array([0.00123577, 0.00276287, 0.00595197, ..., 0.00616533, 0.00361119,\n",
       "        0.0021102 ]),\n",
       " 'prob_infect_approx': array([[0.00123653, 0.00276669, 0.00596975, ..., 0.00618441, 0.00361772,\n",
       "         0.00211243],\n",
       "        [0.00123577, 0.00276287, 0.00595193, ..., 0.00616529, 0.00361118,\n",
       "         0.0021102 ],\n",
       "        [0.00123577, 0.00276287, 0.00595197, ..., 0.00616533, 0.00361119,\n",
       "         0.0021102 ],\n",
       "        ...,\n",
       "        [0.00123577, 0.00276287, 0.00595197, ..., 0.00616533, 0.00361119,\n",
       "         0.0021102 ],\n",
       "        [0.00123577, 0.00276287, 0.00595197, ..., 0.00616533, 0.00361119,\n",
       "         0.0021102 ],\n",
       "        [0.00123577, 0.00276287, 0.00595197, ..., 0.00616533, 0.00361119,\n",
       "         0.0021102 ]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voh9Y9kxsaL8"
   },
   "source": [
    "### B. Bagging Simulator\n",
    "\n",
    "[Return to top](#Notebook-contents) \n",
    "\n",
    "Next, we need to bag the exposure events to tag them to an individual's aggregated risk of infection. The bagging simulator takes in parameters to determine the bag size, and various probabilities determining the makeup of the bag (e.g. number of positive and negative exposures, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YuCKLQrcsVG5"
   },
   "outputs": [],
   "source": [
    "class Bag_Simulator():\n",
    "    def __init__(self, p_pos, r_pos, p_neg, r_neg, max_bag_size, censor_prob_pos, censor_prob_neg, max_pos_in_bag=3):\n",
    "        self.p_pos = p_pos\n",
    "        self.r_pos = r_pos\n",
    "        self.p_neg = p_neg\n",
    "        self.r_neg = r_neg\n",
    "        self.max_bag_size = max_bag_size\n",
    "        self.censor_prob_pos = censor_prob_pos\n",
    "        self.censor_prob_neg = censor_prob_neg\n",
    "        self.max_pos_in_bag = max_pos_in_bag\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pos_neg_bag_sizes(bag_sizes, pos_bag_size_probs, neg_bag_size_probs, N_pos_Y, N_neg_Y,\n",
    "                              max_pos_in_bag=1, N_pos_bags=None):\n",
    "\n",
    "        if max_pos_in_bag > max(bag_sizes):\n",
    "            raise ValueError(\"max_pos_in_bag needs to be less than or equal to bag_sizes\")\n",
    "        if N_pos_bags is None:\n",
    "            raise ValueError(\"Please give an input for N_pos_bags\")\n",
    "        # start from N_pos_bag, each time decrease by 10, until we get the required number of\n",
    "        # negative and positive bag, and return the data simulation result\n",
    "        for N_pos_bags_temp in range(N_pos_bags, 0, -10):\n",
    "            pos_bag_size_samples = np.random.choice(bag_sizes, size=N_pos_bags_temp, replace=True, p=pos_bag_size_probs)\n",
    "            # sample uniformly in order to prepare for the implementation -\n",
    "            # <each positive bag contains N positive exposures>\n",
    "            pos_samples_per_bag = np.random.randint(low=1, high=max_pos_in_bag + 1, size=N_pos_bags_temp)\n",
    "            N_pos_required = np.sum(pos_samples_per_bag)\n",
    "            N_neg_bags = int(N_pos_bags_temp * N_neg_Y / N_pos_Y)\n",
    "            neg_bag_size_samples = np.random.choice(bag_sizes, size=N_neg_bags, replace=True, p=neg_bag_size_probs)\n",
    "            N_neg_required = np.sum(pos_bag_size_samples - pos_samples_per_bag) + np.sum(neg_bag_size_samples)\n",
    "            if N_neg_required < N_neg_Y and N_pos_required < N_pos_Y:\n",
    "\n",
    "                return pos_bag_size_samples, neg_bag_size_samples, pos_samples_per_bag, N_pos_bags_temp, \\\n",
    "                       int(N_pos_bags_temp * pos_neg_ratio)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_neg_bino_norm(k, n, p):\n",
    "        temp_pmf = nbinom.pmf(k, n, p)\n",
    "\n",
    "        return temp_pmf / np.sum(temp_pmf)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split_func(mat, labels, N_pos_bags, N_neg_bags, train_ratio=0.8, order=True):\n",
    "        N_bags = N_pos_bags + N_neg_bags\n",
    "        N_pos_trn = int(N_pos_bags * train_ratio)\n",
    "        N_neg_trn = int(N_neg_bags * train_ratio)\n",
    "        if order:\n",
    "            # if order = True, we use the same data splitting function as the original codebase\n",
    "            # it will keep the first 80% as training data and keep the rest 20% as test data\n",
    "            ind_trn = list(np.arange(N_pos_trn)) + list(np.arange(N_pos_bags, N_pos_bags + N_neg_trn))\n",
    "            ind_tst = list(np.arange(N_pos_trn, N_pos_bags)) + list(np.arange(N_pos_bags + N_neg_trn, N_bags))\n",
    "            np.random.shuffle(ind_trn)\n",
    "            np.random.shuffle(ind_tst)\n",
    "            assign_mat_trn, assign_mat_tst = mat[ind_trn], mat[ind_tst]\n",
    "            bag_labels_trn, bag_labels_tst = labels[ind_trn], labels[ind_tst]\n",
    "        else:\n",
    "            # if order = False, we use our own data splitting function, which would not keep the order\n",
    "            # of train and test data splitting\n",
    "            assign_mat_trn_part_1, assign_mat_tst_part_1, bag_labels_trn_part_1, bag_labels_tst_part_1\\\n",
    "                = train_test_split(mat[:N_pos_bags], labels[:N_pos_bags], train_size=train_ratio)\n",
    "            assign_mat_trn_part_2, assign_mat_tst_part_2, bag_labels_trn_part_2, bag_labels_tst_part_2 \\\n",
    "                = train_test_split(mat[N_pos_bags:], labels[N_pos_bags:], train_size=train_ratio)\n",
    "            assign_mat_trn = np.concatenate((assign_mat_trn_part_1, assign_mat_trn_part_2), axis=0)\n",
    "            assign_mat_tst = np.concatenate((assign_mat_tst_part_1, assign_mat_tst_part_2), axis=0)\n",
    "            bag_labels_trn = np.concatenate((bag_labels_trn_part_1, bag_labels_trn_part_2), axis=0)\n",
    "            bag_labels_tst = np.concatenate((bag_labels_tst_part_1, bag_labels_tst_part_2), axis=0)\n",
    "        # deal with the censoring issues - a user may get exposed to events that are not recorded by their phone\n",
    "        bag_labels_trn = bag_labels_trn[np.sum(assign_mat_trn, axis=1) > 0]\n",
    "        bag_labels_tst = bag_labels_tst[np.sum(assign_mat_tst, axis=1) > 0]\n",
    "        assign_mat_trn = assign_mat_trn[np.sum(assign_mat_trn, axis=1) > 0, :]\n",
    "        assign_mat_tst = assign_mat_tst[np.sum(assign_mat_tst, axis=1) > 0, :]\n",
    "\n",
    "        return assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst\n",
    "\n",
    "    def simulate_bagged_data(self, X, Y, probabilities, order=True, visualize=True):\n",
    "        # probabilities for bag sizes\n",
    "        # next assigned a random bag of size k of these events to each user.\n",
    "        # The value k is sampled from a truncated negative binomial distribution with parameters (p; r)\n",
    "        # where the truncation parameter is the maximum bag size b.\n",
    "        pos_bag_size_probs = self.get_neg_bino_norm(np.arange(self.max_bag_size), self.r_pos, 1-self.p_pos)\n",
    "        neg_bag_size_probs = self.get_neg_bino_norm(np.arange(self.max_bag_size), self.r_neg, 1-self.p_neg)\n",
    "\n",
    "        N_pos_Y = np.sum(Y)\n",
    "        N_neg_Y = Y.shape[0] - N_pos_Y\n",
    "        print(\"total: {}, positives: {}, negatives: {}\".format(N, N_pos_Y, N_neg_Y))\n",
    "        bag_sizes = np.arange(1, self.max_bag_size + 1)\n",
    "        pos_bag_size_samples, neg_bag_size_samples, pos_samples_per_bag, N_pos_bags, N_neg_bags = \\\n",
    "            self.get_pos_neg_bag_sizes(bag_sizes, pos_bag_size_probs, neg_bag_size_probs, N_pos_Y, N_neg_Y, \\\n",
    "                        max_pos_in_bag=min(self.max_bag_size, self.max_pos_in_bag), N_pos_bags=710)\n",
    "        N_pos_required = np.sum(pos_samples_per_bag)\n",
    "        N_neg_required = np.sum(pos_bag_size_samples - pos_samples_per_bag) + np.sum(neg_bag_size_samples)\n",
    "\n",
    "        # evaluate the correctness of N_neg_required and N_pos_required\n",
    "        if N_neg_required > N_neg_Y:\n",
    "            raise ValueError(\"the required number of negative bags should be less than the total number of \"\n",
    "                             \"negative bags\")\n",
    "        if N_pos_required > N_pos_Y:\n",
    "            raise ValueError(\"the required number of positive bags should be less than the total number of \"\n",
    "                             \"positive bags\")\n",
    "\n",
    "        # deal with X and probabilities array, only keep the required number of samples\n",
    "        valid_idx = list(np.where(np.array(Y == 1))[0][0:N_pos_required]) + \\\n",
    "                    list(np.where(np.array(Y == 0))[0][0:N_neg_required])\n",
    "        np.random.shuffle(valid_idx)\n",
    "        X_shuff, Y_shuff, probabilities_shuff = X[valid_idx, :], Y[valid_idx], probabilities[valid_idx]\n",
    "\n",
    "        N_bags = N_pos_bags + N_neg_bags\n",
    "        N_samples = np.sum(pos_bag_size_samples) + np.sum(neg_bag_size_samples)\n",
    "\n",
    "        # get the statistics of empirical bag sizes to better evaluate the accuracy of our data simulation\n",
    "        print('Empirical Bag sizes:')\n",
    "        print('\\t Positive bags: mean size {:.3f}, median size {:d}'.format(np.mean(pos_bag_size_samples),\n",
    "                                                                            int(np.median(pos_bag_size_samples))))\n",
    "        print('\\t Negative bags: mean size {:.3f}, median size {:d}'.format(np.mean(neg_bag_size_samples),\n",
    "                                                                            int(np.median(neg_bag_size_samples))))\n",
    "        ind_pos_exposures = np.where(np.array(Y_shuff == 1))[0]\n",
    "        ind_neg_exposures = np.where(np.array(Y_shuff == 0))[0]\n",
    "\n",
    "        bag_labels = np.concatenate((np.ones(N_pos_bags), np.zeros(N_neg_bags)))\n",
    "        assign_mat = np.zeros((N_bags, N_samples))\n",
    "        curr_ind_pos, curr_ind_neg = 0, 0\n",
    "        # We consider two scenarios for constructing positive bags: (i) each positive bag contains exactly one positive\n",
    "        # exposure and rest are negative exposures, (ii) each positive bag contains N positive exposures,\n",
    "        # where N is sampled uniformly from {1; 2; 3}\n",
    "        # We keep the original format of constructing the matrix in order to better experiment our modeling\n",
    "        for i in range(N_pos_bags):\n",
    "            ind_pos = ind_pos_exposures[curr_ind_pos:curr_ind_pos + pos_samples_per_bag[i]]\n",
    "            assign_mat[i, ind_pos] = 1 * bernoulli.rvs(1 - self.censor_prob_pos)\n",
    "            curr_ind_pos += pos_samples_per_bag[i]\n",
    "            if pos_bag_size_samples[i] > 1:\n",
    "                ind_neg = ind_neg_exposures[curr_ind_neg:curr_ind_neg + pos_bag_size_samples[i] - pos_samples_per_bag[i]]\n",
    "                assign_mat[i, ind_neg] = 1 * bernoulli.rvs(1 - self.censor_prob_neg)\n",
    "                curr_ind_neg += pos_bag_size_samples[i] - pos_samples_per_bag[i]\n",
    "        for i in range(N_neg_bags):\n",
    "            ind1 = i + N_pos_bags\n",
    "            ind2 = ind_neg_exposures[curr_ind_neg:curr_ind_neg + neg_bag_size_samples[i]]\n",
    "            assign_mat[ind1, ind2] = 1 * bernoulli.rvs(1 - self.censor_prob_neg)\n",
    "            curr_ind_neg += neg_bag_size_samples[i]\n",
    "\n",
    "        assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = \\\n",
    "            self.train_test_split_func(assign_mat, bag_labels, N_pos_bags, N_neg_bags, 0.8, order)\n",
    "\n",
    "        print('assign_mat size, X_shuff size:', assign_mat.shape, X_shuff.shape)\n",
    "        print('assign_mat_trn size, assign_mat_tst size', assign_mat_trn.shape, assign_mat_tst.shape)\n",
    "        print('Average positive samples per bag: {}'.format(np.mean(pos_samples_per_bag)))\n",
    "\n",
    "        if visualize:\n",
    "            self.visual_analysis(pos_bag_size_probs, neg_bag_size_probs)\n",
    "\n",
    "        return X_shuff, probabilities_shuff, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst\n",
    "\n",
    "    def visual_analysis(self, pos_bag_size_probs, neg_bag_size_probs):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].stem(np.arange(self.max_bag_size) + 1, pos_bag_size_probs, use_line_collection=True)\n",
    "        ax[0].set_title('Probability of positive bag sizes')\n",
    "\n",
    "        ax[1].stem(np.arange(self.max_bag_size) + 1, neg_bag_size_probs, use_line_collection=True)\n",
    "        ax[1].set_title('Probability of negative bag sizes')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "iHNN36j1sfig",
    "outputId": "dbfc561b-6a0b-451c-8671-87d68bfc9109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.820, median size 3\n",
      "\t Negative bags: mean size 4.030, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17707) (17707, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 17707) (851, 17707)\n",
      "Average positive samples per bag: 1.9704225352112676\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running a test to visualize the bagging simulator\n",
    "bag_sim = Bag_Simulator(p_pos=0.6, r_pos=2, p_neg=0.6, r_neg=2, max_bag_size=16, censor_prob_pos=0, censor_prob_neg=0)\n",
    "X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = \\\n",
    "        bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy1i2hpNsnJq"
   },
   "source": [
    "### C. Training the Model\n",
    "\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "\n",
    "Finally, we recreate the learning model and train it to the simulated dataset. Again, we first define some helper functions such as loss function, conversion of parameters (for reparameterization), initialization of weights and unpacking them, etc. Next, we train the model using stochastic gradient descent code from class. Finally, we combine everything together with the bagging simulator above, such that the function will take in an instance of a bagging simulator class, and run the necessary steps for the inputs to be bagged and used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUtELYIj8n-I"
   },
   "source": [
    "#### Analytical Gradient\n",
    "\n",
    "Intstead of directly applying the gradient descent, we have also re-derive the analytical gradient for each of the parameter that we need to train for. Although we don't have time to implement these derivations in the main body of training process, we expect that with the aid of analytical expression the algorithm will get more efficient. The followings are gradient expression with respect to our parameters:\n",
    "\n",
    "#### **Loss Function**\n",
    "\n",
    "\\begin{aligned}\n",
    "L(\\psi) &= -\\sum_j Y_j \\log (Q_j) + (1-Y_j) \\log(1-Q_j)\\\\\n",
    "\\frac{d L}{d Q} &= -\\sum_j \\frac{Y_j}{Q_j} + \\frac{1-Y_j}{1-Q_j}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "As a reminder,\n",
    "\n",
    "\\begin{aligned}\n",
    "Q_j &= 1 - \\exp\\left[-\\mu R_j\\right] \\\\\n",
    "&= 1 - \\exp\\left[-\\mu \\sum_n f_\\text{risk} (\\bar{x_n}; \\psi)\\right]\\\\\n",
    "f_\\text{risk} &= \\left[\\sum_{b=1}^{N_B} \\tau_{nb} w_b^{\\text{ble}}\\right] \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### **Gradient wrt to scale factor, $\\mu$**\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d Q_j}{d \\mu} &= R_j \\exp\\left[-\\mu R_j\\right]\\\\\n",
    "&= \\sum_n f_\\text{risk} \\exp\\left[-\\mu \\sum_n f_\\text{risk} (\\bar{x_n}; \\psi)\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### **Gradient wrt to bluetooth attenuation weights, $w_{1:4}^\\text{ble}$**\n",
    "\n",
    "Reminder of monotocity, where we are optimizing over $(w^{\\text{ble}}_1, \\Delta_2^{\\text{ble}},\\Delta_3^{\\text{ble}},\\Delta_4^{\\text{ble}})$.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d Q_j}{d w^{\\text{ble}}_b} &= \\frac{d Q_j}{df} \\frac{df}{d w^{\\text{ble}}_b}\\\\\n",
    "&= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\tau_{nb} \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right)\\\\\n",
    "\\frac{d Q_j}{d w^{\\text{ble}}_1}\n",
    "&= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\left[\\sum_{b=1}^{N_B} \\tau_{nb} \\right] \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right)\\\\\n",
    "\\frac{d Q_j}{d \\Delta^{\\text{ble}}_1} &= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\left[\\sum_{b=2}^{N_B} \\tau_{nb} \\right] \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right)\\\\\n",
    "\\frac{d Q_j}{d \\Delta^{\\text{ble}}_2} &= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\left[\\sum_{b=3}^{N_B} \\tau_{nb} \\right] \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right)\\\\\n",
    "\\frac{d Q_j}{d \\Delta^{\\text{ble}}_3} &= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\left[\\sum_{b=4}^{N_B} \\tau_{nb} \\right] \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right)\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "### **Gradient wrt to bluetooth attenuation thresholds, $\\theta_{1:3}^\\text{ble}$**\n",
    "\n",
    "Reminder of soft thresholding, where we are replacing hard binning with sigmoid function:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\tau_{nb} &= \\sum_k \\tau_{nk} I \\left(\\theta^{\\text{ble}}_{b-1} < a_{nk} \\leq \\theta^{\\text{ble}}_{b}\\right)\\\\\n",
    "& \\approx \\sum_k \\tau_{nk} \\sigma_\\tau (a_{nk} - \\theta^{\\text{ble}}_{b-1}) \\sigma_\\tau(\\theta^{\\text{ble}}_{b}-a_{nk})\\\\\n",
    "& \\approx \\sum_k \\tau_{nk} \\left[\\frac{1}{1+\\exp(-t(a_{nk} - \\theta^{\\text{ble}}_{b-1}))}\\right] \\left[\\frac{1}{1+\\exp(-t(\\theta^{\\text{ble}}_{b}-a_{nk}))}\\right]\\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d Q_j}{d \\theta^{\\text{ble}}_b} &= \\frac{d Q_j}{df} \\frac{df}{d\\tau}\\frac{d\\tau}{d \\theta^{\\text{ble}}_b}\\\\\n",
    "&= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\sum_{b=1}^{N_b} w_b^\\text{ble} \\; \\times \\; \\left[\\sum_{l=1}^{N_C} I(c_{n,l})w_l^{\\text{con}}\\right]\\right) \\frac{d\\tau}{d \\theta^{\\text{ble}}_b}\\\\\n",
    "\\frac{d\\tau}{d \\theta^{\\text{ble}}_b}\n",
    "&= \\sum_k \\tau_{nk} \\left[\\frac{-t}{(1+\\exp(-t(a_{nk} - \\theta^{\\text{ble}}_{b})))^2}\\right] \\left[\\frac{1}{1+\\exp(-t(\\theta^{\\text{ble}}_{b+1}-a_{nk}))}\\right]\\\\\n",
    "&+ \\sum_k \\tau_{nk} \\left[\\frac{1}{1+\\exp(-t(a_{nk} - \\theta^{\\text{ble}}_{b-1}))}\\right] \\left[\\frac{t}{(1+\\exp(-t(\\theta^{\\text{ble}}_{b}-a_{nk})))^2}\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "### **Gradient wrt to contagiousness weights, $w_{2:3}^\\text{con}$**\n",
    "\n",
    "Reminder of monotocity, where we are optimizing over $(w^{\\text{con}}_2, \\Delta^{\\text{con}})$.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d Q_j}{d w^{\\text{ble}}_l} &= \\frac{d Q_j}{df} \\frac{df}{d w^{\\text{con}}_l}\\\\\n",
    "&= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\sum_{b=1}^{N_B} \\tau_{nb} w_b^{\\text{ble}} \\; \\times \\; \\sum_{l=1}^{N_C} I(c_{n,l})\\right)\\\\\n",
    "\\frac{d Q_j}{d w^{\\text{con}}_2}\n",
    "&= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\sum_{b=1}^{N_B} \\tau_{nb} w_b^{\\text{ble}} \\; \\times \\; \\sum_{l=2}^{N_C} I(c_{n,l})\\right)\\\\\n",
    "\\frac{d Q_j}{d \\Delta^{\\text{con}}} &= \\left(\\mu \\exp\\left[-\\mu R_j\\right]\\right) \\sum_n \\left(\\sum_{b=1}^{N_B} \\tau_{nb} w_b^{\\text{ble}} \\; \\times \\; I(c_{n,3})\\right)\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpRV3ofL-I2y"
   },
   "source": [
    "#### Training Code\n",
    "\n",
    "[Return to top](#Notebook-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jAK-nPGMsmue"
   },
   "outputs": [],
   "source": [
    "# basic configuration\n",
    "n_rssi_buckets = 4\n",
    "n_infect_levels = 3\n",
    "rssi_lowest_th = -120\n",
    "n_rssi_th = n_rssi_buckets-1  # number of thresholds for rssi buckets\n",
    "model_dim = 1 + n_rssi_buckets + n_rssi_th + n_infect_levels  # total number of parameters\n",
    "\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "# helper functions\n",
    "def sigmoid_func(x):\n",
    "\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# loss function\n",
    "def cross_entropy_loss(y, q):\n",
    "    return -np.mean(y*np.log(q) + (1-y)*np.log(1-q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LRLehSjEsqoI"
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def get_init_parameters():\n",
    "    beta = 0.1\n",
    "\n",
    "    # rssi weights\n",
    "    rssi_w_residual = np.random.rand(n_rssi_buckets) * 0.01\n",
    "\n",
    "    # rssi thresholds\n",
    "    rssi_th_residual = np.random.randint(low=10, high=40, size=n_rssi_th)\n",
    "\n",
    "    # infectiousness weights\n",
    "    infect_w_residual = np.random.rand(n_infect_levels) * 0.01\n",
    "\n",
    "    return np.concatenate([[beta], rssi_w_residual, rssi_th_residual, infect_w_residual])\n",
    "\n",
    "\n",
    "get_beta = lambda weights: weights[0]\n",
    "get_rssi_w = lambda weights: weights[1:1+n_rssi_buckets]\n",
    "get_rssi_th = lambda weights: weights[1+n_rssi_buckets:1+n_rssi_buckets+n_rssi_th]\n",
    "get_infect_w = lambda weights: weights[1+n_rssi_buckets+n_rssi_th:1+n_rssi_buckets+n_rssi_th+n_infect_levels]\n",
    "\n",
    "# helper function to unpack weights\n",
    "def unpack_weights(weights):\n",
    "    return get_beta(weights), get_rssi_w(weights), get_rssi_th(weights), get_infect_w(weights)\n",
    "\n",
    "\n",
    "def residual_to_scoring(weights):\n",
    "    # rssi weights\n",
    "    rssi_w_residual = get_rssi_w(weights)\n",
    "    rssi_w = [np.sum(rssi_w_residual[:i+1]) for i in range(n_rssi_buckets)]\n",
    "\n",
    "    # rssi thresholds\n",
    "    rssi_th_residual = get_rssi_th(weights)\n",
    "    rssi_th =  [rssi_lowest_th+np.sum(rssi_th_residual[:i+1]) for i in range(n_rssi_th)]\n",
    "\n",
    "    # infectiousness weights\n",
    "    infect_w_residual = get_infect_w(weights)\n",
    "    infect_w = [np.sum(infect_w_residual[:i+1]) for i in range(n_infect_levels)]\n",
    "\n",
    "    return np.concatenate([[get_beta(weights)], rssi_w, rssi_th, infect_w])\n",
    "\n",
    "\n",
    "def print_params(weights):\n",
    "    print(\"    beta\", get_beta(weights))\n",
    "    print(\"    rssi_w\", get_rssi_w(weights))\n",
    "    print(\"    rssi_th\", get_rssi_th(weights))\n",
    "    print(\"    infect_w\", get_infect_w(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "94mZ_Ov6ss8R"
   },
   "outputs": [],
   "source": [
    "# continuous loss function, where attenuation is defined in terms of sigmoid functions\n",
    "def loss_fn_ce(params, batch_x, batch_y, assign_mat, sigmoid_temp):\n",
    "\n",
    "    f_rssi = batch_x[:, 0]\n",
    "    beta, rssi_w, rssi_th, infect_w = unpack_weights(residual_to_scoring(params))\n",
    "    infectiousness_bucket = np.eye(n_infect_levels)[np.array(batch_x[:, 2], dtype=int)]\n",
    "    infectiousness_score = np.sum(np.multiply(np.array(infect_w), infectiousness_bucket), axis=1)\n",
    "\n",
    "    rssi_score = rssi_w[0] * sigmoid_func(-sigmoid_temp * (f_rssi - rssi_th[0])) * \\\n",
    "                 sigmoid_func(10. * (f_rssi - rssi_lowest_th)) + rssi_w[-1] * \\\n",
    "                 sigmoid_func(sigmoid_temp * (f_rssi - rssi_th[n_rssi_buckets - 2]))\n",
    "    rssi_score += np.dot(sigmoid_func(sigmoid_temp * (f_rssi[:, np.newaxis] - rssi_th[: n_rssi_buckets - 2])) \\\n",
    "                         * sigmoid_func(-sigmoid_temp * (f_rssi[:, np.newaxis] - rssi_th[1: n_rssi_buckets - 1])),\n",
    "                         rssi_w[1: n_rssi_buckets - 1])\n",
    "    rssi_score *= batch_x[:, 1]\n",
    "\n",
    "    score = beta * rssi_score * infectiousness_score\n",
    "    bag_scores = np.dot(assign_mat, score)\n",
    "    bag_probs = 1 - np.exp(-bag_scores)\n",
    "    bag_probs = np.clip(bag_probs, 1e-5, 1 - 1e-5)\n",
    "    loss = cross_entropy_loss(batch_y, bag_probs)\n",
    "\n",
    "    return loss, bag_probs\n",
    "\n",
    "# discrete loss function, where attenuation is defined in terms of discrete bins\n",
    "def loss_fn_stepbins_ce(params, batch_x, batch_y, assign_mat, return_scores=False):\n",
    "    beta, rssi_w, rssi_th, infect_w = unpack_weights(residual_to_scoring(params))\n",
    "    infectiousness_bucket = jax.nn.one_hot(batch_x[:, 2], num_classes=n_infect_levels)\n",
    "    infectiousness_score = np.sum(np.multiply(np.array(infect_w), infectiousness_bucket), axis=1)\n",
    "    rssi_bucket = np.eye(n_rssi_buckets)[np.array(np.digitize(batch_x[:, 0], rssi_th), dtype=int)]\n",
    "    rssi_score = np.sum(np.multiply(np.array(rssi_w), rssi_bucket), axis=1) * batch_x[:, 1]\n",
    "\n",
    "    score = beta * rssi_score * infectiousness_score\n",
    "    if return_scores:\n",
    "        return score\n",
    "    bag_scores = np.dot(assign_mat, score)\n",
    "    bag_probs = 1 - np.exp(-bag_scores)\n",
    "    bag_probs = np.clip(bag_probs, 1e-5, 1 - 1e-5)\n",
    "    loss = cross_entropy_loss(batch_y, bag_probs)\n",
    "\n",
    "    return loss, bag_probs\n",
    "\n",
    "\n",
    "# numerical calculation of the gradient of the loss function\n",
    "def grad_loss_fn_ce(params, batch_x, batch_y, assign_mat, sigmoid_temp, batch_loss):\n",
    "    def grad(idx):\n",
    "        delta = 1e-10\n",
    "        params_d = np.copy(params)\n",
    "        params_d[idx] += delta\n",
    "        return (loss_fn_ce(params_d, batch_x, batch_y, assign_mat, sigmoid_temp)[0] - batch_loss)/delta\n",
    "    return np.asarray([grad(i) for i in range(len(params))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OwLWtzBfswYs"
   },
   "outputs": [],
   "source": [
    "def sigmoid_temp_fn(init, target, current_iter, num_iters):\n",
    "    return np.interp(current_iter, [0, num_iters * 0.5, num_iters * 0.9, num_iters], [init, init, target, target])\n",
    "\n",
    "# implementation of stochastic gradient descent\n",
    "def train(features, bag_labels, assign_mat, sigmoid_temp_init, sigmoid_temp_target, batch_size, num_iters, lr):\n",
    "    num_samples = len(bag_labels)\n",
    "    batch_size = np.minimum(batch_size, num_samples)\n",
    "\n",
    "    model_params = get_init_parameters()\n",
    "    loss_start, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_init)\n",
    "    loss_start_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"Start\", \"sigmoid loss\", loss_start, \"step loss\", loss_start_step)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        l, r = i*batch_size%num_samples, (i+1)*batch_size%num_samples\n",
    "        if l <= r:\n",
    "            batch_y = bag_labels[l:r]\n",
    "            batch_assign_mat = assign_mat[l:r]\n",
    "        else:\n",
    "            batch_y = np.concatenate([bag_labels[l:], bag_labels[:r]])\n",
    "            batch_assign_mat = np.concatenate([assign_mat[l:], assign_mat[:r]], axis=0)\n",
    "\n",
    "        sigmoid_temp = sigmoid_temp_fn(sigmoid_temp_init, sigmoid_temp_target, i, num_iters)\n",
    "\n",
    "        loss, _ = loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp)\n",
    "        grad = grad_loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp, loss)\n",
    "\n",
    "        model_params -= lr * grad\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            loss_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "            print(\"iter\", i, \"sigmoid loss\", loss, \"step loss\", loss_step, \"sigmoid temp\", sigmoid_temp)\n",
    "\n",
    "    loss_final, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_target)\n",
    "    loss_final_step, probs_final = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"End\", \"sigmoid loss\", loss_final, \"step loss\", loss_final_step)\n",
    "    print_params(model_params)\n",
    "\n",
    "    return model_params, loss_final_step, probs_final\n",
    "\n",
    "# metric used for assessing method: AUC score\n",
    "def auc(score_pred, labels):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, score_pred)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vHOXY8GXszsP"
   },
   "outputs": [],
   "source": [
    "# combining training, evaluation and bagging simulation\n",
    "def train_and_eval_with_bag_config(bag_sim: Bag_Simulator, X_epi, probabilities_true_epi, n_trials=1, n_random_restarts=5, order=True):\n",
    "    \"\"\"Train the risk score model on data generated according to the given bag configuration. \"\"\"\n",
    "    auc_train_trials = []\n",
    "    auc_test_trials = []\n",
    "    for j in range(n_trials):\n",
    "        # Y_epi, N_pos, N_neg, pos_neg_ratio = sample_labels(probabilities_true_epi)\n",
    "        X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi, visualize=False, order=order)\n",
    "        best_loss = np.inf\n",
    "        best_model_params = None\n",
    "        best_final_probs = None\n",
    "        for i in range(n_random_restarts):\n",
    "            print('----------- Trial {}/{}: Training run {}/{} ----------------'.format(j+1, n_trials, i+1, n_random_restarts))\n",
    "            model_params, loss_st_step, final_probs = train(X, bag_labels_trn, assign_mat_trn,\n",
    "                                               sigmoid_temp_init=0.1, sigmoid_temp_target=1,\n",
    "                                               batch_size=200, num_iters=5000, lr=0.001)\n",
    "            if loss_st_step < best_loss:\n",
    "                best_loss = loss_st_step\n",
    "                best_model_params = model_params\n",
    "                best_final_probs = final_probs\n",
    "\n",
    "        print(\"best loss\", best_loss)\n",
    "        print(\"best scoring parameters\")\n",
    "        print_params(residual_to_scoring(best_model_params))\n",
    "\n",
    "        probs_bags_true_trn = 1 - np.exp(np.dot(assign_mat_trn, np.log(1-probabilities_true)))\n",
    "        auc_train_trials.append({\n",
    "            'Learned': auc(best_final_probs, bag_labels_trn),\n",
    "            'True': auc(probs_bags_true_trn, bag_labels_trn),\n",
    "        })\n",
    "\n",
    "        scores_learned = scores_learned = loss_fn_stepbins_ce(best_model_params, X, None, None, return_scores=True)\n",
    "        scores_learned_bags_tst = np.dot(assign_mat_tst, scores_learned)\n",
    "        probs_learned_bags_tst = 1 - np.exp(-1*scores_learned_bags_tst)\n",
    "        probs_bags_true_tst = 1 - np.exp(np.dot(assign_mat_tst, np.log(1-probabilities_true)))\n",
    "        auc_test_trials.append({\n",
    "            'Learned': auc(probs_learned_bags_tst, bag_labels_tst),\n",
    "            'True': auc(probs_bags_true_tst, bag_labels_tst),\n",
    "        })\n",
    "\n",
    "    return auc_train_trials, auc_test_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKX8sTwMs0TN"
   },
   "source": [
    "## 4. Model and Experiments\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "\n",
    "â›” PLEASE DO NOT RERUN THE EXPERIMENT PART SINCE IT TAKES EXTREMELY LONG TIME\n",
    "\n",
    "\n",
    "A few experiments were performed around tweaking different features of the data to check that the parameter learning model is behaving as we expect it to. These experiments were geared towards an outcome-based focus (such that we want to see the outputs and use the expected trend of the output as a proxy for the \"correctness\" of the model), rather than a specific mechanics correctness check. The authors also compared the outcomes to the performance of a ground truth model (\"True\", in subsequent plots) and that of the Swiss model with fixed parameters. We decided to focus our efforts on comparing it to the ground truth to elucidate its general performance trends, since the Swiss model is not our focus.\n",
    "\n",
    "An overview of the experiments and a brief summary of the results are listed below. Specific elaboration of each experiment follows in the respective subsections for the experiments.\n",
    "\n",
    "1. **Maximum bag sizes were varied, while holding other parameters constant.** In particular, while the maximum bag size can go up to 32, the maximum number of positive exposures per bag remains fixed at 1.\n",
    "2. **Censoring probability is varied, while holding other parameters constant.** Censoring probability refers to the chances of an exposure not being recorded and thus not being a part of the data. We test this with different censor probabilities, for fixed maximum bag sizes of 16 and 32, with a maximum of 1 positive exposure per bag.\n",
    "3. **Varying positive bag construction, with varied maximum bag sizes.** The maximum number of positive exposures per bag can now go up to 3, while maximum bag sizes increases up to 32.\n",
    "4. **Robustness to model mismatch.** Since we are working with simulated data, there is a probability that the simulated data is not reflective of the real world, and that data in the real world is much less precise. Hence, the original generative model is approximated and the resulting dataset tested on to see the performance of the current learning model to potentially different generative models.\n",
    "\n",
    "In addition, we ran one of our own experiments just as corroboration to make sure that the learning model is not training or fitting specifically to a specific order of the data, again as a check for the robustness of the model in general.\n",
    "\n",
    "5. **Reordering of data in training.** Training data is reordered so the model will learn parameters on different sequences of data points each time.\n",
    "\n",
    "For all experiments, the metric used in the paper and which we are emulating is the AUC score. A higher AUC score reflects higher accuracy of the model in predicting the parameters that will generate a risk score which gives the maximum log-likelihood for the infection observation. Since we are using our own basic python code for the training, we do not have the efficiency gains of an ML package like `jax`. Hence, our code takes a very long while to run (about 20 minutes for 1 trial with 5 random starting points) and we were unable to run it multiple times to get replicates. Instead, we were only able to initialize the algorithm from 5 different starting points and plotted the results from the best output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DROAtG7ZxLd4"
   },
   "source": [
    "### 1. Varying maximum bag size\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "This experiment is iterated over increasing bag sizes to show the effect that increasing bag size has on the performance of the learning algorithm. Increasing bag size would increase uncertainty in assigning a risk score, since there is more noise with more observations, while the maximum number of positive exposures in the bag still remains fixed at 1. Hence, the signal of the one positive exposure (if it exists) gets drowned out as bag size increases, and it is harder to push the algorithm to an optimum which captures these signals well. \n",
    "\n",
    "We expect the performance of the algorithm to decrease, i.e. the AUC score to fall, as bag size increases. This is reflected in the two plots below, where the AUC drops from a high of around 0.9 to plateau arounnd 0.7. We also note that the performance of the learning algorithm is worse than that of the true parameters, which is to be expected. Overall, the trends for this experiment definitely supports the claim being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf28iXb4s22R",
    "outputId": "b189b595-efaa-455e-c99c-a1bd5b17084a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4431, 4431) (4431, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 4431) (887, 4431)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.2196481992672172 step loss 1.222210239360569\n",
      "iter 0 sigmoid loss 0.9862754799858239 step loss 0.814513886000415 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.34829134088286573 step loss 0.33055410146157227 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3654958522934435 step loss 0.3173655889164178 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3163487501708123 step loss 0.30776392023344307 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34303746484624625 step loss 0.29532438249387144 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3010745930260712 step loss 0.30725786676460737 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25729091373058494 step loss 0.30843284917153 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2339990858005359 step loss 0.2852264088550915 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2946117784621882 step loss 0.2834389517223261 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2538802932606835 step loss 0.2821023762972173 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27690590014658967 step loss 0.28207150875750864\n",
      "    beta 0.31814946321585535\n",
      "    rssi_w [-0.02672788  0.00308919  0.05818862  0.29809107]\n",
      "    rssi_th [22.00970926 12.00974559 26.00950492]\n",
      "    infect_w [0.01011637 0.06290278 0.2990673 ]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.3492833660134254 step loss 1.3501762423425883\n",
      "iter 0 sigmoid loss 1.0852725920246198 step loss 0.7766639402810296 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3449052873836508 step loss 0.3249453762118487 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3599655661093599 step loss 0.3105596722543181 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30729405882520316 step loss 0.30051551777960184 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.33754079587338753 step loss 0.2902281956425082 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2979583363513365 step loss 0.5535757176672355 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2560376819566245 step loss 0.317482220030642 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.23342053877684785 step loss 0.285559235787496 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2941459574845776 step loss 0.283339030795168 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.253804805694507 step loss 0.2821701700847701 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27694375783764374 step loss 0.28211412077414877\n",
      "    beta 0.3189380318706746\n",
      "    rssi_w [-0.01563539  0.04994793  0.29846174  0.04497514]\n",
      "    rssi_th [33.0100867  27.00981929 38.99981351]\n",
      "    infect_w [0.01009629 0.06257141 0.29982455]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.314431752960136 step loss 1.3223808409892337\n",
      "iter 0 sigmoid loss 1.0759138775539359 step loss 0.7317077515354588 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3454184108267995 step loss 0.330822231178298 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3625657197554941 step loss 0.31805266467399923 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3099096490193773 step loss 0.30815852269488203 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3367237385775653 step loss 0.2946497053107379 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.29421561235440014 step loss 0.29308202272137457 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25772315453353195 step loss 0.281640689359193 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.22826954898652813 step loss 0.2806432100951332 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2812712329182976 step loss 0.2807752707018698 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.24316066811962977 step loss 0.2795575323582204 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27076288414010735 step loss 0.2797176575118505\n",
      "    beta 0.34214272559275327\n",
      "    rssi_w [-0.04834133 -0.03303423  0.11643114  0.30446053]\n",
      "    rssi_th [13.00561404 33.0056144  16.00453658]\n",
      "    infect_w [0.01037641 0.06674242 0.3237918 ]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1360164106822468 step loss 1.1594077049401923\n",
      "iter 0 sigmoid loss 0.9226898994047119 step loss 0.881699584307315 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3447300594788919 step loss 0.33690844808999 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.359397899639995 step loss 0.3248854310213846 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30207885745859847 step loss 0.31890956532086456 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3363709486149912 step loss 0.31702291494556284 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.29367699023145855 step loss 0.3737019028574219 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2732222263239344 step loss 0.314808378874788 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.25300941727592285 step loss 0.3063724230940409 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2910865674946439 step loss 0.3029864787099586 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2509768667767587 step loss 0.3015879776365636 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2875716933333297 step loss 0.30173233143928085\n",
      "    beta 0.3819275465859583\n",
      "    rssi_w [-0.00133491  0.05422526  0.35900911  0.07284581]\n",
      "    rssi_th [29.98777505 38.98744486 23.99938031]\n",
      "    infect_w [0.01104492 0.07244988 0.36374857]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.2403519668438656 step loss 1.2490852580666376\n",
      "iter 0 sigmoid loss 1.032214014006301 step loss 0.867480483443673 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3463001030266875 step loss 0.33955959339768405 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3629528043739739 step loss 0.33079954989487065 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3128960099281651 step loss 0.32652306274269177 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34278448432940706 step loss 0.32245603872072054 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3048556793162368 step loss 0.3689122821855853 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2944052874933162 step loss 0.3705178964001299 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2900462691737964 step loss 0.36841679192008675 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.33615207228610167 step loss 0.36798555368067787 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2768485580437252 step loss 0.3678325509947183 sigmoid temp 1.0\n",
      "End sigmoid loss 0.31629904388682406 step loss 0.36768966712358037\n",
      "    beta 0.29173950815964955\n",
      "    rssi_w [-0.02533958  0.01802206  0.20917493  0.18011978]\n",
      "    rssi_th [27.00841907 25.00835387 30.99784968]\n",
      "    infect_w [0.01045732 0.0630152  0.27083847]\n",
      "best loss 0.2797176575118505\n",
      "best scoring parameters\n",
      "    beta 0.34214272559275327\n",
      "    rssi_w [-0.04834133 -0.08137557  0.03505558  0.3395161 ]\n",
      "    rssi_th [-106.99438596  -73.98877156  -57.98423498]\n",
      "    infect_w [0.01037641 0.07711883 0.40091062]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.531, median size 2\n",
      "\t Negative bags: mean size 1.530, median size 2\n",
      "assign_mat size, X_shuff size: (4431, 6781) (6781, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 6781) (887, 6781)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.2542579502446776 step loss 1.2487382612923148\n",
      "iter 0 sigmoid loss 1.0991440368194825 step loss 0.7495445106839727 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33032400626120334 step loss 0.36628090369556265 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41974103946148633 step loss 0.3547656606709759 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.37204511361807846 step loss 0.347653789696991 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3406001112522178 step loss 0.4036199915562351 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30990460618709664 step loss 0.4444212303274524 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.30229686299501124 step loss 0.43658300450524196 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2832648501719038 step loss 0.4184632280591647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.32453606002701746 step loss 0.4026785886173063 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.37495171800940574 step loss 0.3490513812784508 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3371028905955947 step loss 0.3481930017405456\n",
      "    beta 0.26619442821941275\n",
      "    rssi_w [-0.00820623  0.01422462  0.21210928  0.13712829]\n",
      "    rssi_th [18.01018765 36.01016367 32.99884028]\n",
      "    infect_w [0.00944253 0.04811085 0.24852945]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.4386797658024109 step loss 1.437051178897278\n",
      "iter 0 sigmoid loss 1.26710050584824 step loss 0.540926449889315 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33710987817393984 step loss 0.37760118046583463 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4228594436300864 step loss 0.3630179279868117 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3776594556156215 step loss 0.3559227381650932 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.33448178270814977 step loss 0.38326814155192285 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3066721859569305 step loss 0.4287893896730894 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.30562287351209627 step loss 0.4009645384006627 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.29224453283880003 step loss 0.39318677139244596 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.33107559809742104 step loss 0.383451019446852 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3996469968631663 step loss 0.37731657674175195 sigmoid temp 1.0\n",
      "End sigmoid loss 0.34887275574179116 step loss 0.3752514998105998\n",
      "    beta 0.24507336526525803\n",
      "    rssi_w [-0.07370648 -0.0252912   0.07195639  0.23113137]\n",
      "    rssi_th [27.01000343 10.00993883 15.00963243]\n",
      "    infect_w [0.01057806 0.04871464 0.25377082]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.2145943589558894 step loss 1.2292449096664293\n",
      "iter 0 sigmoid loss 1.0657432882995377 step loss 0.7813910710366821 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3276137433260783 step loss 0.358233910930718 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41508424386466586 step loss 0.34236649022081955 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36730344393186437 step loss 0.32975119174741874 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3300639430405276 step loss 0.49979604588623766 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3053861670318048 step loss 0.543407607064737 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29371004814447277 step loss 0.5078765095440005 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2704012278262117 step loss 0.32778058524572434 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.31698399266707566 step loss 0.31781969888418493 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31492519133804886 step loss 0.3168103721174622 sigmoid temp 1.0\n",
      "End sigmoid loss 0.31043643090370043 step loss 0.31669828216149226\n",
      "    beta 0.2929171922210315\n",
      "    rssi_w [-0.02779731  0.05170849  0.26305149  0.1024189 ]\n",
      "    rssi_th [35.01225472 24.01199559 30.99939719]\n",
      "    infect_w [0.00843376 0.04895851 0.27700931]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1565252853743866 step loss 1.157828979867429\n",
      "iter 0 sigmoid loss 1.0141812626301707 step loss 0.7999139062573913 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3300350761205623 step loss 0.36078568683330753 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41891618024698085 step loss 0.34673737999570814 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.37149819373131676 step loss 0.33654393882703854 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.33920007544697034 step loss 0.34077328018130315 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3091270281657758 step loss 0.48882039223925816 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.33685400055753106 step loss 0.3554555615783302 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3111267887904567 step loss 0.3487347387723489 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.330637352697524 step loss 0.34542802097490233 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3930632843702508 step loss 0.34140681203905016 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3364206885214906 step loss 0.3368920906119168\n",
      "    beta 0.14682351562035334\n",
      "    rssi_w [0.08292439 0.12534931 0.39435069 0.07400231]\n",
      "    rssi_th [29.00632536 28.00621409 36.99969133]\n",
      "    infect_w [0.00671981 0.03269384 0.15098285]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.211002781684638 step loss 1.2453393225844651\n",
      "iter 0 sigmoid loss 1.0602174116097216 step loss 0.7758903826781265 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3234545002479548 step loss 0.35488784285481795 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4061939142812665 step loss 0.33750105316544055 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3574537604012478 step loss 0.33722094612732323 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.30536489883608153 step loss 0.6355887406828218 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2982555164212942 step loss 0.6406657923450457 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29692938848060524 step loss 0.3673220005953579 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2740566414020418 step loss 0.31499985225024557 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.31079578982933553 step loss 0.30849609850343557 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.30445453980848597 step loss 0.307813102304284 sigmoid temp 1.0\n",
      "End sigmoid loss 0.29929800452213684 step loss 0.3078427541738534\n",
      "    beta 0.32133188337658536\n",
      "    rssi_w [-0.00046041  0.02589036  0.24405924  0.19126697]\n",
      "    rssi_th [25.00418659 36.00412456 14.99856078]\n",
      "    infect_w [0.00852617 0.04813495 0.30638078]\n",
      "best loss 0.3078427541738534\n",
      "best scoring parameters\n",
      "    beta 0.32133188337658536\n",
      "    rssi_w [-4.60413277e-04  2.54299438e-02  2.69489184e-01  4.60756159e-01]\n",
      "    rssi_th [-94.99581341 -58.99168885 -43.99312806]\n",
      "    infect_w [0.00852617 0.05666112 0.3630419 ]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.442, median size 2\n",
      "\t Negative bags: mean size 2.460, median size 2\n",
      "assign_mat size, X_shuff size: (4431, 10889) (10889, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 10889) (887, 10889)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0009368453708052 step loss 1.0248526300033787\n",
      "iter 0 sigmoid loss 0.8231362585072307 step loss 0.80889379428807 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3628610158793373 step loss 0.4012783201372545 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.36172196094055536 step loss 0.3941323763140489 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.34934776868486694 step loss 0.3914627814591779 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4367228915978661 step loss 0.3969166380659574 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3252530793532269 step loss 0.48469674641811367 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39182889847809826 step loss 0.3922787187857123 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.38475436692661263 step loss 0.3852666409476967 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4190733923236498 step loss 0.3847234319673131 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.39157700924857425 step loss 0.38462227368192925 sigmoid temp 1.0\n",
      "End sigmoid loss 0.37826725600014494 step loss 0.38510683956852026\n",
      "    beta 0.3302946622290136\n",
      "    rssi_w [-0.04149985  0.08944592  0.29476089  0.06709602]\n",
      "    rssi_th [37.99055327 36.98963673 18.99927916]\n",
      "    infect_w [0.00883776 0.04646333 0.31384184]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0169572395096111 step loss 1.0203426143383083\n",
      "iter 0 sigmoid loss 0.8412057684413875 step loss 0.767353303559227 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3678601794554108 step loss 0.40213354739378115 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3695608839609963 step loss 0.397648774933657 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35900305527165016 step loss 0.39624251882817163 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4446735743051898 step loss 0.39415603956653644 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.33104566778180955 step loss 0.39244676338173845 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.397821579863171 step loss 0.3923947204503723 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3900990714252515 step loss 0.3919042920844357 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4202575141902929 step loss 0.3905120285741393 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.40110131135992083 step loss 0.3902550421131676 sigmoid temp 1.0\n",
      "End sigmoid loss 0.38577807715733203 step loss 0.3907950402788242\n",
      "    beta 0.3335286758414767\n",
      "    rssi_w [-0.07584819 -0.06677438  0.19058492  0.23899636]\n",
      "    rssi_th [11.0005262  35.00052862 31.99398383]\n",
      "    infect_w [0.01031902 0.05503392 0.3159286 ]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0484433033742078 step loss 1.0587712699886058\n",
      "iter 0 sigmoid loss 0.8622807955403934 step loss 0.7670525044417578 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3629512560785062 step loss 0.38651290856272463 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35881933266672017 step loss 0.3735329989261551 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.34054715434828803 step loss 0.36894619769280046 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.41951123149628833 step loss 0.541875603843097 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.32330832482106237 step loss 0.5689730730368658 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.357686665528317 step loss 0.48570311618842565 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34634188376798747 step loss 0.4204324095957439 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3985344534770286 step loss 0.3637922260118371 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3648880718368695 step loss 0.36013264034437176 sigmoid temp 1.0\n",
      "End sigmoid loss 0.35149434911927163 step loss 0.3605828786598372\n",
      "    beta 0.26986989814782303\n",
      "    rssi_w [-0.01655406  0.02685145  0.15609169  0.203555  ]\n",
      "    rssi_th [27.00710348 29.00703766 21.99790526]\n",
      "    infect_w [0.00682411 0.03888786 0.25217181]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.021225537552424 step loss 1.0174427860005293\n",
      "iter 0 sigmoid loss 0.8422612533781575 step loss 0.7482279136250211 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36889547851892823 step loss 0.3900729636002297 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.370134259979335 step loss 0.3813461153692668 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35829194557512195 step loss 0.3733966558719989 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4378453946600935 step loss 0.4168825216240139 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3273189027184783 step loss 0.5102442485853002 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3752551399616602 step loss 0.4674887613737611 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36226441503490264 step loss 0.4340540299336691 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.40403639990647877 step loss 0.3787452138671756 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3790056199808578 step loss 0.36950699873382176 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3614832262563044 step loss 0.369813211204048\n",
      "    beta 0.21193359183980787\n",
      "    rssi_w [-0.02447912 -0.02369076  0.05872583  0.24376672]\n",
      "    rssi_th [11.01113978 25.01114586 19.01097838]\n",
      "    infect_w [0.00608109 0.0337888  0.20483724]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1630920721450544 step loss 1.1778537357295034\n",
      "iter 0 sigmoid loss 0.9560840221778392 step loss 0.72463120453073 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3619910516719601 step loss 0.3850839928406265 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35665167743095594 step loss 0.3699143093813687 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33757430179609194 step loss 0.3637993720242214 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4171085368497141 step loss 0.5984291844121125 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.32255441301808885 step loss 0.6328150684560299 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.34344936461604286 step loss 0.5057578985171322 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.33197177895213076 step loss 0.35361564733886125 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.385408963562775 step loss 0.34864595236928425 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3612986607373753 step loss 0.34748513149915033 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3412614695223663 step loss 0.3481679213500683\n",
      "    beta 0.28151688568890376\n",
      "    rssi_w [-0.0305788   0.04681709  0.19106086  0.1788425 ]\n",
      "    rssi_th [35.00722378 24.00703123 18.99867989]\n",
      "    infect_w [0.00705589 0.03596621 0.26577491]\n",
      "best loss 0.3481679213500683\n",
      "best scoring parameters\n",
      "    beta 0.28151688568890376\n",
      "    rssi_w [-0.0305788   0.01623829  0.20729915  0.38614165]\n",
      "    rssi_th [-84.99277622 -60.985745   -41.9870651 ]\n",
      "    infect_w [0.00705589 0.0430221  0.30879701]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.434, median size 3\n",
      "\t Negative bags: mean size 3.549, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 15644) (15644, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 15644) (887, 15644)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0989571474625857 step loss 1.1018578231800826\n",
      "iter 0 sigmoid loss 1.1491678917969221 step loss 0.6052432478972876 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43019727367072585 step loss 0.4303060231518011 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4221958864305085 step loss 0.42951982774716935 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4159549802963056 step loss 0.4298024230291397 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.47475275498545827 step loss 0.4299192160000223 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.428945102741489 step loss 0.4306723208746355 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.42646279131652903 step loss 0.42910824760835076 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5405900096016509 step loss 0.42926517851668833 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.34354036220633405 step loss 0.42916078951635594 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.44435030878977727 step loss 0.4290950126184315 sigmoid temp 1.0\n",
      "End sigmoid loss 0.42913390821474634 step loss 0.42913875156407705\n",
      "    beta 0.18132076678712963\n",
      "    rssi_w [-0.03284964 -0.01530286  0.03611927  0.14672396]\n",
      "    rssi_th [22.00132156 10.00134886 10.001404  ]\n",
      "    infect_w [0.01062208 0.02639868 0.16054654]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.2119670796864503 step loss 1.2229387054156347\n",
      "iter 0 sigmoid loss 1.2683595662648006 step loss 0.570528102632815 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42813118261076055 step loss 0.417124536305877 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4038746277819699 step loss 0.39759907954873974 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3840140172937482 step loss 0.7650994889189427 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4199193013336431 step loss 0.8689976152125455 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38364709348881654 step loss 0.8811888709028907 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3574572382830311 step loss 0.38289884093419563 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4669730734935383 step loss 0.36973486761395324 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28145735030036656 step loss 0.3700513715525977 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3912889287350083 step loss 0.37003353814577594 sigmoid temp 1.0\n",
      "End sigmoid loss 0.36189237967312826 step loss 0.37063758401831925\n",
      "    beta 0.2873728333601221\n",
      "    rssi_w [-0.01242542  0.03102715  0.26516283  0.09476956]\n",
      "    rssi_th [30.00057151 35.00046219 23.9995277 ]\n",
      "    infect_w [0.00624657 0.03341343 0.28058031]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.089235446815925 step loss 1.0867601230048185\n",
      "iter 0 sigmoid loss 1.1384517746100855 step loss 0.6357239189450119 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42497498357951646 step loss 0.4243888699108853 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4117079770049823 step loss 0.4174013021488541 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.39683855143774827 step loss 0.410110564388828 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4326121988220846 step loss 0.4144896440304075 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3801074999034559 step loss 0.4315787615785423 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3490388213558331 step loss 0.4007558547017735 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.47992287676022516 step loss 0.4001026047967735 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3001034681846128 step loss 0.40180583662212876 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.4206628851940893 step loss 0.40039472266095844 sigmoid temp 1.0\n",
      "End sigmoid loss 0.38871202477845457 step loss 0.4014085668291584\n",
      "    beta 0.3192839032577609\n",
      "    rssi_w [-0.05132325 -0.03842799  0.11761297  0.2893943 ]\n",
      "    rssi_th [16.98970557 26.98970409 27.98812128]\n",
      "    infect_w [0.00991987 0.03800155 0.30651046]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.2374652044087817 step loss 1.2697797102512385\n",
      "iter 0 sigmoid loss 1.2976138857638984 step loss 0.5763021466150031 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4241994071074896 step loss 0.4116796711420008 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4079279671152121 step loss 0.39619283423932644 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.39126940321822007 step loss 0.4218825181625615 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4249241941451551 step loss 0.7093116996353145 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38229500069968725 step loss 0.7345878140197787 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.38007825460196515 step loss 0.4127735321746032 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.48574342987301306 step loss 0.3791740754076931 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2860272335881121 step loss 0.3782040757862411 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3918948375370654 step loss 0.377177215459471 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3688068899537123 step loss 0.37754337545691624\n",
      "    beta 0.2456382403564049\n",
      "    rssi_w [-0.00466312  0.02261362  0.24071925  0.07923244]\n",
      "    rssi_th [26.00967569 35.00961444 31.99964521]\n",
      "    infect_w [0.00606669 0.03059335 0.23279802]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9990187365202167 step loss 1.019347885136546\n",
      "iter 0 sigmoid loss 1.0410919190990617 step loss 0.7029442166259489 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4219953137915449 step loss 0.411482232579493 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.40595504998153326 step loss 0.39661792945829033 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3882560140898776 step loss 0.5540792308147697 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.42270296800306795 step loss 0.7158349577557306 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38143377942134804 step loss 0.737033081734651 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3774064329720085 step loss 0.4055128882817777 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4850261034436116 step loss 0.3783232867447902 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2852441221257129 step loss 0.3776859913850543 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3926951459609441 step loss 0.37677072302007253 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3684148357709782 step loss 0.3771969653459169\n",
      "    beta 0.2556671974656254\n",
      "    rssi_w [-0.01794022  0.03459107  0.22175838  0.11948838]\n",
      "    rssi_th [35.00910756 26.00896472 24.99929547]\n",
      "    infect_w [0.00630109 0.03116419 0.23967559]\n",
      "best loss 0.37063758401831925\n",
      "best scoring parameters\n",
      "    beta 0.2873728333601221\n",
      "    rssi_w [-0.01242542  0.01860172  0.28376456  0.37853411]\n",
      "    rssi_th [-89.99942849 -54.9989663  -30.9994386 ]\n",
      "    infect_w [0.00624657 0.03966    0.3202403 ]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.111, median size 4\n",
      "\t Negative bags: mean size 3.988, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17757) (17757, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 17757) (887, 17757)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9748990167748784 step loss 0.9859036567571016\n",
      "iter 0 sigmoid loss 0.9204370479055964 step loss 0.7205776203992035 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.48068134312406 step loss 0.41566616313443766 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42690976696065575 step loss 0.4074721516275898 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.358682439677635 step loss 0.406635593633988 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.40626159439451826 step loss 1.1298781195394467 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.346180246001649 step loss 1.2188714588124245 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3371343632484743 step loss 0.40778335353739853 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.32875233721474884 step loss 0.3965907613235664 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3659329837963686 step loss 0.39277077285224915 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.40592914292714816 step loss 0.3912566532701605 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3792666618154413 step loss 0.3938106767867642\n",
      "    beta 0.31213998890878825\n",
      "    rssi_w [-0.04467326  0.06984278  0.29299468  0.04111122]\n",
      "    rssi_th [37.98794502 32.98738379 29.99973622]\n",
      "    infect_w [0.00488498 0.03825571 0.30008489]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.152542444634709 step loss 1.1522866583609814\n",
      "iter 0 sigmoid loss 1.0858167025861456 step loss 0.6014132656539012 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4832140267495075 step loss 0.41256665937311987 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.43022222063782567 step loss 0.40195501631098035 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35979231327915884 step loss 0.3912787577576726 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.40630284633328256 step loss 0.873480293500598 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.351230999589402 step loss 0.9469848240254967 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.33939729376038286 step loss 0.3893535992202328 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.31228573817122457 step loss 0.3785803080589754 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3389561677099671 step loss 0.37275639823519263 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.389688210338941 step loss 0.3713697156095296 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3603329659509825 step loss 0.3770192666736725\n",
      "    beta 0.27512365585448106\n",
      "    rssi_w [-0.03679403 -0.01900935  0.07385382  0.29175778]\n",
      "    rssi_th [18.99482058 19.99482397 27.99438988]\n",
      "    infect_w [0.00583846 0.03403145 0.29139881]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1597757367794224 step loss 1.1580803535819795\n",
      "iter 0 sigmoid loss 1.1011447432798462 step loss 0.5617375015403211 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4823674233072873 step loss 0.41294184464269407 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4272972456620168 step loss 0.39914260661616086 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3573633064707212 step loss 0.3903061870890647 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.39992844613333306 step loss 0.6591298642428416 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.36723951868724625 step loss 0.4674436792622308 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.34261832979773277 step loss 0.3885454792259474 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3192256841696225 step loss 0.3778430668941695 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3382443046100865 step loss 0.37235303248433815 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3895818842608566 step loss 0.3711448202500897 sigmoid temp 1.0\n",
      "End sigmoid loss 0.36007935471147745 step loss 0.37597411229998257\n",
      "    beta 0.2336049182398824\n",
      "    rssi_w [-0.04801234 -0.03228204  0.10058249  0.30689587]\n",
      "    rssi_th [22.99510837 19.99512424 23.99424339]\n",
      "    infect_w [0.00617988 0.03857789 0.31589681]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9054840584248571 step loss 0.9128406414394724\n",
      "iter 0 sigmoid loss 0.8610691703889679 step loss 0.6880609068382071 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4832408382458469 step loss 0.4171116281463462 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4310355092018325 step loss 0.41039150607042313 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36111102000859474 step loss 0.40375199495803615 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4106125035354161 step loss 0.4298056050266513 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.44379451848360346 step loss 0.43797578360279893 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4021829426619099 step loss 0.431245105312617 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.372676774577981 step loss 0.4246252405338701 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.40398728081359897 step loss 0.421373533969354 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.43679432779395777 step loss 0.4158077196241885 sigmoid temp 1.0\n",
      "End sigmoid loss 0.40470323843500783 step loss 0.4087460914588197\n",
      "    beta 0.07536484219740058\n",
      "    rssi_w [-0.02253801 -0.0067258   0.08045713  0.26329795]\n",
      "    rssi_th [19.99400157 20.99403192 30.99339984]\n",
      "    infect_w [0.10592904 0.08796898 0.3552571 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1067272503033452 step loss 1.1115718601801887\n",
      "iter 0 sigmoid loss 1.0502086881486052 step loss 0.6732712002007268 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4823979767143507 step loss 0.41582542594842986 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42901097496177143 step loss 0.4067246366256657 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36016451263982235 step loss 0.40384476046103407 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.41060781824615517 step loss 1.0947507774682055 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.34674874653780835 step loss 1.1815050986917504 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.336159067952802 step loss 0.4067184808833454 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3230778763091036 step loss 0.3926995721898426 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35841104268777735 step loss 0.3881869705657731 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4008175564446487 step loss 0.38650891528584413 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3736397474248406 step loss 0.389879186423813\n",
      "    beta 0.30955920915222135\n",
      "    rssi_w [-0.02489753  0.0481256   0.29801111  0.02520082]\n",
      "    rssi_th [33.98849526 35.98819671 38.99988173]\n",
      "    infect_w [0.00510078 0.03584486 0.29729084]\n",
      "best loss 0.37597411229998257\n",
      "best scoring parameters\n",
      "    beta 0.2336049182398824\n",
      "    rssi_w [-0.04801234 -0.08029438  0.0202881   0.32718397]\n",
      "    rssi_th [-97.00489163 -77.00976739 -53.015524  ]\n",
      "    infect_w [0.00617988 0.04475777 0.36065458]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.037, median size 3\n",
      "\t Negative bags: mean size 3.992, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17719) (17719, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 17719) (887, 17719)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9584355952189165 step loss 0.9595069357448618\n",
      "iter 0 sigmoid loss 0.9155362039793868 step loss 0.6752425610384393 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44330682317406406 step loss 0.4239566373152859 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4413084124380372 step loss 0.41351743661258245 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3404559227328157 step loss 0.4058181748698215 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3682367589200335 step loss 0.9318070198402737 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3749802592091828 step loss 1.0089297398063108 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.45270445429834794 step loss 0.3963004660913007 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.35029233394839937 step loss 0.38915704120736727 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3475395871499697 step loss 0.3920814797293475 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.30526468054531264 step loss 0.3859755813327383 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3706779935799182 step loss 0.38651332769284164\n",
      "    beta 0.2949379518917382\n",
      "    rssi_w [-0.02083399 -0.00368505  0.04657364  0.28543612]\n",
      "    rssi_th [19.99435535  9.99436729 36.99420367]\n",
      "    infect_w [0.00637619 0.02707732 0.28362898]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0138017550285168 step loss 1.0256546806081344\n",
      "iter 0 sigmoid loss 0.9864079503365781 step loss 0.6784153598565799 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43979824897093067 step loss 0.43347518476337255 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4368671576406108 step loss 0.4278280189570286 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3407430177530455 step loss 0.4269559566314634 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3792049194763557 step loss 0.44043125939459465 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3722549596178888 step loss 0.5202180820738351 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4940794675369483 step loss 0.42425933809560185 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.38633359349117724 step loss 0.42221971894193017 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35854694940279286 step loss 0.4224683558308123 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.30470171812167707 step loss 0.422024863177062 sigmoid temp 1.0\n",
      "End sigmoid loss 0.41770753059157606 step loss 0.4219408704439557\n",
      "    beta 0.3040799584879238\n",
      "    rssi_w [-0.05819764  0.09544139  0.26171356  0.07078446]\n",
      "    rssi_th [38.99352128 38.99245134 14.99919075]\n",
      "    infect_w [0.00929784 0.03873178 0.28991958]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9746253547817743 step loss 0.9735765249305646\n",
      "iter 0 sigmoid loss 0.931087828181728 step loss 0.6573153593484814 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44385676703578186 step loss 0.42392621701631783 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4430958483340802 step loss 0.4163069058981526 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.34517216449511573 step loss 0.408610355698138 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3771845938640909 step loss 0.5193985479359687 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3797174405647921 step loss 0.5532277305555318 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.45946059306428977 step loss 0.5259614331752199 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3749086625005987 step loss 0.46090629116178283 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3353888036484151 step loss 0.4537383749257938 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.29288040128234494 step loss 0.41393621278548076 sigmoid temp 1.0\n",
      "End sigmoid loss 0.401287047060194 step loss 0.4124267397071571\n",
      "    beta 0.19649131493816296\n",
      "    rssi_w [-0.03739503 -0.02244763  0.06617088  0.1878629 ]\n",
      "    rssi_th [16.00859271 23.00858779 16.00836327]\n",
      "    infect_w [0.00614487 0.02404731 0.18514966]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9877497029337007 step loss 0.9974353777040189\n",
      "iter 0 sigmoid loss 0.9505657346040545 step loss 0.658009446086451 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44070108425446475 step loss 0.42043000755785187 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4335668877127297 step loss 0.40675983463846016 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3174385451688586 step loss 0.3875504087594457 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3505832120232563 step loss 0.38091119395678297 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37586555920865244 step loss 0.3811394567968361 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4423536893168996 step loss 0.37536973167824284 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3431746867583359 step loss 0.3751973391236418 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3255857978257046 step loss 0.37794756566718213 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.29627796233145637 step loss 0.375840165759274 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3650240756447838 step loss 0.3756677560178296\n",
      "    beta 0.28135943247177264\n",
      "    rssi_w [-0.07310736 -0.00284085  0.09900217  0.2554516 ]\n",
      "    rssi_th [38.00308203 10.00298624 17.00126918]\n",
      "    infect_w [0.00582939 0.02755165 0.27153014]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0069936172229015 step loss 1.0087008191829636\n",
      "iter 0 sigmoid loss 0.979822516634024 step loss 0.6084432120245528 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44211491371467326 step loss 0.42120138054691286 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4373179583337275 step loss 0.40639168664940567 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3298535064865496 step loss 0.39725737670134775 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35756767995416483 step loss 0.7917337828834067 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37557014755077717 step loss 0.8455885135647134 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.44920927619332957 step loss 0.38686193788397766 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34458356758771136 step loss 0.3788719972030017 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3303508989710538 step loss 0.3825570222984718 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.29961822864174914 step loss 0.37699547246921594 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3669119694932571 step loss 0.3768125801558805\n",
      "    beta 0.2759777129572981\n",
      "    rssi_w [-0.03090733 -0.02230944  0.0743536   0.26903605]\n",
      "    rssi_th [16.00100298 23.00100458 26.00062674]\n",
      "    infect_w [0.00573934 0.02682251 0.26771483]\n",
      "best loss 0.3756677560178296\n",
      "best scoring parameters\n",
      "    beta 0.28135943247177264\n",
      "    rssi_w [-0.07310736 -0.0759482   0.02305397  0.27850557]\n",
      "    rssi_th [-81.99691797 -71.99393173 -54.99266255]\n",
      "    infect_w [0.00582939 0.03338104 0.30491117]\n"
     ]
    }
   ],
   "source": [
    "bag_size = [1, 2, 4, 8, 16, 32]\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "idx = 0\n",
    "\n",
    "auc_train_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_train_true = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_true = np.zeros((len(bag_size),n_trials))\n",
    "for bs in bag_size:\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=bs,censor_prob_pos=0.,censor_prob_neg=0,max_pos_in_bag=1)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "dDKN6jWJpNe3",
    "outputId": "639e6b02-86c1-48fb-eba9-ce6afa90d98e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa208325fa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the above two cells are deleted because of plotting redundancy\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(bag_size, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(bag_size, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: No Censoring\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(bag_size, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(bag_size, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: No Censoring\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHjxaD6-xNYo"
   },
   "source": [
    "### 2. Varying censoring probability\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "This experiment is iterated over censoring probabilities to show the effect that increasing censoring probability has on the performance of the learning algorithm. Increasing censoring probability would reduce accuracy in assigning a risk score, since there is less signal when some positive exposures are censored or not taken into account in learning the model. With less data, the algorithm will definitely be biased against calculating a higher risk score. \n",
    "\n",
    "We expect the performance of the algorithm to decrease, i.e. the AUC score to fall, as censoring probability increases. This is reflected in the below too sets of experiments. When we set the maximum bag size to be 16, two plots shows that the AUC drops from a high of around 0.8 to less than 0.6. We even try a differnt maximum bag size (32) to cross check, and again the AUC drops from 0.8 - 0.55. Overall, the trends for this experiment supports the claim being made, although it definitely begs the question of what is the point of making this claim and experiment, knowing that this is a fundamental design flaw that may not be helpful to pandemic efforts (e.g. being more cautious and compensating for potentially censored exposures, instead of just recognizing and accepting that the prediction and risk scoring will drop). A discussion on this is expounded on in part III.1, Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JQsRgrJw_y4",
    "outputId": "5b7518b0-95b1-437d-dd9d-f74f8580eba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.000, median size 3\n",
      "\t Negative bags: mean size 3.971, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17617) (17617, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3532, 17617) (884, 17617)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9485228239751493 step loss 0.9524340213955766\n",
      "iter 0 sigmoid loss 1.0343423184328464 step loss 0.6536440470489701 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.45214364711373617 step loss 0.42511188606857947 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3571010128038134 step loss 0.4195549973570463 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3435219386823791 step loss 0.41440946804973894 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35325216987076385 step loss 0.5087466194759144 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3700036360287546 step loss 1.145408216296316 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.35588216299828945 step loss 0.41873685690255147 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.37595680910828255 step loss 0.40436777788332756 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28164639033773736 step loss 0.40285287761041194 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.38987522206750796 step loss 0.40215533607564347 sigmoid temp 1.0\n",
      "End sigmoid loss 0.39344512772269913 step loss 0.40321511359522894\n",
      "    beta 0.2947369887567163\n",
      "    rssi_w [-0.02575168 -0.01621143  0.07142658  0.2794411 ]\n",
      "    rssi_th [13.98907232 21.98907775 34.98866313]\n",
      "    infect_w [0.00724824 0.03873066 0.28251154]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.14943818398294 step loss 1.1508897243176852\n",
      "iter 0 sigmoid loss 1.2557359585516663 step loss 0.5557833168387539 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4508196587804173 step loss 0.4275497969088081 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35547297680540507 step loss 0.42168086961587464 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3395045998501148 step loss 0.42230080035011863 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35170631784315 step loss 0.47126115663349716 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3880691986365569 step loss 0.503058190888692 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.41941342697912776 step loss 0.42516980241927976 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.476709972279983 step loss 0.4248801562847659 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3220774160970542 step loss 0.4245700188280031 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4499855058706702 step loss 0.4250450174758641 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4241541966654932 step loss 0.4246325811225136\n",
      "    beta 0.02832020345723263\n",
      "    rssi_w [0.24586174 0.27157564 0.35787701 0.17132981]\n",
      "    rssi_th [33.00311548 18.00314873 29.99882307]\n",
      "    infect_w [0.00710947 0.03132744 0.13540038]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1304133199294242 step loss 1.1377520168495279\n",
      "iter 0 sigmoid loss 1.2363686285478745 step loss 0.5831857728900853 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4496449356136363 step loss 0.4260296490993249 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35410033815079983 step loss 0.42073701360069365 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3371250959310653 step loss 0.4228881254935475 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3496888316439012 step loss 0.46747892965393 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37915072213385625 step loss 0.48816113861820476 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3604408903221612 step loss 0.47979283347073026 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.42064154651723357 step loss 0.46680568432696795 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.31073096103622805 step loss 0.45254805595411923 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.41764578651059964 step loss 0.4478320891827461 sigmoid temp 1.0\n",
      "End sigmoid loss 0.41308277781443875 step loss 0.44774814315593897\n",
      "    beta 0.23317912939125873\n",
      "    rssi_w [-0.06249766  0.01661313  0.12618638  0.19569143]\n",
      "    rssi_th [36.00253939 15.00238247 27.99636375]\n",
      "    infect_w [0.01196022 0.04126821 0.22465949]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.015521709248962 step loss 1.0236195851683034\n",
      "iter 0 sigmoid loss 1.1090438251954853 step loss 0.6510692594580176 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4472961125835201 step loss 0.4147531359000531 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3507394110809725 step loss 0.40375726370060594 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33003924076800023 step loss 0.5170095533094967 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3469403707402416 step loss 0.6085852197998672 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37593652643270414 step loss 0.5641841946140157 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.34329560749563526 step loss 0.5135708478286853 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.39130642785188924 step loss 0.40101083372908636 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2854403753106812 step loss 0.3938515238159006 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.38026866786094776 step loss 0.39093751699549295 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3849005996629174 step loss 0.39162909583420014\n",
      "    beta 0.22502367030542414\n",
      "    rssi_w [-0.03992202  0.05117245  0.16654142  0.13518303]\n",
      "    rssi_th [39.00866455 19.00843891 23.99886942]\n",
      "    infect_w [0.0068536  0.03650233 0.21240824]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.082907238827845 step loss 1.09193641848689\n",
      "iter 0 sigmoid loss 1.1831446035407525 step loss 0.5870042710141307 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44897088966852394 step loss 0.41376247431167207 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3508920728870821 step loss 0.39930739698736 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3272176638255874 step loss 0.43324625213717066 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.37183166583388627 step loss 0.4705377784714426 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.35940255824816547 step loss 0.4723046575423035 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3445822496398017 step loss 0.4176612468243527 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3856794766657225 step loss 0.4097568209336681 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28569697442994135 step loss 0.40228548046264695 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.35286496327226813 step loss 0.39312629071048066 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3761087762489566 step loss 0.3926933560458921\n",
      "    beta 0.2140592703618992\n",
      "    rssi_w [-0.03871878  0.02148719  0.04755239  0.23628181]\n",
      "    rssi_th [37.00740956 14.00732435 10.00771133]\n",
      "    infect_w [0.00617474 0.03486513 0.20947108]\n",
      "best loss 0.39162909583420014\n",
      "best scoring parameters\n",
      "    beta 0.22502367030542414\n",
      "    rssi_w [-0.03992202  0.01125043  0.17779185  0.31297488]\n",
      "    rssi_th [-80.99133545 -61.98289654 -37.98402712]\n",
      "    infect_w [0.0068536  0.04335592 0.25576416]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.044, median size 3\n",
      "\t Negative bags: mean size 3.999, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17753) (17753, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3533, 17753) (887, 17753)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0599834949409723 step loss 1.1062477182182773\n",
      "iter 0 sigmoid loss 1.4051344173269846 step loss 0.6107159758085565 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43534891794879216 step loss 0.42904225919784367 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41032156211156606 step loss 0.4211831537444761 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4855695857297 step loss 0.4364954725314619 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34451759182698827 step loss 0.993579087498757 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40491072670632616 step loss 1.0520217417097737 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31771335902707915 step loss 0.41456826248461537 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46659016913515244 step loss 0.3990308561022598 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.30898678638579113 step loss 0.4008058130243649 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.389049494861305 step loss 0.3980956753969632 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3900882935916594 step loss 0.39802476815877347\n",
      "    beta 0.2919483028078872\n",
      "    rssi_w [-0.01151736  0.04029357  0.26601983  0.08592018]\n",
      "    rssi_th [31.99408322 35.99393058 18.99959966]\n",
      "    infect_w [0.01145329 0.03194882 0.27620047]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0670571800086088 step loss 1.0623264794717502\n",
      "iter 0 sigmoid loss 1.4116791129770043 step loss 0.5488023859967738 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44038700731743147 step loss 0.43057351112953757 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41605959155227734 step loss 0.4245035909108968 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.49748894954635325 step loss 0.42108460273633647 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35562365338324325 step loss 0.4936779908743291 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4096309855039331 step loss 0.5187795456942368 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3693601646905407 step loss 0.42457615238991764 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5121919810437153 step loss 0.45714890179585477 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35674651166030247 step loss 0.43244054963694495 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.38383056655954023 step loss 0.4219241782839748 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4173506410105693 step loss 0.4212092914177328\n",
      "    beta 0.15634688993172433\n",
      "    rssi_w [-0.05108632 -0.00804121  0.07442572  0.20719607]\n",
      "    rssi_th [32.00888283 12.00885515 10.008674  ]\n",
      "    infect_w [0.01165317 0.02915182 0.15113625]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9900218081249702 step loss 0.9878817371270485\n",
      "iter 0 sigmoid loss 1.3060513544764973 step loss 0.5906340084967605 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4411355857669449 step loss 0.4326544955707184 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41700377693824026 step loss 0.4278583581849244 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5009678172858747 step loss 0.4254598334995291 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3603349956682841 step loss 0.485112322340791 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.41031594507744296 step loss 0.5201888679308462 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3714413946606516 step loss 0.504698254118722 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5183634630291492 step loss 0.48239529001687664 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3623522425378898 step loss 0.4249026489338522 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3897396113131552 step loss 0.4295295724610477 sigmoid temp 1.0\n",
      "End sigmoid loss 0.42103952877360656 step loss 0.42771110755550656\n",
      "    beta 0.183620429469633\n",
      "    rssi_w [-0.06142202 -0.01029964  0.07686148  0.15757274]\n",
      "    rssi_th [32.00763326 11.00756941 10.00738694]\n",
      "    infect_w [0.01266182 0.03220065 0.15987909]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9107695609239481 step loss 0.9211004573134386\n",
      "iter 0 sigmoid loss 1.2006807383621378 step loss 0.631883229726673 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4382706947719744 step loss 0.4266256245333012 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41350903398636646 step loss 0.4161222224171302 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4913200594844489 step loss 0.4068220773064525 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.348667910751725 step loss 0.6983159009674679 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4075734886155 step loss 0.7348744096513725 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3240645617208411 step loss 0.4093419681719954 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46504018102797884 step loss 0.3949912411679229 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3143934442026155 step loss 0.3970401889845131 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.37780717461402935 step loss 0.39258860990515343 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3850075575529071 step loss 0.39274639616054025\n",
      "    beta 0.2621638751059251\n",
      "    rssi_w [-0.02587634 -0.01999205  0.07087278  0.24118029]\n",
      "    rssi_th [14.00258364 29.0025847  21.00226237]\n",
      "    infect_w [0.01046306 0.0310202  0.24515122]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9636217882332689 step loss 0.96514180036121\n",
      "iter 0 sigmoid loss 1.2725944228160733 step loss 0.6066144225576261 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4377295245514257 step loss 0.42521745816455425 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4133443446584828 step loss 0.41433022918426404 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.48894942205642683 step loss 0.40212468340248025 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3474029450603537 step loss 0.5327690768355758 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.408254859672487 step loss 0.5449602094631594 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3435723855367614 step loss 0.40368418346344065 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.48264767449873447 step loss 0.397441853844039 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3246068385453376 step loss 0.3983566264532936 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3635080556042601 step loss 0.3966021563632778 sigmoid temp 1.0\n",
      "End sigmoid loss 0.39388078359715795 step loss 0.3966434229814081\n",
      "    beta 0.2259173942891323\n",
      "    rssi_w [-0.0416623  -0.00861539  0.07337219  0.20025934]\n",
      "    rssi_th [32.00927737 14.00924981 14.00915292]\n",
      "    infect_w [0.01037495 0.03249099 0.20572465]\n",
      "best loss 0.39274639616054025\n",
      "best scoring parameters\n",
      "    beta 0.2621638751059251\n",
      "    rssi_w [-0.02587634 -0.04586839  0.02500438  0.26618468]\n",
      "    rssi_th [-105.99741636  -76.99483166  -55.99256929]\n",
      "    infect_w [0.01046306 0.04148326 0.28663448]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.970, median size 3\n",
      "\t Negative bags: mean size 3.968, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17584) (17584, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3523, 17584) (884, 17584)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0091192812980763 step loss 1.0171784881714627\n",
      "iter 0 sigmoid loss 0.9615358359978212 step loss 0.7050878943806502 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4570312594234545 step loss 0.4448243319748371 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4613930570784137 step loss 0.4368654583817928 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4410105007238011 step loss 0.4346942147296875 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4449085487468312 step loss 0.6071722021043133 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.43372214227493067 step loss 1.0066692139456777 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3922170578826427 step loss 0.4257798638694882 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36243982700599 step loss 0.42283419323365734 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3550462737092562 step loss 0.4216491061736858 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.38809020990084536 step loss 0.4190891636895279 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4103341706022644 step loss 0.4197662291159915\n",
      "    beta 0.2685724704270086\n",
      "    rssi_w [-0.00712551  0.03621298  0.25114947  0.02801793]\n",
      "    rssi_th [28.99620539 37.99608298 37.99989344]\n",
      "    infect_w [0.01563272 0.0236943  0.25348238]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.049152173833648 step loss 1.0498266529794034\n",
      "iter 0 sigmoid loss 1.004021473765049 step loss 0.6420516187673501 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4570624570482974 step loss 0.451084093190618 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4633643529247766 step loss 0.4463916269720146 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.44109040562950513 step loss 0.4450100749690276 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44598335308238113 step loss 0.47419185480498144 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4359588816634663 step loss 0.49109751194013423 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.41428474077082583 step loss 0.48647493188989605 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.40143255203548506 step loss 0.4792421447447725 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3712569898075199 step loss 0.4735901721702228 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.4287315544860122 step loss 0.46107723614891816 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43725261080247857 step loss 0.46020117120608584\n",
      "    beta 0.2260951504322705\n",
      "    rssi_w [-0.03317102  0.02232079  0.08911527  0.19004575]\n",
      "    rssi_th [35.00058746 16.00051223 27.99716342]\n",
      "    infect_w [0.02437053 0.02479763 0.20749323]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9986099356031158 step loss 1.0105788815711287\n",
      "iter 0 sigmoid loss 0.9534682455165338 step loss 0.676352305853482 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4549225267159791 step loss 0.4436914660345952 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.46102273543632394 step loss 0.4346812051356985 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.44278493282760034 step loss 0.4239584508694613 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4437346875873551 step loss 0.41826053726787266 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4351591404914782 step loss 0.42319932619469575 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40076666317377985 step loss 0.4120463836205764 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36330989453288587 step loss 0.41310077012530094 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3532656456386959 step loss 0.41354408135854304 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.38694567799424745 step loss 0.41204247925037135 sigmoid temp 1.0\n",
      "End sigmoid loss 0.408252422621521 step loss 0.41271684545185744\n",
      "    beta 0.25942930198242775\n",
      "    rssi_w [-0.05664674  0.00276783  0.08105622  0.22761729]\n",
      "    rssi_th [36.00112004 11.00105374 18.00050734]\n",
      "    infect_w [0.01505614 0.02384547 0.24395094]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.0397123963081616 step loss 1.0359454659286025\n",
      "iter 0 sigmoid loss 0.9952559380828041 step loss 0.645200347451241 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.46050170917813094 step loss 0.45444529388153526 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.47547094223423664 step loss 0.4528024525770459 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4448117384159485 step loss 0.45284885542238945 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4568898844689771 step loss 0.4534924482863655 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4811908392399674 step loss 0.45335260815131234 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40299227278618427 step loss 0.4528990900158876 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4051636484221848 step loss 0.4528376358194307 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4050051151936872 step loss 0.453037931096741 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.43388693329971945 step loss 0.45282372976035773 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4528009903738666 step loss 0.45280110597814105\n",
      "    beta 0.14962466659819143\n",
      "    rssi_w [0.00534867 0.01094172 0.03378874 0.1094588 ]\n",
      "    rssi_th [15.00051979 10.00054239 14.00061888]\n",
      "    infect_w [0.01993644 0.0151893  0.12671971]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0664850666778414 step loss 1.0632348498290887\n",
      "iter 0 sigmoid loss 1.0151918477058215 step loss 0.6280593718103179 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4592750590553426 step loss 0.4543886229006539 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4742010140539216 step loss 0.4528903401506405 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4441011094783359 step loss 0.45316611031049475 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.45545634286189135 step loss 0.45464987005892565 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4762683976784608 step loss 0.45511956665112174 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40274466104395484 step loss 0.45308026171178434 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.40502184491797194 step loss 0.45280731182105705 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.40491108464324244 step loss 0.4529946054090768 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.43373918548998297 step loss 0.45282601196206806 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45277353147753197 step loss 0.45280007107418485\n",
      "    beta 0.16622680848167307\n",
      "    rssi_w [-0.01948132 -0.00832676  0.02009129  0.13530891]\n",
      "    rssi_th [13.00131    15.00132324 16.00137398]\n",
      "    infect_w [0.02242738 0.01710177 0.14286346]\n",
      "best loss 0.41271684545185744\n",
      "best scoring parameters\n",
      "    beta 0.25942930198242775\n",
      "    rssi_w [-0.05664674 -0.05387891  0.02717731  0.2547946 ]\n",
      "    rssi_th [-83.99887996 -72.99782621 -54.99731888]\n",
      "    infect_w [0.01505614 0.03890162 0.28285256]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.096, median size 4\n",
      "\t Negative bags: mean size 3.958, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17634) (17634, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3512, 17634) (879, 17634)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.902388605664256 step loss 0.9070246140432743\n",
      "iter 0 sigmoid loss 0.7241558607078243 step loss 0.718813812121556 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.49055629142088364 step loss 0.43204114920788733 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41648183874465444 step loss 0.42618520805447424 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43187041943915444 step loss 0.42350764609846864 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.33673175695719637 step loss 0.42902118640871656 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.43525049939971017 step loss 0.6125652034974723 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39476749309960224 step loss 0.46659848067406245 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3504156770294445 step loss 0.4250678395624062 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3776823278487054 step loss 0.42255149335775216 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5592986058864331 step loss 0.4211812926602127 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4168989310742883 step loss 0.4211099955583812\n",
      "    beta 0.19395527800215576\n",
      "    rssi_w [0.00090942 0.03254    0.16168495 0.04005769]\n",
      "    rssi_th [28.00577212 30.00571613 37.99989463]\n",
      "    infect_w [0.02256417 0.01922161 0.16899231]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.8337433031905223 step loss 0.8352257331258391\n",
      "iter 0 sigmoid loss 0.6701696433728042 step loss 0.693789519440687 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4919169740664433 step loss 0.43604900258489937 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4170034012719984 step loss 0.4316392173988014 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43160840237472187 step loss 0.42894434014740085 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3374151833971695 step loss 0.42685411957975505 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4345907151155172 step loss 0.4339665130572028 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40664024487599304 step loss 0.4246139812662994 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.35779331884029064 step loss 0.42233932318225764 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3874802029718667 step loss 0.4229940978608172 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5521448523976116 step loss 0.4219227480600603 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4138927125426052 step loss 0.42231905250485285\n",
      "    beta 0.2512865405173258\n",
      "    rssi_w [-0.02314682 -0.01733824  0.0781462   0.22050568]\n",
      "    rssi_th [10.99317894 30.99318184 26.99266361]\n",
      "    infect_w [0.02357733 0.02688842 0.23132508]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9437675731687688 step loss 0.9429834558221684\n",
      "iter 0 sigmoid loss 0.7578141972637173 step loss 0.6774755106290956 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4947849831945143 step loss 0.43691233001235974 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4182067950726893 step loss 0.4342226026056398 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4369429124230817 step loss 0.4333025823907796 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3428457514371722 step loss 0.4331499984515151 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.44204814389802644 step loss 0.43845937932836776 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4092696319561845 step loss 0.47438180934585955 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3437268120898675 step loss 0.47091061712031207 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3949515985875924 step loss 0.46203252996048644 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.5639393112901012 step loss 0.44000627145287435 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4292607761268049 step loss 0.4376260251472908\n",
      "    beta 0.1718736048455071\n",
      "    rssi_w [-0.00449395 -0.00559749  0.01534216  0.14319203]\n",
      "    rssi_th [11.00451968 12.00453178 29.00453209]\n",
      "    infect_w [0.02466868 0.02067534 0.14287634]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9799640407576043 step loss 0.9697776644365664\n",
      "iter 0 sigmoid loss 0.7876691247090409 step loss 0.6890607696721578 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4926341669459019 step loss 0.4396946857162193 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41750314596419796 step loss 0.4372063355999314 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4340642676788278 step loss 0.43657272471780023 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34080081894701636 step loss 0.4361515295940306 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4383829580693521 step loss 0.43569686698537613 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4272689670660425 step loss 0.43546193210359696 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3490875785363643 step loss 0.43483447986768653 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4203700976630776 step loss 0.4346885223986256 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5583264551182314 step loss 0.43481660565114405 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43350841571196264 step loss 0.4346455675254362\n",
      "    beta 0.22289415707300578\n",
      "    rssi_w [-0.03407153 -0.03412527  0.12764422  0.15161307]\n",
      "    rssi_th [12.00009793 34.00010181 31.99781381]\n",
      "    infect_w [0.03559379 0.02441238 0.19825241]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.929281172290601 step loss 0.9347226574695162\n",
      "iter 0 sigmoid loss 0.7492220474968144 step loss 0.7047325825809073 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4919797591122175 step loss 0.44009635331160546 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4172991690448767 step loss 0.4380714082056613 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43385145764772015 step loss 0.43800635935500876 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3403227244479004 step loss 0.4387340891381862 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.43817508947658185 step loss 0.44021273654145143 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4193381290487607 step loss 0.440456268868668 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34356428627545094 step loss 0.43825070479047995 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4083786121460907 step loss 0.4375553125700773 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.556668310210864 step loss 0.4375357264836558 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43191327123962625 step loss 0.43727848027506444\n",
      "    beta 0.2219142470310075\n",
      "    rssi_w [-0.04144339 -0.03500091  0.14544069  0.13048948]\n",
      "    rssi_th [11.00214576 38.00214534 31.99850534]\n",
      "    infect_w [0.03555179 0.02664454 0.19672933]\n",
      "best loss 0.4211099955583812\n",
      "best scoring parameters\n",
      "    beta 0.19395527800215576\n",
      "    rssi_w [0.00090942 0.03344942 0.19513437 0.23519206]\n",
      "    rssi_th [-91.99422788 -61.98851175 -23.98861712]\n",
      "    infect_w [0.02256417 0.04178578 0.21077809]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.787, median size 3\n",
      "\t Negative bags: mean size 3.973, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17471) (17471, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3501, 17471) (879, 17471)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9921783371561332 step loss 1.0173855798188418\n",
      "iter 0 sigmoid loss 0.9269219750114297 step loss 0.6837216725243115 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.32729082454972414 step loss 0.447413749704823 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.469697960861379 step loss 0.44175749928384195 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4329845844923987 step loss 0.4458342986412993 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.39460034159754187 step loss 0.5710946511257343 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4014219596999531 step loss 0.6729878219088087 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.47186534136545505 step loss 0.44868155300328794 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5589760550743925 step loss 0.4394366931665223 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.36744989880866813 step loss 0.43676535935872834 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4774994539596357 step loss 0.43605662296477465 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43233149377289454 step loss 0.43604196601607687\n",
      "    beta 0.202078935859673\n",
      "    rssi_w [0.00243876 0.0302765  0.09492518 0.1512279 ]\n",
      "    rssi_th [31.00132295 26.00128486 15.99817122]\n",
      "    infect_w [0.01928202 0.02754398 0.17879142]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0270578280813023 step loss 1.0274578093088536\n",
      "iter 0 sigmoid loss 0.9571048151387903 step loss 0.6678341318614818 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3292984928623415 step loss 0.45009908827099887 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4734489246221591 step loss 0.447002506486519 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43875549023993315 step loss 0.4460268773126735 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3971715961938319 step loss 0.44797486675768605 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4010728695519595 step loss 0.4873292246314941 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.46807274406255034 step loss 0.4883904470346781 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5765911302125063 step loss 0.47915065145357877 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.38568898716039995 step loss 0.4543885777305441 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.49693555174565757 step loss 0.447442200677125 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44327288054591835 step loss 0.4467226480928144\n",
      "    beta 0.18739755030795494\n",
      "    rssi_w [-0.00300147  0.01735094  0.09389586  0.13374406]\n",
      "    rssi_th [26.00158731 26.00157192 26.9986949 ]\n",
      "    infect_w [0.02459057 0.02637592 0.16096391]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1034456701829443 step loss 1.1117209417511473\n",
      "iter 0 sigmoid loss 1.0288873465198909 step loss 0.6032901547997278 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3290518010780096 step loss 0.445942669284933 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4733335666242821 step loss 0.4403782418276905 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4390823592786521 step loss 0.43797440790915143 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3977876713684324 step loss 0.43573631484728753 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40110402464842354 step loss 0.4569443287891918 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.46750094578318674 step loss 0.43617701123858843 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5536453193468777 step loss 0.432975847976988 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3547087568689131 step loss 0.43184731342689153 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4684117314478516 step loss 0.4315249023137672 sigmoid temp 1.0\n",
      "End sigmoid loss 0.42869919936636985 step loss 0.43169628551925854\n",
      "    beta 0.1999487820285694\n",
      "    rssi_w [-0.00169242  0.04222482  0.17627796  0.0261435 ]\n",
      "    rssi_th [35.00407228 26.00394061 38.99993296]\n",
      "    infect_w [0.01834921 0.02520295 0.17672264]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.898936323363491 step loss 0.9020127542934837\n",
      "iter 0 sigmoid loss 0.8383129717419874 step loss 0.6667525995413653 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3290484027898134 step loss 0.4469550072496422 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.47240470915674854 step loss 0.44137771320775715 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43607405796312987 step loss 0.43701994760852786 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3953448952261091 step loss 0.43239025782956153 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4000471098803534 step loss 0.43328609087354586 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4660450139639387 step loss 0.4295922830538131 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5557301585050974 step loss 0.42887979643848245 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35871067641682686 step loss 0.42892401302489136 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.46097907350054973 step loss 0.4290972989995316 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4252629625814993 step loss 0.42919212006514657\n",
      "    beta 0.22124534320978598\n",
      "    rssi_w [-0.03638763  0.00101764  0.07510408  0.18647587]\n",
      "    rssi_th [33.00156364 15.00153722 16.00124738]\n",
      "    infect_w [0.01790256 0.02809339 0.19991486]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.8911573584412767 step loss 0.894239624125361\n",
      "iter 0 sigmoid loss 0.8312106161767413 step loss 0.689086468409968 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33089490178206377 step loss 0.45184317934002316 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.47667432835912565 step loss 0.4490469656299259 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.44214666790780127 step loss 0.44852108393065737 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3986710137758685 step loss 0.4473048206825218 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40317716055479047 step loss 0.4534008103795817 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4617861002945912 step loss 0.4452476835167671 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5875028729049019 step loss 0.4449817558279512 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3923250891050699 step loss 0.444744534754929 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4868462528393252 step loss 0.44475480103905546 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4413596786285821 step loss 0.4450004882973234\n",
      "    beta 0.2202726240823182\n",
      "    rssi_w [ 0.00022603 -0.0007246   0.05456782  0.19549985]\n",
      "    rssi_th [ 9.99509767 24.99510684 37.99487477]\n",
      "    infect_w [0.02286324 0.03503979 0.19672156]\n",
      "best loss 0.42919212006514657\n",
      "best scoring parameters\n",
      "    beta 0.22124534320978598\n",
      "    rssi_w [-0.03638763 -0.03536999  0.03973409  0.22620996]\n",
      "    rssi_th [-86.99843636 -71.99689914 -55.99565176]\n",
      "    infect_w [0.01790256 0.04599595 0.24591082]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.725, median size 3\n",
      "\t Negative bags: mean size 4.014, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17580) (17580, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3492, 17580) (873, 17580)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9950644156091666 step loss 1.012646853090109\n",
      "iter 0 sigmoid loss 1.0735260363205448 step loss 0.6541289556518118 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3654219048225363 step loss 0.4596528135411609 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4831919534176488 step loss 0.4562648007985465 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5363940931969442 step loss 0.4654407499868636 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4614557256265476 step loss 0.5922264458049383 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.46978525909034263 step loss 0.6420901484225616 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32778899081258855 step loss 0.4711623330887816 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.44734698737486767 step loss 0.4519738967776688 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4759911488787735 step loss 0.4506514079080209 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5029878105587683 step loss 0.4504755988233535 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4481748235490813 step loss 0.4502061613989258\n",
      "    beta 0.18634199244461627\n",
      "    rssi_w [0.0071482  0.02470378 0.07567104 0.14387545]\n",
      "    rssi_th [28.0015476  28.00152858 15.99865068]\n",
      "    infect_w [0.0348379  0.02996212 0.15582672]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0645818463258092 step loss 1.079394937692549\n",
      "iter 0 sigmoid loss 1.1456306067517632 step loss 0.6467044298579331 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36573104537245504 step loss 0.46339362456023425 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.48342297591269767 step loss 0.46091972590621433 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5368512927908423 step loss 0.46406811628620115 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.46097566759494285 step loss 0.47454843661941737 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4686033910039963 step loss 0.5139910418876927 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3330454303347901 step loss 0.4609062729581079 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46523466002313635 step loss 0.4544839117035226 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.46503960542536843 step loss 0.4542186067445012 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5217953975470853 step loss 0.4543675508268044 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4500246961435221 step loss 0.45409405115101803\n",
      "    beta 0.20770542735460965\n",
      "    rssi_w [-0.00035298  0.04724827  0.17478361  0.05836451]\n",
      "    rssi_th [33.99575933 34.99561849 16.99974228]\n",
      "    infect_w [0.03498191 0.03907104 0.17853957]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.964833756816844 step loss 0.9620848336973198\n",
      "iter 0 sigmoid loss 1.0389925443581312 step loss 0.6348913103309652 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3669916036654247 step loss 0.46098677758397366 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4873985337965553 step loss 0.45795517719108964 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5433998271313825 step loss 0.45646563346429314 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4753091680130083 step loss 0.4641168766616103 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.47172451946311583 step loss 0.5602415493076168 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.33463243915451957 step loss 0.536221644175283 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4463432473869466 step loss 0.45896087908162747 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4886280479348005 step loss 0.4565956065011295 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5037251765639997 step loss 0.45609249818149655 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4551672275198782 step loss 0.4558250747732186\n",
      "    beta 0.15606698216809037\n",
      "    rssi_w [-0.00550537  0.00963967  0.03718686  0.12374464]\n",
      "    rssi_th [20.00477552 11.00478126 24.00475025]\n",
      "    infect_w [0.03379308 0.02530598 0.11902364]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9507303685143156 step loss 0.9747558112119313\n",
      "iter 0 sigmoid loss 1.026581021931174 step loss 0.674544726588081 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3648780182082942 step loss 0.46045185612268985 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.48210204537383355 step loss 0.45772105104247895 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5350158613243763 step loss 0.4695010027997829 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4590246038717109 step loss 0.7226671638128339 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4690405818001785 step loss 0.7662299406062077 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32584986133533733 step loss 0.4560760489842857 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.44972455299618724 step loss 0.44809053435853197 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4750980115120734 step loss 0.4476500208937995 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.513859067710874 step loss 0.44788982228389684 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4444806046209674 step loss 0.4475494362985874\n",
      "    beta 0.1949460476906694\n",
      "    rssi_w [0.0076947  0.03256419 0.14218536 0.09548   ]\n",
      "    rssi_th [32.0011637  30.00111571 14.9996392 ]\n",
      "    infect_w [0.03118139 0.02945028 0.16688242]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0526877450143948 step loss 1.0761272446630075\n",
      "iter 0 sigmoid loss 1.1316283220036987 step loss 0.6047454335499178 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3665577523534743 step loss 0.4601971191731279 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.48625430097737116 step loss 0.4558234594918321 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5410294528965572 step loss 0.4544763872328382 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4676688215297252 step loss 0.46199424781405046 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4690886414704405 step loss 0.6575337008894859 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3250775610652015 step loss 0.4578395107000471 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4534267208546979 step loss 0.4475654985446088 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.47008423879156574 step loss 0.44731628381722427 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.5183449980710413 step loss 0.44751794466521644 sigmoid temp 1.0\n",
      "End sigmoid loss 0.444027607194745 step loss 0.447189863219822\n",
      "    beta 0.1979141678847384\n",
      "    rssi_w [0.00247538 0.00873832 0.03193863 0.17947988]\n",
      "    rssi_th [10.00035868 20.00036399 34.00031369]\n",
      "    infect_w [0.03017734 0.03241795 0.16946648]\n",
      "best loss 0.447189863219822\n",
      "best scoring parameters\n",
      "    beta 0.1979141678847384\n",
      "    rssi_w [0.00247538 0.0112137  0.04315233 0.22263222]\n",
      "    rssi_th [-109.99964132  -89.99927734  -55.99896365]\n",
      "    infect_w [0.03017734 0.06259529 0.23206177]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.854, median size 3\n",
      "\t Negative bags: mean size 4.031, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17737) (17737, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3503, 17737) (876, 17737)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9305103147358927 step loss 0.9337027008608452\n",
      "iter 0 sigmoid loss 1.1188505394404797 step loss 0.6298369441454157 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43863333933520016 step loss 0.47023422658365005 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.45090296633245464 step loss 0.4699071920731872 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3938175327282301 step loss 0.4693750792702582 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4444856692249602 step loss 0.47099154963092166 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.47139684255913944 step loss 0.4710584894227841 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4552011256543315 step loss 0.46973897459544167 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.467974674974956 step loss 0.4692967718525514 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.40048522538732895 step loss 0.46916989001668385 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.48509155925616626 step loss 0.4692366805153945 sigmoid temp 1.0\n",
      "End sigmoid loss 0.468095651497619 step loss 0.46917969915945995\n",
      "    beta 0.15081064013978676\n",
      "    rssi_w [-0.01404111 -0.01539842  0.02173585  0.11412656]\n",
      "    rssi_th [13.00217513 23.00218222 12.00218891]\n",
      "    infect_w [0.05146164 0.0151069  0.10626142]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9783905657696419 step loss 0.977363479856561\n",
      "iter 0 sigmoid loss 1.1753600261246653 step loss 0.6360027815905505 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4375717239916106 step loss 0.47051027939427215 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.45068360055836104 step loss 0.4697509790936517 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3908917901068723 step loss 0.46902762935051556 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44288873302202836 step loss 0.47005469090172525 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.46593852145069986 step loss 0.469842192942888 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4569536789398934 step loss 0.47006460393181004 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4639990495158676 step loss 0.4693212786918911 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3953482664646545 step loss 0.46898087654295745 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47502680449679474 step loss 0.4691659821413699 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46578494050342284 step loss 0.4690494088627622\n",
      "    beta 0.17446013750023184\n",
      "    rssi_w [-0.02464478 -0.01658293  0.11966385  0.08070362]\n",
      "    rssi_th [18.00227707 31.00227531 30.99934227]\n",
      "    infect_w [0.05941076 0.01877467 0.13452496]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.973755053916082 step loss 0.9697100564272692\n",
      "iter 0 sigmoid loss 1.1734622936010588 step loss 0.6274709039436911 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4382853746132894 step loss 0.4693539294281288 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4506428833739702 step loss 0.4675708101697039 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.39162106067303426 step loss 0.46694486703852744 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44329084071618213 step loss 0.46605542783943565 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.46632209737949554 step loss 0.4657341367644723 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.45311289692532825 step loss 0.4650103655950317 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46237436668484483 step loss 0.46484699087895587 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.39247852019912743 step loss 0.4647739045959348 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47628314946056904 step loss 0.46468493961525265 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46383277003317186 step loss 0.4646811464213035\n",
      "    beta 0.17541367585961468\n",
      "    rssi_w [-0.00826179  0.00939047  0.06866879  0.13081521]\n",
      "    rssi_th [26.99857092 14.99857918 30.99819578]\n",
      "    infect_w [0.05266577 0.01980319 0.13869218]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9812416158866549 step loss 0.9858779557326423\n",
      "iter 0 sigmoid loss 1.178415803829379 step loss 0.6339396208161198 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4380748705507992 step loss 0.46891699459767494 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.45067128146570345 step loss 0.46678025973091053 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3911179671991769 step loss 0.4660798401491497 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4429603185695652 step loss 0.46487939841975884 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4651819899287035 step loss 0.4645094198594816 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4529272249231859 step loss 0.4635139138581381 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4611112640966595 step loss 0.46330016671475965 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.388531184593112 step loss 0.4631881808520622 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4756053942593418 step loss 0.4631021735782821 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46234005953145413 step loss 0.4630884433632045\n",
      "    beta 0.17890276154665088\n",
      "    rssi_w [-0.01030565  0.00993532  0.06763728  0.13726195]\n",
      "    rssi_th [29.99898801 12.99899493 27.99858342]\n",
      "    infect_w [0.04984021 0.01976952 0.1425016 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.093545430903593 step loss 1.0976030172074902\n",
      "iter 0 sigmoid loss 1.3135583069155263 step loss 0.6228266520493595 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43812502173781936 step loss 0.4673543615012976 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.45097498236679223 step loss 0.46447293426605707 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3894926527534832 step loss 0.46391891708384536 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44203659787084193 step loss 0.4627456615376147 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4631908877663934 step loss 0.46506362321575057 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4538346569748539 step loss 0.4623905159363272 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46368895122225795 step loss 0.461520624692847 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3779325483052229 step loss 0.461169210732971 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4797961508712353 step loss 0.46105185748543853 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45963792481654603 step loss 0.46102484376426656\n",
      "    beta 0.17375233201933896\n",
      "    rssi_w [0.00905162 0.05287537 0.13475946 0.01689901]\n",
      "    rssi_th [39.00077323 24.00061347 35.99995513]\n",
      "    infect_w [0.04123245 0.01777155 0.14197923]\n",
      "best loss 0.46102484376426656\n",
      "best scoring parameters\n",
      "    beta 0.17375233201933896\n",
      "    rssi_w [0.00905162 0.06192699 0.19668645 0.21358546]\n",
      "    rssi_th [-80.99922677 -56.99861331 -20.99865817]\n",
      "    infect_w [0.04123245 0.059004   0.20098323]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.028, median size 3\n",
      "\t Negative bags: mean size 4.039, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17890) (17890, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3489, 17890) (864, 17890)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.8777273930922844 step loss 0.880192874593367\n",
      "iter 0 sigmoid loss 0.9566767614042072 step loss 0.6564652261532506 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5253813903295778 step loss 0.4570790845520184 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42830688622974494 step loss 0.4556820117083965 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1500 sigmoid loss 0.4627952730710912 step loss 0.45516953869446297 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4921212728842522 step loss 0.4549555499055795 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38845123482287675 step loss 0.45526154352456377 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.47355223133692315 step loss 0.4552503392758387 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46096018927304555 step loss 0.45500551152998175 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4291620033327601 step loss 0.45491197087525537 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4751036484476478 step loss 0.4548666263673317 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45348202575275753 step loss 0.4548140698241487\n",
      "    beta 0.14728609837023612\n",
      "    rssi_w [0.00731248 0.01810238 0.0530715  0.09736733]\n",
      "    rssi_th [29.00160072 18.00160384 13.00162932]\n",
      "    infect_w [0.05039158 0.00865997 0.10098917]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9529191524880816 step loss 0.9592558814167672\n",
      "iter 0 sigmoid loss 1.0367800971915972 step loss 0.6382908660567891 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5260639471350342 step loss 0.45673070217867073 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4286933601643906 step loss 0.45508898605515363 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.46268530003331854 step loss 0.4542705530605495 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49242573810027246 step loss 0.4535455751053015 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38868950798012974 step loss 0.45321414385774184 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4759505023594929 step loss 0.45143743867939545 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4565122786878314 step loss 0.45099637794702585 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.42225567957504917 step loss 0.4507425498054632 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47760837673619183 step loss 0.4506812338575852 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44995066171262427 step loss 0.4506283274243954\n",
      "    beta 0.16989530232370811\n",
      "    rssi_w [0.01213265 0.01469823 0.03623819 0.13540469]\n",
      "    rssi_th [25.99925863  9.99927661 32.99920824]\n",
      "    infect_w [0.05879413 0.01367295 0.12841263]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9049522700138144 step loss 0.9126308526028939\n",
      "iter 0 sigmoid loss 0.9863220184474112 step loss 0.6531845656771288 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.524837339351982 step loss 0.45660983833138274 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4280235500797482 step loss 0.4548942275120709 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4625420063155596 step loss 0.45396978014320727 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49097321038528163 step loss 0.45316658381515146 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38810167528407663 step loss 0.45262876026750487 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4749003647574081 step loss 0.45197322864016104 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4567370024919809 step loss 0.45144081073404374 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.421250777817563 step loss 0.45110047305052137 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47673217008999413 step loss 0.4508074968262059 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44977086674076955 step loss 0.45060527981062687\n",
      "    beta 0.16713812783498697\n",
      "    rssi_w [0.00790698 0.02221856 0.03989189 0.12961505]\n",
      "    rssi_th [35.99967047 13.99967091 17.99975307]\n",
      "    infect_w [0.05505863 0.01467598 0.12580068]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.999330405175377 step loss 1.0103048439599616\n",
      "iter 0 sigmoid loss 1.0820979818885035 step loss 0.6413962074520848 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.524513500686999 step loss 0.4579521561396509 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42785895621513703 step loss 0.4568450077228733 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4624966162278007 step loss 0.45655298119616594 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4912322851541331 step loss 0.45662311618705215 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.388285933316554 step loss 0.4571297561345019 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.47379866132620696 step loss 0.45765723492224636 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4627808913125241 step loss 0.4574904586278754 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.42227637328079565 step loss 0.4568201786168088 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4783643152108902 step loss 0.4562981791413302 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4549348747016768 step loss 0.455953430606858\n",
      "    beta 0.1524875393148605\n",
      "    rssi_w [0.02116223 0.0258964  0.05469208 0.10194352]\n",
      "    rssi_th [27.9998728  23.99987226 25.99928315]\n",
      "    infect_w [0.05902218 0.01140717 0.10343262]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.059071450994615 step loss 1.073481603910481\n",
      "iter 0 sigmoid loss 1.1476513227041918 step loss 0.6410551683683031 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5243366746589527 step loss 0.45723358561389016 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42776387780379205 step loss 0.4557162965215706 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.46235875062669973 step loss 0.45524237515519034 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4903099513965333 step loss 0.4550117166766253 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38808982109180024 step loss 0.4553635137804799 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4758340432116268 step loss 0.4522144687323552 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4567365346854237 step loss 0.4518595848521039 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.42254443066768965 step loss 0.4516615571090528 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47843848422204543 step loss 0.45163137669979786 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45045089954136036 step loss 0.4515903059313341\n",
      "    beta 0.1714659920131311\n",
      "    rssi_w [0.02799005 0.03368496 0.13269333 0.03104774]\n",
      "    rssi_th [32.99901361 36.99896036 22.999895  ]\n",
      "    infect_w [0.06189439 0.01348182 0.12946966]\n",
      "best loss 0.45060527981062687\n",
      "best scoring parameters\n",
      "    beta 0.16713812783498697\n",
      "    rssi_w [0.00790698 0.03012555 0.07001744 0.19963249]\n",
      "    rssi_th [-84.00032953 -70.00065861 -52.00090554]\n",
      "    infect_w [0.05505863 0.06973462 0.19553529]\n"
     ]
    }
   ],
   "source": [
    "# max bag size 16\n",
    "censor_prob = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "idx = 0\n",
    "\n",
    "auc_train_learned = np.zeros((len(censor_prob),n_trials))\n",
    "auc_train_true = np.zeros((len(censor_prob),n_trials))\n",
    "auc_test_learned = np.zeros((len(censor_prob),n_trials))\n",
    "auc_test_true = np.zeros((len(censor_prob),n_trials))\n",
    "for prob in censor_prob:\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=16,censor_prob_pos=prob,censor_prob_neg=0,max_pos_in_bag=1)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "ghkOYE-MxQtY",
    "outputId": "82baf244-db5f-4687-85d5-236726cdf9f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1c921dca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(censor_prob, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(censor_prob, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Censor Probability\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: Max Bag Size = 16\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(censor_prob, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(censor_prob, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Censor Probability\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: Max Bag Size = 16\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4a2fveXIxlcE",
    "outputId": "990809dc-72d9-472e-b47d-5acaa1f45f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.287, median size 3\n",
      "\t Negative bags: mean size 4.009, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17962) (17962, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3539, 17962) (886, 17962)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9118934098450943 step loss 0.909669808186479\n",
      "iter 0 sigmoid loss 0.6813132590804648 step loss 0.7224957709916452 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42864037965183344 step loss 0.4251824955255295 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35591760806873546 step loss 0.4196330665695337 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.49162052755086877 step loss 0.41328101346893126 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.42485674607065205 step loss 0.4216879154718225 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3722794338677258 step loss 1.1317223003385162 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31748749229604406 step loss 0.4150162074509731 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.40417024974072224 step loss 0.4011409212391632 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.41871511007971535 step loss 0.3999600919866094 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31842775698571024 step loss 0.4040927848602059 sigmoid temp 1.0\n",
      "End sigmoid loss 0.390089913120393 step loss 0.4014798039846761\n",
      "    beta 0.2981239439174942\n",
      "    rssi_w [-0.02881316 -0.00117054  0.05825849  0.28388062]\n",
      "    rssi_th [19.98995763 12.98996206 37.98967797]\n",
      "    infect_w [0.00900965 0.03121122 0.28614133]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1497703253310148 step loss 1.174294552387767\n",
      "iter 0 sigmoid loss 0.8655076600989596 step loss 0.6337094162928667 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4258412556429518 step loss 0.41937980813968145 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35119335200132423 step loss 0.4104788041451449 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47470519842184195 step loss 0.3976514349940032 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.41071392049124883 step loss 0.3946224286390281 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3715305949935344 step loss 0.41600823111799035 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3058377415822132 step loss 0.38326474323395154 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.38204560604350424 step loss 0.38245973918063153 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.38997595062561835 step loss 0.382128668378131 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.28773397777850157 step loss 0.38647014271639407 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3733260645380961 step loss 0.3838017198514066\n",
      "    beta 0.27868130308684724\n",
      "    rssi_w [-0.04905825 -0.02417877  0.09577571  0.25675278]\n",
      "    rssi_th [26.00029499 21.00028112 18.99892468]\n",
      "    infect_w [0.00779567 0.02798425 0.26789786]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9227242049054967 step loss 0.9237348673029135\n",
      "iter 0 sigmoid loss 0.6907250483588215 step loss 0.7222826912634355 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42556791298080526 step loss 0.42020876054859263 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3501606227479393 step loss 0.41103617708766876 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.46939825489370735 step loss 0.4033234127370597 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.40845050881944883 step loss 0.4050296808996737 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3710327462645864 step loss 0.40918198022448926 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.307846735081686 step loss 0.3974592885837757 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3805091993979822 step loss 0.39207751990783124 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.398161963829178 step loss 0.388963195594134 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2936414400328864 step loss 0.3926500146697445 sigmoid temp 1.0\n",
      "End sigmoid loss 0.37297687685611813 step loss 0.3900426466031585\n",
      "    beta 0.2726467363516448\n",
      "    rssi_w [-0.0403939   0.01818501  0.0535601   0.26725361]\n",
      "    rssi_th [36.99786807 12.99778893 16.99793344]\n",
      "    infect_w [0.00776285 0.02819645 0.26256143]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9982740812919559 step loss 1.0097173266836759\n",
      "iter 0 sigmoid loss 0.7455446582472015 step loss 0.7447278628276126 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42423375382082157 step loss 0.41445698728247304 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35026711331265764 step loss 0.40305672088780253 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47507111087436116 step loss 0.39609442585240884 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.41234194766211824 step loss 0.7178653503526092 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3714231616779135 step loss 0.7900546154341493 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3109240951592708 step loss 0.43056306123469784 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.39237632510502185 step loss 0.3840170891833283 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.390416816555756 step loss 0.3810514488853801 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2749145523572135 step loss 0.38279029122552805 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3762288993019626 step loss 0.3814475744201258\n",
      "    beta 0.24707054019069685\n",
      "    rssi_w [-0.01005645  0.03055713  0.22518549  0.06520085]\n",
      "    rssi_th [31.00740738 31.00730698 30.99973133]\n",
      "    infect_w [0.00719336 0.02494017 0.23522369]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0830921233415653 step loss 1.1022109753817362\n",
      "iter 0 sigmoid loss 0.8102095900865767 step loss 0.7062607766314764 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4221856679340618 step loss 0.4175881577785034 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34780115001891226 step loss 0.4067512777955055 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.46587906604801593 step loss 0.40998638398813847 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4061901041829323 step loss 0.8094763266137135 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3686361850450399 step loss 0.8835916594964472 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40385603234024287 step loss 0.4349540637002899 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.44284444834901265 step loss 0.42818054842871384 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.45304129869464077 step loss 0.416762289031831 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3183842326999284 step loss 0.39954799089426213 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3833593340910248 step loss 0.3867271934148009\n",
      "    beta 0.07789904671030567\n",
      "    rssi_w [0.01751365 0.07240997 0.3029526  0.09553433]\n",
      "    rssi_th [35.99924476 27.99901623 20.99960752]\n",
      "    infect_w [0.01013795 0.05371529 0.31047018]\n",
      "best loss 0.3814475744201258\n",
      "best scoring parameters\n",
      "    beta 0.24707054019069685\n",
      "    rssi_w [-0.01005645  0.02050068  0.24568617  0.31088702]\n",
      "    rssi_th [-88.99259262 -57.98528564 -26.98555431]\n",
      "    infect_w [0.00719336 0.03213352 0.26735722]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.018, median size 3\n",
      "\t Negative bags: mean size 4.099, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 18104) (18104, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3537, 18104) (884, 18104)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0105839180604048 step loss 1.0158629689557725\n",
      "iter 0 sigmoid loss 1.0200794858969855 step loss 0.686096997575165 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.436428643180973 step loss 0.42803830911001073 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.36950109971365025 step loss 0.4196116580563786 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3531968859765589 step loss 0.42159450032117274 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.45075324023855967 step loss 0.5990451949748531 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.444062340073537 step loss 0.641680364711601 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3475543826639715 step loss 0.533935468938191 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.37213341013094436 step loss 0.42603830184011793 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.43749801369477326 step loss 0.41519316920420074 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.32586533889966113 step loss 0.412635354219877 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4089960427030807 step loss 0.41288195658665433\n",
      "    beta 0.20245363613048398\n",
      "    rssi_w [-0.02043003  0.03999343  0.16491407  0.08063926]\n",
      "    rssi_th [33.0099112  24.00979046 34.99962448]\n",
      "    infect_w [0.0074555  0.03742398 0.18014548]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0509808439587844 step loss 1.0624912637332433\n",
      "iter 0 sigmoid loss 1.0620104545632159 step loss 0.6919799742447202 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43708465341641306 step loss 0.44007726684970744 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3692436964218237 step loss 0.4366518657123465 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3540883287474905 step loss 0.4367854173780225 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4543005508205465 step loss 0.4433758145119965 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4503091204660633 step loss 0.4751853183848075 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.35885244080816286 step loss 0.4369952495199496 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.41264339710329523 step loss 0.4343200979286101 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4532139339427711 step loss 0.43283217822309594 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3422971378020802 step loss 0.43258681601429255 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4294807781401373 step loss 0.43263333376568935\n",
      "    beta 0.27043038202935177\n",
      "    rssi_w [-0.0472705   0.08987954  0.22815365  0.06080994]\n",
      "    rssi_th [38.9953336  38.99443632 16.9993819 ]\n",
      "    infect_w [0.00971815 0.05254384 0.24949428]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9911838567962058 step loss 0.9888170679475695\n",
      "iter 0 sigmoid loss 0.9964051923229036 step loss 0.6649731362317823 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4387127328371072 step loss 0.44170168730173043 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.37433287880530125 step loss 0.43952711161187896 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35844025135195534 step loss 0.4385291837382651 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.46356807822971674 step loss 0.44021591818924943 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4498654286041603 step loss 0.4493531816408822 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3433725454672219 step loss 0.43819605748556684 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.41147545201028884 step loss 0.4364171986696831 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4625232390408203 step loss 0.4372105247372046 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3407851223559065 step loss 0.43605673688317437 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43193153707596843 step loss 0.4368200147315756\n",
      "    beta 0.28911491206363704\n",
      "    rssi_w [-0.08828627 -0.06492814  0.18730441  0.17966138]\n",
      "    rssi_th [21.00431314 26.00430221 34.99687293]\n",
      "    infect_w [0.01197882 0.07054941 0.26559154]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.0722462776188868 step loss 1.087536373108656\n",
      "iter 0 sigmoid loss 1.082061308842106 step loss 0.6621553469796533 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4355679145737295 step loss 0.4350671479458526 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.36439230689888374 step loss 0.4293991147644412 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.34853039148584597 step loss 0.44100326860375316 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.441526433159095 step loss 1.1108620649785417 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.44549578029016146 step loss 1.2282347550509387 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.36079720073445676 step loss 0.42781821548921306 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.39451546655024616 step loss 0.4206543866588788 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4044165893378816 step loss 0.41812609708219667 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3096964394498815 step loss 0.41776163669813265 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4053726118538696 step loss 0.41889227336738716\n",
      "    beta 0.2890584461206046\n",
      "    rssi_w [-0.03189624  0.0607351   0.26187921  0.06148749]\n",
      "    rssi_th [36.98876955 33.98838765 21.9995927 ]\n",
      "    infect_w [0.00893046 0.04231932 0.27286585]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9931518853613719 step loss 0.9984355428424393\n",
      "iter 0 sigmoid loss 1.003789914087546 step loss 0.6830414039559528 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43660947285282203 step loss 0.44018274816943437 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3688116799525528 step loss 0.4371553712052097 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3532287219350809 step loss 0.4382531709641948 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.452835449165738 step loss 0.4484568655498046 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.44984590138214714 step loss 0.49653446687666275 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.35821580836445704 step loss 0.4362251359115219 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.41273506099956025 step loss 0.4339643322189932 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.45353856630674955 step loss 0.4326363226370431 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.34239929789477636 step loss 0.43244315918307813 sigmoid temp 1.0\n",
      "End sigmoid loss 0.42947615189931454 step loss 0.4325132500921343\n",
      "    beta 0.26857067741682383\n",
      "    rssi_w [-0.04111931  0.08432171  0.22492491  0.07511917]\n",
      "    rssi_th [38.99535554 38.99454458 11.99921869]\n",
      "    infect_w [0.00968925 0.052482   0.24755704]\n",
      "best loss 0.41288195658665433\n",
      "best scoring parameters\n",
      "    beta 0.20245363613048398\n",
      "    rssi_w [-0.02043003  0.0195634   0.18447747  0.26511673]\n",
      "    rssi_th [-86.9900888  -62.98029834 -27.98067386]\n",
      "    infect_w [0.0074555  0.04487948 0.22502496]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.777, median size 3\n",
      "\t Negative bags: mean size 3.947, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17367) (17367, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3519, 17367) (881, 17367)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.975409330070861 step loss 0.9747734083481637\n",
      "iter 0 sigmoid loss 0.9947749792931379 step loss 0.6665896444573158 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43156300927072844 step loss 0.44032029015651436 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.432681985277631 step loss 0.4351593424051562 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.48877041786288594 step loss 0.43269791395636126 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.46553317119450993 step loss 0.4700573735734595 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4098037254745317 step loss 0.5625555819629008 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3785314531256958 step loss 0.5292608753073194 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3602177079424901 step loss 0.47152276650698405 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.494661873887446 step loss 0.43790191944513346 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3887555243351633 step loss 0.4336439153846939 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43026163333088885 step loss 0.4338004908622908\n",
      "    beta 0.18551154673241968\n",
      "    rssi_w [-0.00847205  0.02551247  0.13901872  0.07837722]\n",
      "    rssi_th [28.00571798 26.00567502 34.99963387]\n",
      "    infect_w [0.0189463  0.0229701  0.15949123]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9365859613768692 step loss 0.9371835642768587\n",
      "iter 0 sigmoid loss 0.9519630975786008 step loss 0.6567671551083899 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.432197248107825 step loss 0.44079616662713506 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4347742928447135 step loss 0.43593253105243807 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.49239768791794625 step loss 0.4322804923202989 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.47043137061785745 step loss 0.43341420945433495 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.41111035556085523 step loss 0.5608960941455017 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3773053004792821 step loss 0.5301070707546228 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.35637525043582213 step loss 0.4469311496239352 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4966412670118065 step loss 0.4343129298379118 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.3851130043216623 step loss 0.4314830869182055 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4278895794449158 step loss 0.4318875045904978\n",
      "    beta 0.18231107462286503\n",
      "    rssi_w [-0.00606771 -0.0056085   0.03354293  0.15533621]\n",
      "    rssi_th [11.00629479 18.00630289 26.0062607 ]\n",
      "    infect_w [0.01750993 0.02275888 0.15568493]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9837152628256155 step loss 0.9753088325935021\n",
      "iter 0 sigmoid loss 1.0021716967428251 step loss 0.6642001436522473 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43177755313560257 step loss 0.4473882058097413 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4331932073040805 step loss 0.44552923350384005 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4896612168219148 step loss 0.44569422495029043 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.46658607305153543 step loss 0.4528133367905907 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4103173869600594 step loss 0.4761702541524535 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3811596614786715 step loss 0.4748197970874019 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36592677182679767 step loss 0.47085293561892033 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.49588209912217535 step loss 0.4670960108397933 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.398794506696815 step loss 0.4668827851236352 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4383300190202453 step loss 0.46584355405777456\n",
      "    beta 0.21342259131907895\n",
      "    rssi_w [-0.07090414  0.0141532   0.14261377  0.11291345]\n",
      "    rssi_th [39.00201394 11.0018502  34.99927517]\n",
      "    infect_w [0.02797307 0.02363476 0.18882392]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9838212638467987 step loss 1.0053717094814325\n",
      "iter 0 sigmoid loss 1.001179400233784 step loss 0.6711324186475526 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4290967555538488 step loss 0.4359482981047094 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4279080977279142 step loss 0.4291051886917749 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.48001957032664555 step loss 0.5648794928110087 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44984754121455145 step loss 0.7231473173777293 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40669745015746445 step loss 0.7702475844889615 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.37067477197242643 step loss 0.4471504660523068 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.329119128213188 step loss 0.41859328667754353 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5001296728779288 step loss 0.414932375732261 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.36372627213842057 step loss 0.41346849671287533 sigmoid temp 1.0\n",
      "End sigmoid loss 0.40759242732336964 step loss 0.4138035111793725\n",
      "    beta 0.22980290120534477\n",
      "    rssi_w [-0.00462991  0.02796374  0.0709572   0.19786224]\n",
      "    rssi_th [32.00109709 26.00104867  9.99853786]\n",
      "    infect_w [0.01489422 0.02707087 0.20936405]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.019815334213676 step loss 1.0151553138911897\n",
      "iter 0 sigmoid loss 1.0392836592530141 step loss 0.6554907089003936 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4320717185147129 step loss 0.4475771943711337 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.43288218485636276 step loss 0.44520999157557745 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.48901280870051733 step loss 0.4451941603708432 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4645925935434852 step loss 0.45242060677200746 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4091297975455231 step loss 0.4750023222792399 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.38105775721433277 step loss 0.47453857556351203 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36389787274636604 step loss 0.4706415473335185 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.49558830710507523 step loss 0.4669394011813872 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.39928163008528217 step loss 0.46670531391782466 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4379322204754461 step loss 0.4656914629743128\n",
      "    beta 0.21724181014916014\n",
      "    rssi_w [-0.07006987  0.01593608  0.13636348  0.12788232]\n",
      "    rssi_th [39.00122381 11.00105945 32.99864619]\n",
      "    infect_w [0.0278955  0.02380691 0.19310472]\n",
      "best loss 0.4138035111793725\n",
      "best scoring parameters\n",
      "    beta 0.22980290120534477\n",
      "    rssi_w [-0.00462991  0.02333382  0.09429103  0.29215327]\n",
      "    rssi_th [-87.99890291 -61.99785423 -51.99931637]\n",
      "    infect_w [0.01489422 0.04196509 0.25132914]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.042, median size 3\n",
      "\t Negative bags: mean size 3.967, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17632) (17632, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3513, 17632) (883, 17632)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0394749757544528 step loss 1.04937901463078\n",
      "iter 0 sigmoid loss 1.1149425729781945 step loss 0.6603080591808738 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.450918372682334 step loss 0.4361282774183669 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42328549538595195 step loss 0.42969104576917655 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47424223109354086 step loss 0.4249257712257901 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4304975514512563 step loss 0.4486850978440188 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.45915491586915613 step loss 0.6487664598410355 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40073780081057064 step loss 0.4296323232120459 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4249094862148429 step loss 0.4204359167060696 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.47356624038210965 step loss 0.41954594869861744 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4638139970914715 step loss 0.41898770496485516 sigmoid temp 1.0\n",
      "End sigmoid loss 0.41657905349704866 step loss 0.4190370887657087\n",
      "    beta 0.20896930802140048\n",
      "    rssi_w [0.00543773 0.02632381 0.17959965 0.04736566]\n",
      "    rssi_th [26.00705993 34.00702138 33.9998527 ]\n",
      "    infect_w [0.01631651 0.02651848 0.18627774]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.8856128878508515 step loss 0.8944196747374434\n",
      "iter 0 sigmoid loss 0.9566773344609482 step loss 0.6677822226829427 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.45021513218700465 step loss 0.4368597081644636 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.422885438607405 step loss 0.43096000610022134 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4731204745386524 step loss 0.4270496561837842 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.42900536014490315 step loss 0.45944477002123213 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.45836236391293583 step loss 0.492531239820557 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4029102322491031 step loss 0.4457225632801077 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4252347879890415 step loss 0.43119864973596406 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.47404769821446135 step loss 0.4152101617377285 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4484554089665083 step loss 0.4136497944162795 sigmoid temp 1.0\n",
      "End sigmoid loss 0.41132540582375293 step loss 0.4141210932190844\n",
      "    beta 0.21983051884101315\n",
      "    rssi_w [ 0.00362804  0.0271268  -0.00037571  0.19871599]\n",
      "    rssi_th [29.00405231 22.00403507 11.00480535]\n",
      "    infect_w [0.01625747 0.02808354 0.19794873]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9326424440023182 step loss 0.9398708208267889\n",
      "iter 0 sigmoid loss 1.0219158037987843 step loss 0.6760467881427428 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4515854028353184 step loss 0.4426532597708663 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4234818164225321 step loss 0.4393237251295167 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47596014398624253 step loss 0.4384028746243495 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.43212322227072947 step loss 0.4528955068239608 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4607318479522253 step loss 0.5367253921583635 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40661824117733397 step loss 0.5274892754478812 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4449335531642361 step loss 0.4893162679311381 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.48384585834582094 step loss 0.44358637384446464 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.486935627162822 step loss 0.439345110688053 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43537555230918973 step loss 0.4386412444494034\n",
      "    beta 0.19275089807303517\n",
      "    rssi_w [-0.00147577  0.02558054  0.08989928  0.14075895]\n",
      "    rssi_th [30.00212776 23.00210033 26.99846332]\n",
      "    infect_w [0.02181818 0.02998092 0.16539211]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9842572198957232 step loss 0.9890190438358423\n",
      "iter 0 sigmoid loss 1.070055167707073 step loss 0.6472942074830003 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4508460235926078 step loss 0.43820944593818517 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42305099146228675 step loss 0.43185433959169994 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4732355152131003 step loss 0.4249003193908451 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4290477037330709 step loss 0.4187520596593889 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4584385605550256 step loss 0.41715914436361073 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39872922863398086 step loss 0.4139509115596956 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4207050854361289 step loss 0.4137276526300545 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4729500743148563 step loss 0.4139690844304706 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4429121692830191 step loss 0.4138665249757335 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4095065604693693 step loss 0.41422775182367133\n",
      "    beta 0.23103801987934816\n",
      "    rssi_w [-0.02846403  0.00295602  0.05646949  0.20439562]\n",
      "    rssi_th [35.00303933 13.00302307 15.00356182]\n",
      "    infect_w [0.01590755 0.02791553 0.21030352]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9313221708448703 step loss 0.949567022327887\n",
      "iter 0 sigmoid loss 1.0100984004721116 step loss 0.699364429662078 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.45020106396049137 step loss 0.4387158893890138 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4223598489186554 step loss 0.43359943417627067 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47232494356086263 step loss 0.4301579223658677 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.42869662198001746 step loss 0.4545631837670273 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.45898956986640543 step loss 0.8709494595576058 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39417172958864766 step loss 0.4239317591613941 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.42272327881070554 step loss 0.418822531322704 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.46590205384581856 step loss 0.418142479203301 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.442300083096001 step loss 0.4169653648582701 sigmoid temp 1.0\n",
      "End sigmoid loss 0.40978285621959804 step loss 0.41772675728652225\n",
      "    beta 0.25341725057016473\n",
      "    rssi_w [0.00216955 0.027851   0.2284861  0.05936821]\n",
      "    rssi_th [27.99701162 37.99693915 23.99978396]\n",
      "    infect_w [0.01614029 0.02850586 0.23415236]\n",
      "best loss 0.4141210932190844\n",
      "best scoring parameters\n",
      "    beta 0.21983051884101315\n",
      "    rssi_w [0.00362804 0.03075484 0.03037912 0.22909511]\n",
      "    rssi_th [-90.99594769 -68.99191262 -57.98710727]\n",
      "    infect_w [0.01625747 0.04434101 0.24228974]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.939, median size 3\n",
      "\t Negative bags: mean size 4.015, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17735) (17735, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3512, 17735) (877, 17735)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.087839254180437 step loss 1.0932722472974823\n",
      "iter 0 sigmoid loss 1.1580944565529783 step loss 0.6042819237385558 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3883770423088354 step loss 0.46206789398650605 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.5598798704688486 step loss 0.4588964477639659 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43083801627709406 step loss 0.457866080495336 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.45593633232047337 step loss 0.4732778790464496 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.36669955481210076 step loss 0.4935591779545636 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3986793340759807 step loss 0.5000972359686572 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.41548718908569 step loss 0.4889929219894398 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.48563417894335825 step loss 0.4773407205231478 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.39261680405593696 step loss 0.4693484297042276 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4522314210847185 step loss 0.46903112289832455\n",
      "    beta 0.2022447088105949\n",
      "    rssi_w [-0.02450282  0.02090326  0.07854919  0.16199313]\n",
      "    rssi_th [34.99996329 15.99991352 28.9981647 ]\n",
      "    infect_w [0.04027041 0.02481356 0.17741309]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9769801545813155 step loss 0.9895869765171298\n",
      "iter 0 sigmoid loss 1.0414140169367325 step loss 0.6595855356211184 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.38710949783753035 step loss 0.45745335448537217 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.5607508857273084 step loss 0.45174979548768024 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4303093445104374 step loss 0.4482192323582017 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.45546137547121046 step loss 0.4501377978922481 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.36525160006765583 step loss 0.4749784391651915 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3872355513784562 step loss 0.4469832239240161 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4079235026817222 step loss 0.44564362227235493 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4855835885423269 step loss 0.44532519602709114 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.39529941781284317 step loss 0.44511394878810945 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44260718421550255 step loss 0.44536095378854046\n",
      "    beta 0.19333186501425673\n",
      "    rssi_w [-0.01951442 -0.00453176  0.06498826  0.15840143]\n",
      "    rssi_th [18.00463323 29.00462999 13.00463065]\n",
      "    infect_w [0.0268492  0.01805363 0.16773854]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9723191551487554 step loss 0.9868577068563075\n",
      "iter 0 sigmoid loss 1.0427272222422101 step loss 0.6843398815722586 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.38708424129706104 step loss 0.4579593209106654 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.5574917410898389 step loss 0.45210931352939426 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43008506739714236 step loss 0.45699448865240566 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.45312912243249814 step loss 0.6434117157704216 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.36417514871955325 step loss 0.7567894783514513 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3858572613345401 step loss 0.4580473383766367 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.40952115503006253 step loss 0.44766484366093245 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4841621298853792 step loss 0.44585627763020985 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.39175513460170985 step loss 0.4451755282787314 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44168459963243145 step loss 0.445334862952551\n",
      "    beta 0.20282383666737616\n",
      "    rssi_w [-0.00041208  0.03474548  0.117681    0.13389502]\n",
      "    rssi_th [35.00277811 24.00271563 18.99906765]\n",
      "    infect_w [0.02813538 0.01996431 0.17805803]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.3048400825421331 step loss 1.3313937166071235\n",
      "iter 0 sigmoid loss 1.3891621198346586 step loss 0.5352856558008654 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3872755940882426 step loss 0.45983023413021845 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.5580070072768042 step loss 0.4544503786871219 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.43102430414713994 step loss 0.45283753872370197 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4552990771752873 step loss 0.45464885045985487 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3669374861704011 step loss 0.48008000169158693 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3940395298771413 step loss 0.44585835676635827 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4039399058866395 step loss 0.4436712959593652 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4690872972798757 step loss 0.4433380233948163 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.38515572203357806 step loss 0.4432191164640763 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4385595261527159 step loss 0.44384421094332654\n",
      "    beta 0.23770571682947264\n",
      "    rssi_w [-0.01477425  0.05253822  0.21173482  0.05693385]\n",
      "    rssi_th [38.99611499 29.99583421 22.99974188]\n",
      "    infect_w [0.03089407 0.02192226 0.2185698 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1251017938771906 step loss 1.153248956285027\n",
      "iter 0 sigmoid loss 1.1827454649428086 step loss 0.6365536877881028 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.38767818118237946 step loss 0.4622200294211987 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.557755558522556 step loss 0.4582216430522008 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4308674040525117 step loss 0.45874610951171907 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4552375072002184 step loss 0.46421757835856936 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3672370398162992 step loss 0.49110771892266186 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.40246994269044684 step loss 0.4546094406474768 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.41621633451466883 step loss 0.4527603371725568 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.47350435903842486 step loss 0.4524828729245317 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3855785439914275 step loss 0.4523254252897514 sigmoid temp 1.0\n",
      "End sigmoid loss 0.44740894657739183 step loss 0.45295310314625864\n",
      "    beta 0.2346472988661418\n",
      "    rssi_w [-0.00837042  0.05131483  0.20556818  0.04912882]\n",
      "    rssi_th [36.99563745 35.99537857 19.99970056]\n",
      "    infect_w [0.03750086 0.02206022 0.21273202]\n",
      "best loss 0.44384421094332654\n",
      "best scoring parameters\n",
      "    beta 0.23770571682947264\n",
      "    rssi_w [-0.01477425  0.03776397  0.24949879  0.30643264]\n",
      "    rssi_th [-81.00388501 -51.00805079 -28.00830892]\n",
      "    infect_w [0.03089407 0.05281632 0.27138612]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.932, median size 3\n",
      "\t Negative bags: mean size 3.998, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17668) (17668, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3508, 17668) (872, 17668)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.050193211866523 step loss 1.0504721120240479\n",
      "iter 0 sigmoid loss 1.0908038676429923 step loss 0.6486547223238058 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5034013090626045 step loss 0.4661657807572751 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.44914205157930764 step loss 0.4644501171456039 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.500902996841499 step loss 0.4640788510887457 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44600130400909344 step loss 0.4698435042107657 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.517910589010403 step loss 0.5053340675700123 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.45413855404736986 step loss 0.5061476693904973 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5217557880555941 step loss 0.49768506457548 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.45008406554664077 step loss 0.46878389180573427 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4684652942332036 step loss 0.4652570808824822 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4602250509745371 step loss 0.46484924200515065\n",
      "    beta 0.17107647304049892\n",
      "    rssi_w [0.00147151 0.0171863  0.07599566 0.12505067]\n",
      "    rssi_th [20.99973078 30.99972341 24.9985636 ]\n",
      "    infect_w [0.04403552 0.02900909 0.13537564]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.2714348419789183 step loss 1.2781397097213405\n",
      "iter 0 sigmoid loss 1.3118618766502763 step loss 0.5229664789235658 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5037496468317292 step loss 0.4684802471035948 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4482844251856782 step loss 0.46645439465193156 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5002562779000669 step loss 0.46634781541732284 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44473280644218494 step loss 0.47492613760498936 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5191254114529005 step loss 0.47869154503565897 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4528244284834673 step loss 0.4778567175183733 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5217858486617992 step loss 0.4777762885414834 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4483311031411097 step loss 0.47665125656930607 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47037748208785807 step loss 0.4767025883442265 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4605190993798862 step loss 0.47634472010706275\n",
      "    beta 0.1779951442320126\n",
      "    rssi_w [-0.02227368  0.00961429  0.08297067  0.13461191]\n",
      "    rssi_th [31.99876555 17.99874318 25.99782724]\n",
      "    infect_w [0.0498416  0.03454115 0.15594837]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.4211125767465704 step loss 1.5096454360189335\n",
      "iter 0 sigmoid loss 1.4670603670159226 step loss 0.47421417040987024 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5020515374474315 step loss 0.46432705501988975 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.45155169554735314 step loss 0.46429181742813425 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5034332445937347 step loss 0.46306317901639144 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4517188227215982 step loss 0.46316789761902405 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5165759995892931 step loss 0.4617988483669019 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.46213884963781354 step loss 0.46079046831098375 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5200335289759411 step loss 0.45928688369943077 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.45764754327681073 step loss 0.4576817769677528 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.46979927036151264 step loss 0.45592213499286605 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4534181698523756 step loss 0.4543422159568081\n",
      "    beta 0.12391024600534309\n",
      "    rssi_w [0.06007068 0.0633297  0.2042788  0.03013188]\n",
      "    rssi_th [27.99918662 37.99908902 27.999888  ]\n",
      "    infect_w [0.03059275 0.02420756 0.09308692]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9179277978930521 step loss 0.9211401127183539\n",
      "iter 0 sigmoid loss 0.9566137027413437 step loss 0.6785582489859799 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5041566250954793 step loss 0.4668169494979409 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.44926402279345823 step loss 0.4651849442108793 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5023524018451344 step loss 0.463681264668372 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4465302471160753 step loss 0.4646579061113853 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5174123004701656 step loss 0.46599390560501763 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.45511254841026344 step loss 0.46264115860991323 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.526069320190685 step loss 0.46117359311344436 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4554983282816255 step loss 0.461312875756086 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.47464890303288404 step loss 0.4609603664765773 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45840283626888306 step loss 0.46136527621673\n",
      "    beta 0.19175056325805287\n",
      "    rssi_w [-0.0003365   0.00584606  0.04666665  0.16771546]\n",
      "    rssi_th [14.99655143 18.99656117 38.99640875]\n",
      "    infect_w [0.04669049 0.03656233 0.1569127 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.953806040000221 step loss 0.9426649895129358\n",
      "iter 0 sigmoid loss 0.9931579255699364 step loss 0.6790931312128008 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.504107324037143 step loss 0.4693083638369083 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4498994930000185 step loss 0.468031228269876 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5022529216598269 step loss 0.46848715624815146 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.44824177435478973 step loss 0.46809139709617914 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5164704360998982 step loss 0.4699851543463971 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4582616427711632 step loss 0.4676402122765692 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5190849432831768 step loss 0.4677589469512025 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.45706927822379007 step loss 0.46731743311345936 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.46795109173886457 step loss 0.467358459699436 sigmoid temp 1.0\n",
      "End sigmoid loss 0.466141536086455 step loss 0.4673510778241987\n",
      "    beta 0.18010156038462669\n",
      "    rssi_w [-0.03095758 -0.01866058  0.11327139  0.1051141 ]\n",
      "    rssi_th [26.00092401 21.00092798 34.99891061]\n",
      "    infect_w [0.05830536 0.0340719  0.13844239]\n",
      "best loss 0.4543422159568081\n",
      "best scoring parameters\n",
      "    beta 0.12391024600534309\n",
      "    rssi_w [0.06007068 0.12340038 0.32767918 0.35781106]\n",
      "    rssi_th [-92.00081338 -54.00172436 -26.00183636]\n",
      "    infect_w [0.03059275 0.05480031 0.14788723]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.948, median size 3\n",
      "\t Negative bags: mean size 4.049, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17870) (17870, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3492, 17870) (875, 17870)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.122070744003032 step loss 1.1487630514560048\n",
      "iter 0 sigmoid loss 0.9670005357697355 step loss 0.6429736442012415 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4391366171580114 step loss 0.46400554142392275 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42650789733560585 step loss 0.4626351132184985 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47483163960839486 step loss 0.4649766873201727 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.48830498355050234 step loss 0.48461982969866285 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4006045710123388 step loss 0.610632968313629 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39711732970059105 step loss 0.4633227125098513 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.562446188791133 step loss 0.4593151383215866 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5266268773024599 step loss 0.45873157665882647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5645035440298435 step loss 0.4585271900171728 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45703464710548436 step loss 0.4585006455335067\n",
      "    beta 0.15908882494409796\n",
      "    rssi_w [0.01513308 0.02794613 0.0730871  0.10456733]\n",
      "    rssi_th [24.00113117 33.00112008 15.99910039]\n",
      "    infect_w [0.0469209  0.03296887 0.11531267]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9463269639812599 step loss 0.9539284222238102\n",
      "iter 0 sigmoid loss 0.8147533683674911 step loss 0.6746724053739839 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4392354276234232 step loss 0.46334593407064434 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42747404306380504 step loss 0.46044028635678425 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4753787147090007 step loss 0.45824530217504644 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4883832089302419 step loss 0.45772565104118335 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40178147626283645 step loss 0.4592724282528812 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39660898944250556 step loss 0.45653347294572716 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5584773028676874 step loss 0.4563856511093013 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.524778438713505 step loss 0.456265717082645 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5713573258225625 step loss 0.45620324803166007 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45489873425246025 step loss 0.4562521957652737\n",
      "    beta 0.162156224611698\n",
      "    rssi_w [0.00391552 0.01332672 0.03535265 0.13037823]\n",
      "    rssi_th [22.00057042 27.00056914 13.00103899]\n",
      "    infect_w [0.04476564 0.03086553 0.11980603]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9793296397888168 step loss 0.9866112925292869\n",
      "iter 0 sigmoid loss 0.8424879393116559 step loss 0.6688258576327764 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43915883956429974 step loss 0.46698611093137926 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42887832268090054 step loss 0.4660347363258706 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47696116554576634 step loss 0.4654188240496694 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49054323129625016 step loss 0.46520515945476454 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.40349286348234353 step loss 0.46564783782801605 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39569417428362497 step loss 0.46552978067398454 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5685232407106143 step loss 0.4653655833051503 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5409519847002876 step loss 0.4652105582593811 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.570612334345103 step loss 0.46507439782872756 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46375106978552116 step loss 0.4651105668407215\n",
      "    beta 0.16493052984168913\n",
      "    rssi_w [-0.01016592  0.00281782  0.06761892  0.12286244]\n",
      "    rssi_th [23.99819541 18.99820181 31.9978325 ]\n",
      "    infect_w [0.06043972 0.04256316 0.11347867]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.8562330938515436 step loss 0.8555642570171704\n",
      "iter 0 sigmoid loss 0.7350063187641596 step loss 0.6831825046723977 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43917261765707666 step loss 0.46774883766509645 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.42998780835643124 step loss 0.467991331298429 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47769275150007545 step loss 0.4693814164995946 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49117576979741123 step loss 0.4724126527400592 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.406411897846652 step loss 0.483174907252442 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.400786543277129 step loss 0.5101028315610763 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5740277900525801 step loss 0.499725028852719 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5439905251491914 step loss 0.4745730665790884 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5675739093544455 step loss 0.4712566443413256 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4661934687834033 step loss 0.47009653628417264\n",
      "    beta 0.13629956557560377\n",
      "    rssi_w [-0.00888067  0.00384041  0.03459493  0.09561554]\n",
      "    rssi_th [22.00134184 15.0013481  14.00133626]\n",
      "    infect_w [0.05090801 0.02453106 0.08016311]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1256068527230048 step loss 1.1262261282526855\n",
      "iter 0 sigmoid loss 0.9583826006474058 step loss 0.6094090846335091 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43918600608344405 step loss 0.4643001242251231 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4275347085923646 step loss 0.4617292876795894 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.47569557303992566 step loss 0.45959928527088245 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.48876122203006017 step loss 0.45866498312181964 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4016517153378447 step loss 0.45928890346331636 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.39552829204950546 step loss 0.4573767231492989 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5682952912361178 step loss 0.4571635289111572 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5319881967201165 step loss 0.45703978017372293 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5640337192748328 step loss 0.4569387701570394 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45628284177713574 step loss 0.45704768481908664\n",
      "    beta 0.17374890642619648\n",
      "    rssi_w [-0.00913231 -0.00420919  0.06215439  0.13768595]\n",
      "    rssi_th [11.99872852 33.99872932 21.99848824]\n",
      "    infect_w [0.05115039 0.04256908 0.13333738]\n",
      "best loss 0.4562521957652737\n",
      "best scoring parameters\n",
      "    beta 0.162156224611698\n",
      "    rssi_w [0.00391552 0.01724224 0.05259489 0.18297312]\n",
      "    rssi_th [-97.99942958 -70.99886043 -57.99782144]\n",
      "    infect_w [0.04476564 0.07563117 0.1954372 ]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 4.046, median size 3\n",
      "\t Negative bags: mean size 3.951, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17576) (17576, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3481, 17576) (878, 17576)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.868340760639224 step loss 0.8668661369703308\n",
      "iter 0 sigmoid loss 0.8612517119287328 step loss 0.6683878902035966 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4761673106096875 step loss 0.4608032744657371 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4262378736789351 step loss 0.4597934258526387 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1500 sigmoid loss 0.5085652774944142 step loss 0.45921051533791063 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4977037633250163 step loss 0.4586980481405367 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5250271974262489 step loss 0.45831163061109365 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4345540268469537 step loss 0.45802535551316476 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4217222315784516 step loss 0.4579799389705816 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4932562526846435 step loss 0.45791290986815913 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5054018519103042 step loss 0.4579194838652503 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45674383587181566 step loss 0.4579974604116454\n",
      "    beta 0.15848293904708868\n",
      "    rssi_w [-0.00099646  0.01315242  0.05480578  0.11448192]\n",
      "    rssi_th [30.99882895 14.99883482 24.9985695 ]\n",
      "    infect_w [0.06258591 0.02474049 0.10678335]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1270915751440902 step loss 1.1534528452426431\n",
      "iter 0 sigmoid loss 1.1266719449635263 step loss 0.5608836741123522 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4748054519891128 step loss 0.45993879240826313 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4266749951694694 step loss 0.4595759873933723 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5078105587994715 step loss 0.4596379889766263 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49744158738737143 step loss 0.4595297765101663 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5248988424864947 step loss 0.46023428207536743 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.43567478779977975 step loss 0.4593431766490877 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.42617149132665494 step loss 0.45890050757651973 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4933529353459262 step loss 0.4583525596948272 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5053208352563517 step loss 0.45813847409517683 sigmoid temp 1.0\n",
      "End sigmoid loss 0.45719931371167766 step loss 0.4580772092994998\n",
      "    beta 0.13982105811761728\n",
      "    rssi_w [0.02903556 0.04261583 0.06656545 0.08229681]\n",
      "    rssi_th [34.00100779 22.00098932 18.99969886]\n",
      "    infect_w [0.05203326 0.0155668  0.08700549]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0384253749584487 step loss 1.0519137404921997\n",
      "iter 0 sigmoid loss 1.0369354999231997 step loss 0.6382203078284431 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4764514552664811 step loss 0.4595229694395131 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4258230342656073 step loss 0.45863533634359666 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5076071789234551 step loss 0.4583963360261181 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49791441988260265 step loss 0.4581512226582662 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5253673311923457 step loss 0.4590757181578046 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4311654138225806 step loss 0.45585852120197784 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.408846499297153 step loss 0.45553006497757853 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4825783998740495 step loss 0.4551789061853551 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5097922288676492 step loss 0.4551453368775139 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4541349982600823 step loss 0.4552088700108696\n",
      "    beta 0.16193951684885236\n",
      "    rssi_w [0.02154052 0.03992957 0.1199036  0.02995197]\n",
      "    rssi_th [30.99941021 35.99936545 25.99991558]\n",
      "    infect_w [0.05743841 0.02339488 0.11805686]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.137670220625792 step loss 1.1551368911651974\n",
      "iter 0 sigmoid loss 1.1394049577713776 step loss 0.5897520640925555 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.47601413721844466 step loss 0.46013993496217304 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4268799745796349 step loss 0.4597083266064767 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5071184934195999 step loss 0.45989210744918607 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4976863090625189 step loss 0.4601689572431519 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5252190665335681 step loss 0.4616603438839094 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.43571786894740455 step loss 0.4603042673026627 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4251350726317086 step loss 0.4592311582477833 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.49243001301379463 step loss 0.45858156546825707 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5077919123690401 step loss 0.458346305869169 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4569501399256659 step loss 0.45827191517483307\n",
      "    beta 0.1440730307804671\n",
      "    rssi_w [0.0243433  0.0421321  0.06866924 0.07232369]\n",
      "    rssi_th [37.00130932 20.0012856  20.99981308]\n",
      "    infect_w [0.05522088 0.01606109 0.09416195]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9060860548357565 step loss 0.8990630428028576\n",
      "iter 0 sigmoid loss 0.8971221576132505 step loss 0.6439398407988968 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.47718606782448303 step loss 0.4621716378101358 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4263760221554493 step loss 0.46170825664728915 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5103875158561412 step loss 0.46190004101532045 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.49771970623734774 step loss 0.4630122843320353 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5245215069128899 step loss 0.46445163608943507 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.44288875945336764 step loss 0.46270072966759307 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4381440293964037 step loss 0.462557877268341 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5038733826667224 step loss 0.46263668479873443 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5058299093613952 step loss 0.4625065212695499 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46071251364931287 step loss 0.46241789166147035\n",
      "    beta 0.13357306543883\n",
      "    rssi_w [0.00064022 0.00855895 0.02395354 0.09057627]\n",
      "    rssi_th [26.00070971 10.00072354 14.00073763]\n",
      "    infect_w [0.05727278 0.01315329 0.07348374]\n",
      "best loss 0.4552088700108696\n",
      "best scoring parameters\n",
      "    beta 0.16193951684885236\n",
      "    rssi_w [0.02154052 0.0614701  0.1813737  0.21132567]\n",
      "    rssi_th [-89.00058979 -53.00122434 -27.00130877]\n",
      "    infect_w [0.05743841 0.08083329 0.19889015]\n"
     ]
    }
   ],
   "source": [
    "# max bag size 32\n",
    "censor_prob = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "idx = 0\n",
    "\n",
    "auc_train_learned = np.zeros((len(censor_prob),n_trials))\n",
    "auc_train_true = np.zeros((len(censor_prob),n_trials))\n",
    "auc_test_learned = np.zeros((len(censor_prob),n_trials))\n",
    "auc_test_true = np.zeros((len(censor_prob),n_trials))\n",
    "for prob in censor_prob:\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=32,censor_prob_pos=prob,censor_prob_neg=0,max_pos_in_bag=1)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WghNZDEbxokP",
    "outputId": "1c4f32e7-6349-4aca-aca9-ac8de4855a1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1c8e79a00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(censor_prob, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(censor_prob, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Censor Probability\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: Max Bag Size = 32\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(censor_prob, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(censor_prob, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Censor Probability\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: Max Bag Size = 32\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChmEPbzDxd2t"
   },
   "source": [
    "### 3. Varying Positive Bag Construction\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "This experiment first increases the maximum number of positive exposures per bag from 1 to 3, then iterates over increasing bag sizes. We want to compare the results here to that of Experiment 1., to show the effect that increasing maximum number of positive exposures has on the performance of the learning algorithm. Increasing maximum number of positive exposures would increase the signal received by the algorithm. Hence, the algorithm should perform better as long as the gain in signal (compared to experiment 1) is not outweighed by the gain in noise from increasing bag size. \n",
    "\n",
    "We expect the performance of the algorithm to increase, i.e. the AUC score to increase, compared to experiment 1. However, this will only happen for maximum bag sizes up till around 3 or 4, and will decrease after. This is reflected in the two plots below, where the AUC increases slightly (from around 0.8 to 0.9) up until a maximum bag size of 4, then drops. This trend supports the claim being made, since having up to 3 positive exposures in a bag is significant when the number of positive exposures make up a substantial percentage of the bag size. In this case, a maximum of 3 positive exposures in a bag of 4 exposures would mean a signal strength of 75%. However as the bag gets larger but the positive exposures remain at a fixed maximum of 3, it faces the same problem as Experiment 1 where the noise of the additional exposures drown out the signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcfR3dX0xsca",
    "outputId": "466f988a-7dcc-4815-9122-b53849c4d6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4431, 4431) (4431, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 4431) (887, 4431)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.435833557100843 step loss 1.4919998719849097\n",
      "iter 0 sigmoid loss 1.4457935865163787 step loss 0.6713896398738584 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.39454125185947403 step loss 0.33957002420014953 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3095893621262033 step loss 0.33017755074426913 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33892465124191545 step loss 0.3257831404756726 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.26839281863461933 step loss 0.3239480777134672 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28425681386804286 step loss 0.32995353237192987 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25639420126707274 step loss 0.32061880256656455 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2892982448335 step loss 0.3165769226657959 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.21932085816532698 step loss 0.3147733684101123 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2629467147084974 step loss 0.31369451680656774 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30482443933883296 step loss 0.3136665358227709\n",
      "    beta 0.36454351959026565\n",
      "    rssi_w [-0.00891451  0.08174279  0.33709637  0.08386947]\n",
      "    rssi_th [36.9872118  34.98642146 17.99918278]\n",
      "    infect_w [0.0102498  0.07409035 0.34600911]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1792263453195573 step loss 1.1826108468348329\n",
      "iter 0 sigmoid loss 1.1990823699835427 step loss 0.8106054425825353 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3974026165081266 step loss 0.32538955075162307 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3096188217105617 step loss 0.31214543410696954 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3422317515591384 step loss 0.3033524503903451 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.26825366710442416 step loss 0.29548802482457254 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28294446855487193 step loss 0.5030384986190611 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.22992604978931375 step loss 0.5015503347567078 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.24954707464115258 step loss 0.29951641071840024 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.22563704164832693 step loss 0.2928560175857487 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2649010382904389 step loss 0.2907834609730259 sigmoid temp 1.0\n",
      "End sigmoid loss 0.28367750290483273 step loss 0.29082108181776367\n",
      "    beta 0.30473032824597224\n",
      "    rssi_w [-1.46438908e-04  3.00162578e-02  2.69548916e-01  1.01503879e-01]\n",
      "    rssi_th [27.01117852 31.01107618 29.99942152]\n",
      "    infect_w [0.008287   0.06144131 0.28524676]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1191399356421223 step loss 1.117764397265231\n",
      "iter 0 sigmoid loss 1.1367670741115896 step loss 0.8009906519988977 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3986518293753795 step loss 0.3388143991981894 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30981340508745703 step loss 0.32953356025963493 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.342443260434693 step loss 0.3248614336007339 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.26776852413466234 step loss 0.31962083119983353 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.27856308835321664 step loss 0.316338204553544 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.24318170645122394 step loss 0.31393276372833945 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.277330900011492 step loss 0.31301754386668723 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.21250065032442744 step loss 0.31172182295338186 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.25444173212357796 step loss 0.3099931958763543 sigmoid temp 1.0\n",
      "End sigmoid loss 0.296660898888017 step loss 0.3099177658023278\n",
      "    beta 0.38678136837489985\n",
      "    rssi_w [-0.06358413 -0.05965949  0.18421374  0.31571538]\n",
      "    rssi_th [ 9.9931627  36.99316644 23.98724104]\n",
      "    infect_w [0.01057328 0.07909798 0.3676716 ]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1297204879905975 step loss 1.1434506076765911\n",
      "iter 0 sigmoid loss 1.1484704952597613 step loss 0.8513777113552274 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.39385518470340813 step loss 0.3435475617052379 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3099017476281969 step loss 0.33473553740951906 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33870253977027104 step loss 0.3314618130836289 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2685811786062221 step loss 0.33088375195390374 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28482503290620953 step loss 0.3368402483927518 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.26164328957725336 step loss 0.3293335173396089 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2995110392603005 step loss 0.32641051492866774 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.22931538191358253 step loss 0.32518955757664536 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2797674015726871 step loss 0.3245272774881173 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3157458890404614 step loss 0.32454450314208916\n",
      "    beta 0.35603490633185897\n",
      "    rssi_w [-0.01570107  0.09986237  0.31855147  0.07725113]\n",
      "    rssi_th [38.98969191 34.98860717 16.99908833]\n",
      "    infect_w [0.01059267 0.07457185 0.33635803]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1317548255216041 step loss 1.163480103409567\n",
      "iter 0 sigmoid loss 1.144695683373002 step loss 0.8538863053731299 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.39182661301601995 step loss 0.34242551969720547 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3089554289357865 step loss 0.33390019365089874 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3337516121299441 step loss 0.3309409216861679 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2650709687756752 step loss 0.33437485581814014 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2759487549603042 step loss 0.36522202078372157 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2569143065094508 step loss 0.3307036174404193 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.29361876308116797 step loss 0.3250149106759687 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.22243437662067908 step loss 0.3231074958143326 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.27036288106123685 step loss 0.3221558237212638 sigmoid temp 1.0\n",
      "End sigmoid loss 0.311077933292284 step loss 0.32216296608877387\n",
      "    beta 0.36454288648558586\n",
      "    rssi_w [-0.00448539  0.08228686  0.32755887  0.09916323]\n",
      "    rssi_th [34.98756897 37.98693841 11.9988121 ]\n",
      "    infect_w [0.01025086 0.0722514  0.34586982]\n",
      "best loss 0.29082108181776367\n",
      "best scoring parameters\n",
      "    beta 0.30473032824597224\n",
      "    rssi_w [-1.46438908e-04  2.98698189e-02  2.99418735e-01  4.00922614e-01]\n",
      "    rssi_th [-92.98882148 -61.9777453  -31.97832378]\n",
      "    infect_w [0.008287   0.06972831 0.35497506]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.554, median size 2\n",
      "\t Negative bags: mean size 1.560, median size 2\n",
      "assign_mat size, X_shuff size: (4431, 6908) (6908, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 6908) (790, 6908)\n",
      "Average positive samples per bag: 1.4943661971830986\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.1837801759657274 step loss 1.2112377357946957\n",
      "iter 0 sigmoid loss 0.9477913519377271 step loss 0.7209081263101211 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3349688385253218 step loss 0.3232795960383862 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.33193688632245694 step loss 0.3123857349011004 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3263300072175103 step loss 0.30728753602285613 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3521643168437044 step loss 0.3044053616695704 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2929035110604238 step loss 0.34364100922277707 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.22718399095888608 step loss 0.3422480529431119 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3151558089963482 step loss 0.33936665296542945 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3020099866160886 step loss 0.3390493804195359 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.2542830269043291 step loss 0.32905569172890275 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2941304094978999 step loss 0.3288035508403608\n",
      "    beta 0.29751522417621457\n",
      "    rssi_w [-0.05757349  0.04433764  0.15860202  0.22673117]\n",
      "    rssi_th [37.00570723 15.00545638 27.99653896]\n",
      "    infect_w [0.00603402 0.04106273 0.28536993]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0601334787383894 step loss 1.0654949194468064\n",
      "iter 0 sigmoid loss 0.8473871708757903 step loss 0.7719838610891679 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3358095408821296 step loss 0.31887620898804514 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3350853338556621 step loss 0.3064395364302617 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3291085029425044 step loss 0.2967381278475104 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3530951516511754 step loss 0.2887718105380951 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2933878997243796 step loss 0.7735290052018553 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2063604325587362 step loss 0.28465096296298875 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.30693890477765573 step loss 0.27273308100803395 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.26097021568243856 step loss 0.27066882084426147 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2407030837320341 step loss 0.269901603201037 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2561414628813297 step loss 0.2710686046457263\n",
      "    beta 0.3634742521379074\n",
      "    rssi_w [-0.01154868 -0.00428187  0.04535941  0.35006989]\n",
      "    rssi_th [10.99261993 18.99264053 36.99242331]\n",
      "    infect_w [0.00471397 0.04650676 0.35207539]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1639796715373854 step loss 1.1811425386861225\n",
      "iter 0 sigmoid loss 0.9284115028915347 step loss 0.7294162285758815 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3335159830450552 step loss 0.3176963318497122 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3305163587440608 step loss 0.3043830246996297 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32202154272876926 step loss 0.2931326156538014 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3443084953176339 step loss 0.2792229339554346 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2852249221041937 step loss 0.2860006081020005 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.199785821826446 step loss 0.26685282524331494 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2901449573196514 step loss 0.26719614341088943 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.25875819584754806 step loss 0.2672318451239024 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2333486454015771 step loss 0.2669578206589667 sigmoid temp 1.0\n",
      "End sigmoid loss 0.25200365815369447 step loss 0.26809192783551933\n",
      "    beta 0.36607689856961156\n",
      "    rssi_w [-0.06398688 -0.04166101  0.13257696  0.32276791]\n",
      "    rssi_th [19.99852829 26.9985198  18.99590321]\n",
      "    infect_w [0.00487024 0.04710775 0.35427045]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.2262461833797738 step loss 1.2520149662114275\n",
      "iter 0 sigmoid loss 0.9766126877462472 step loss 0.7122862492365746 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33091606989769007 step loss 0.31243579527376303 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.32810681453381546 step loss 0.2989146787279141 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3192870760210069 step loss 0.28919696235115716 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34272054687740366 step loss 0.42157176043497707 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28416236270566086 step loss 0.4513712909451253 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2051172326431882 step loss 0.4030877990070281 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.28913486226025037 step loss 0.36262301574766986 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2742578929935363 step loss 0.28407620779237336 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.22815258909756497 step loss 0.27994586458291176 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27074257032024857 step loss 0.27995832094764345\n",
      "    beta 0.3056887607460034\n",
      "    rssi_w [-0.00532949  0.01914278  0.15780747  0.24799286]\n",
      "    rssi_th [23.00639063 33.0063528  18.99640253]\n",
      "    infect_w [0.0047028  0.04246963 0.29287029]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.021817981225012 step loss 1.0260073352275603\n",
      "iter 0 sigmoid loss 0.8173883088173843 step loss 0.7768269362778379 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3364914217610597 step loss 0.3296259681014695 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3361932172935505 step loss 0.3214757685543663 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3319806000143506 step loss 0.3186399166540513 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3594927558308194 step loss 0.3151109943646052 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3043574597304923 step loss 0.3144751384316902 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.23721909738585878 step loss 0.311844137700054 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3723133697155322 step loss 0.3082690947166901 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.30838675307275787 step loss 0.30692484717495894 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.27360888055421456 step loss 0.30658058234453833 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2990636209262062 step loss 0.30696139900386\n",
      "    beta 0.36303294459184554\n",
      "    rssi_w [-0.04639609 -0.02993565  0.13173107  0.32121327]\n",
      "    rssi_th [18.99071552 22.99073304 32.98891586]\n",
      "    infect_w [0.00479624 0.05639082 0.35024903]\n",
      "best loss 0.26809192783551933\n",
      "best scoring parameters\n",
      "    beta 0.36607689856961156\n",
      "    rssi_w [-0.06398688 -0.10564789  0.02692907  0.34969698]\n",
      "    rssi_th [-100.00147171  -73.00295191  -54.0070487 ]\n",
      "    infect_w [0.00487024 0.05197799 0.40624844]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.414, median size 2\n",
      "\t Negative bags: mean size 2.426, median size 2\n",
      "assign_mat size, X_shuff size: (4431, 10742) (10742, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 10742) (817, 10742)\n",
      "Average positive samples per bag: 2.0183098591549298\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0483955193136627 step loss 1.0664725511442985\n",
      "iter 0 sigmoid loss 1.0458123143958244 step loss 0.6544109294534421 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.30327933245903926 step loss 0.314813838483179 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.2964877765415271 step loss 0.3108225514537911 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3178248920763817 step loss 0.3311268129989106 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.31292756466964344 step loss 0.33292217562247917 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3729603223219333 step loss 0.3879918284976411 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.26749964963095507 step loss 0.33596707377274204 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21860730973363338 step loss 0.32194410773507515 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2604660968419151 step loss 0.3153186318609429 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.24858026958776364 step loss 0.3097385186582696 sigmoid temp 1.0\n",
      "End sigmoid loss 0.28119306946340394 step loss 0.3096328033667339\n",
      "    beta 0.3058978226804342\n",
      "    rssi_w [-0.01693843 -0.00571563  0.09103949  0.30506625]\n",
      "    rssi_th [20.99563883 30.99562752 21.99290101]\n",
      "    infect_w [-0.01157165  0.05782105  0.32857977]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.2178879164927356 step loss 1.206422856990227\n",
      "iter 0 sigmoid loss 1.21068714209863 step loss 0.5394622130786187 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.30674051948516295 step loss 0.3292190675603544 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30020624930255047 step loss 0.3278972554982895 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33253447719016066 step loss 0.3281798985735014 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3372285072959565 step loss 0.3283385456158762 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.27537759883751456 step loss 0.33254825030481727 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3098528363204261 step loss 0.3287167638352251 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.24777028023015824 step loss 0.3274240229948868 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2824104372556783 step loss 0.34094871888692 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.30168266503123276 step loss 0.3315158794176655 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30991339316690386 step loss 0.32304373093927113\n",
      "    beta 0.24706479922743652\n",
      "    rssi_w [-0.09225507 -0.03835916  0.20779519  0.19616545]\n",
      "    rssi_th [29.00383935 20.00377375 35.99687701]\n",
      "    infect_w [-0.02313488  0.09997147  0.32300484]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0268411340218473 step loss 1.0266487973957241\n",
      "iter 0 sigmoid loss 1.0223135992904222 step loss 0.6321602044295461 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.31230767514244673 step loss 0.31419583184840894 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.2939397419853703 step loss 0.2976057751234797 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3186886246721539 step loss 0.2805673564093317 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.31279275482712465 step loss 0.472573501767167 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.24889500846355056 step loss 0.5279823734863572 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.24573026265541792 step loss 0.28736376473271874 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.17095897572158655 step loss 0.2606262767320319 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.24094577156780503 step loss 0.2513552911294643 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.22306664110468652 step loss 0.2500201737192396 sigmoid temp 1.0\n",
      "End sigmoid loss 0.24085525018082787 step loss 0.2515791994867952\n",
      "    beta 0.295672622549381\n",
      "    rssi_w [-0.03164335 -0.02234683  0.06816968  0.30315742]\n",
      "    rssi_th [12.00361296 27.00361929 25.00323664]\n",
      "    infect_w [0.00306017 0.03513377 0.30808236]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9390940890024152 step loss 0.9371965487314721\n",
      "iter 0 sigmoid loss 0.9343324391241532 step loss 0.6674767816547984 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3463482275755533 step loss 0.35843742586010635 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30055663619528333 step loss 0.32850452856223694 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30926887605252223 step loss 0.30852103166693134 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3026805140001392 step loss 0.30056177770798975 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2473837046743271 step loss 0.3185193435828881 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2701627910954156 step loss 0.2939600376447682 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21620013954411882 step loss 0.2862455865630266 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2684452149468559 step loss 0.2849267478331681 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2722652693491858 step loss 0.2821448956725052 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2721432080114936 step loss 0.28275430960526104\n",
      "    beta 0.32629780854765966\n",
      "    rssi_w [-0.06765316 -0.02421641  0.1230735   0.35122977]\n",
      "    rssi_th [26.98816212 15.98812446 28.98670742]\n",
      "    infect_w [-0.00773397  0.05503144  0.38716006]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.009873767145957 step loss 1.0154980658978043\n",
      "iter 0 sigmoid loss 1.001346140742862 step loss 0.643558623756567 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.30219716488713844 step loss 0.3088504127602089 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.28792231793587797 step loss 0.29411022272910275 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30755810374247067 step loss 0.27681133169618743 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3173552592422565 step loss 0.2916625780194401 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.33520682179030503 step loss 0.31351861265221614 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3114154007777099 step loss 0.29349192477470654 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21028255381481412 step loss 0.28101508466886665 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2725427736444487 step loss 0.26386361178176526 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.22414357511092994 step loss 0.2555271479151027 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30694891027731397 step loss 0.3083261280744437\n",
      "    beta 0.03425330135909943\n",
      "    rssi_w [0.06265861 0.0799978  0.12309871 0.37879034]\n",
      "    rssi_th [21.99968989 27.99969156 14.00127127]\n",
      "    infect_w [0.0685701  0.0758079  0.25987471]\n",
      "best loss 0.2515791994867952\n",
      "best scoring parameters\n",
      "    beta 0.295672622549381\n",
      "    rssi_w [-0.03164335 -0.05399018  0.0141795   0.31733692]\n",
      "    rssi_th [-107.99638704  -80.99276775  -55.98953112]\n",
      "    infect_w [0.00306017 0.03819394 0.3462763 ]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.434, median size 3\n",
      "\t Negative bags: mean size 3.483, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 15399) (15399, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 15399) (852, 15399)\n",
      "Average positive samples per bag: 2.0056338028169014\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0764540839697798 step loss 1.0680860619494743\n",
      "iter 0 sigmoid loss 1.0571995477007587 step loss 0.5923029620621589 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.35349329524167694 step loss 0.3428925034902044 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3282976187412433 step loss 0.33319961108215057 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2875475354758166 step loss 0.33293641250709827 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.27049013474742095 step loss 0.34437576566110933 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2856665415763507 step loss 0.34715589405350583 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25219977081734934 step loss 0.34070224888127904 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3075139183737931 step loss 0.3326408952534338 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.34907251021461183 step loss 0.32289887930971967 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3354762252465908 step loss 0.31863772601471907 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2982569591401199 step loss 0.31058687584263783\n",
      "    beta 0.16773482136290807\n",
      "    rssi_w [-0.03025482  0.02445459  0.08361863  0.3236848 ]\n",
      "    rssi_th [31.99791559 18.99782999 19.99422803]\n",
      "    infect_w [-0.00421518  0.05111055  0.35084305]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9837427463527523 step loss 0.9853270351952809\n",
      "iter 0 sigmoid loss 0.9657276046484056 step loss 0.6374495920030804 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36043785387132254 step loss 0.34884306367090734 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34108294889277724 step loss 0.33786101785254563 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.31556670800770914 step loss 0.3250590433665159 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.270607229444227 step loss 0.8377032127425301 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3011086423980984 step loss 0.8309427974479386 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-395ab36fc33b>:19: RuntimeWarning: overflow encountered in exp\n",
      "  bag_probs = 1 - np.exp(-bag_scores)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000 sigmoid loss 0.361063284980753 step loss 0.3780265261237605 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34023816510789084 step loss 0.34353019935326723 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.37793763709219325 step loss 0.32696753404057216 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3295722714835959 step loss 0.31347956858420006 sigmoid temp 1.0\n",
      "End sigmoid loss 0.29408189928379236 step loss 0.30341406381645697\n",
      "    beta 0.1484755444118967\n",
      "    rssi_w [-0.01813235  0.00640908  0.0569794   0.41795193]\n",
      "    rssi_th [20.99382379 13.9938754  33.99351701]\n",
      "    infect_w [-0.00643011  0.06611393  0.3235244 ]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0169659204554367 step loss 1.0119708920559902\n",
      "iter 0 sigmoid loss 0.9965926446947005 step loss 0.611458029663613 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3546095576688197 step loss 0.3367901473824934 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.326642512513709 step loss 0.3229570700660556 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.27688344697871226 step loss 0.31763805316651095 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.25173923305856977 step loss 0.40874838635468697 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37310314317143 step loss 0.42301222589244375 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32332039586700345 step loss 0.39959244176702746 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34804359614898234 step loss 0.35846935182046524 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3704301056595952 step loss 0.34568069579463845 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31154266932138724 step loss 0.33259063746122214 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2982127213464786 step loss 0.30633768993297\n",
      "    beta 0.07736834789208158\n",
      "    rssi_w [-0.02128961 -0.00127299  0.03391992  0.31068733]\n",
      "    rssi_th [21.0046793  29.00467287 11.00732811]\n",
      "    infect_w [0.06833397 0.03146502 0.45051903]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.8546811667772868 step loss 0.8548432792437243\n",
      "iter 0 sigmoid loss 0.8359833828888756 step loss 0.6537020282676334 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.40197094134946665 step loss 0.3803343009296961 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.33097630155924274 step loss 0.34460344288853473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2951258644571462 step loss 0.33704474602620077 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2614404242516807 step loss 0.35376099087239266 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3056894909194974 step loss 0.4295898218880118 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2823663444008604 step loss 0.3355202948496509 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.29809695373233 step loss 0.32701793984980787 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.38337026045441064 step loss 0.3235559202587417 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3226554971590123 step loss 0.32344237014282246 sigmoid temp 1.0\n",
      "End sigmoid loss 0.31722687366499713 step loss 0.3240929079117391\n",
      "    beta 0.30009501972007013\n",
      "    rssi_w [-0.06461549 -0.04439714  0.14008108  0.33291061]\n",
      "    rssi_th [18.99103118 23.99103123 31.98874044]\n",
      "    infect_w [0.00259953 0.03525011 0.37475352]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9244275800577914 step loss 0.9322311183768813\n",
      "iter 0 sigmoid loss 0.9046230489237681 step loss 0.6887417478054525 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3540892576095851 step loss 0.3528404388882456 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.331726770278583 step loss 0.34909699551566103 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30601224684148687 step loss 0.3505912417700841 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2659980496724373 step loss 1.1367359919725215 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30134408950073616 step loss 1.1685634029316383 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2691861153941832 step loss 0.3480156886758861 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2784945991477422 step loss 0.3278522008633694 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.37157799266368174 step loss 0.32378965584476394 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31792484603835364 step loss 0.3237929265458603 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3074184051366011 step loss 0.32492785615681796\n",
      "    beta 0.3460132558779193\n",
      "    rssi_w [-0.04102027  0.06576481  0.33833382  0.08359598]\n",
      "    rssi_th [35.98657379 36.98597718 18.9992383 ]\n",
      "    infect_w [0.00306472 0.03050086 0.35469057]\n",
      "best loss 0.30341406381645697\n",
      "best scoring parameters\n",
      "    beta 0.1484755444118967\n",
      "    rssi_w [-0.01813235 -0.01172326  0.04525614  0.46320807]\n",
      "    rssi_th [-99.00617621 -85.01230081 -51.01878381]\n",
      "    infect_w [-0.00643011  0.05968383  0.38320823]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.931, median size 3\n",
      "\t Negative bags: mean size 3.930, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17416) (17416, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 17416) (860, 17416)\n",
      "Average positive samples per bag: 1.9985915492957746\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.074308041622844 step loss 1.0921744240159226\n",
      "iter 0 sigmoid loss 0.8619189928794129 step loss 0.6518237903780099 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.26507007930097654 step loss 0.3485297353464759 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4446410466742245 step loss 0.7253296453474026 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4017298907253675 step loss 0.3675385912175915 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2990026053676507 step loss 0.7399962098287965 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.367577239586316 step loss 0.36139885138598016 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32620531303128203 step loss 0.32421556809137225 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.36144595221694326 step loss 0.3373443413782715 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3493079336246524 step loss 0.31924415633707787 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2074003097736241 step loss 0.2902538630602254 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2724944469840246 step loss 0.28339924249703924\n",
      "    beta 0.13189956687688068\n",
      "    rssi_w [-0.00397016  0.02179501  0.35264055  0.12778646]\n",
      "    rssi_th [28.99852922 35.99834323 19.99927207]\n",
      "    infect_w [4.15408403e-04 4.62507655e-02 4.27374179e-01]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1103325337201766 step loss 1.1133034319288388\n",
      "iter 0 sigmoid loss 0.8924771371095206 step loss 0.6400954788247705 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.26624823027622563 step loss 0.3447295402164742 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.43550623345766565 step loss 0.3436227707596619 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36062109468326087 step loss 0.5540757471136449 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2882741619257891 step loss 0.5828633049353199 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2935349141016008 step loss 0.5657585950665054 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.3025930930340457 step loss 1.845195097136538 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 2.129899361060242 step loss 1.845195097136538 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.245028515709444 step loss 1.845195097136538 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.9210432372436185 step loss 1.845195097136538 sigmoid temp 1.0\n",
      "End sigmoid loss 1.845195097136538 step loss 1.845195097136538\n",
      "    beta -0.00539213834981353\n",
      "    rssi_w [0.07355895 0.13715468 0.26202025 0.09229795]\n",
      "    rssi_th [34.00018558 26.99999996 29.99967401]\n",
      "    infect_w [0.78599135 0.28040119 0.359496  ]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9139636998553873 step loss 0.9196519283333209\n",
      "iter 0 sigmoid loss 0.7322910048120181 step loss 0.6752741017282132 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33898886978636883 step loss 0.39964662994535144 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4005466970333886 step loss 0.4163286079163426 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.30548559455380114 step loss 0.40200219758226224 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.28871977080782335 step loss 0.41519961757655566 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.29267673011679984 step loss 0.41364174910859974 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000 sigmoid loss 0.2968253711135796 step loss 0.4000035851690708 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3745890384429692 step loss 0.3561649864528792 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.38725731952714026 step loss 0.36662921300409795 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2500555710165875 step loss 0.36434578255789307 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3388179772676223 step loss 0.3623049301904934\n",
      "    beta 0.14613440990341997\n",
      "    rssi_w [-0.10771422 -0.00399769  0.23925365  0.21491513]\n",
      "    rssi_th [35.00733858 15.00708283 36.99725229]\n",
      "    infect_w [-0.02483374  0.06426976  0.3276071 ]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.020502828122302 step loss 1.0352542578316004\n",
      "iter 0 sigmoid loss 0.8159785647510587 step loss 0.6886808369313763 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.2664697308527969 step loss 0.3406726903657647 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4181667215903628 step loss 0.5629326450987093 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3533212567049334 step loss 0.5344117634128035 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.30055805952540743 step loss 0.5780934627905712 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.37640688093125974 step loss 0.5600865389669549 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3420271277728741 step loss 0.426859316745658 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3380468728916101 step loss 0.33433941994464506 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.34053239520319406 step loss 0.3009536333940574 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.1969585954083506 step loss 0.2857748861956967 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2742300279904177 step loss 0.2848308848231574\n",
      "    beta 0.13760233263649937\n",
      "    rssi_w [-0.01767127  0.03267217  0.2938986   0.07462121]\n",
      "    rssi_th [34.00408502 29.00391847 29.99971742]\n",
      "    infect_w [0.00529859 0.02420644 0.4476416 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1543778445741473 step loss 1.163442077431679\n",
      "iter 0 sigmoid loss 0.9286415499628997 step loss 0.5845306563581355 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.26845824188101114 step loss 0.3582968875396573 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4284872456262534 step loss 0.36066583169919425 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32027423714905423 step loss 0.7078032422219086 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3011595255506872 step loss 0.7247499019694966 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28784294982672615 step loss 0.9059959322767932 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.26739782660480726 step loss 0.3163899687525215 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.29161851414725865 step loss 0.30686736682965626 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.31407569874828395 step loss 0.2993400721677094 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2076803107072706 step loss 0.3104873231794138 sigmoid temp 1.0\n",
      "End sigmoid loss 0.28354476097922127 step loss 0.3021113292328123\n",
      "    beta 0.3193823764318203\n",
      "    rssi_w [-0.0400224  -0.03125329  0.08747432  0.31810784]\n",
      "    rssi_th [10.99042496 28.99042374 28.98979827]\n",
      "    infect_w [0.00128175 0.02095975 0.3506989 ]\n",
      "best loss 0.28339924249703924\n",
      "best scoring parameters\n",
      "    beta 0.13189956687688068\n",
      "    rssi_w [-0.00397016  0.01782485  0.37046539  0.49825185]\n",
      "    rssi_th [-91.00147078 -55.00312755 -35.00385548]\n",
      "    infect_w [4.15408403e-04 4.66661739e-02 4.74040353e-01]\n",
      "total: 33600, positives: 5383, negatives: 28217\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.961, median size 3\n",
      "\t Negative bags: mean size 3.927, median size 3\n",
      "assign_mat size, X_shuff size: (4431, 17425) (17425, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3544, 17425) (856, 17425)\n",
      "Average positive samples per bag: 1.9464788732394367\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.3387237447419145 step loss 1.3447208390311294\n",
      "iter 0 sigmoid loss 1.4562781881829232 step loss 0.45767753117627785 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3481410442884669 step loss 0.36325707517502465 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.41914099900034957 step loss 0.3527017760912043 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3148681431432876 step loss 0.3473734658047806 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3123269871352352 step loss 0.8738140103561342 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.33996594301944816 step loss 0.9109644721564347 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3135136527935177 step loss 0.3344141973991924 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34145795792662825 step loss 0.32457008597150916 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2348321126410569 step loss 0.3243639474936104 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2781944057533604 step loss 0.32525861347697055 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3035439403433146 step loss 0.3189210492412512\n",
      "    beta 0.31314490894486224\n",
      "    rssi_w [-0.02473453  0.04985327  0.32375727  0.02382609]\n",
      "    rssi_th [32.98770738 36.98733673 37.99985511]\n",
      "    infect_w [-0.00811771  0.04365196  0.32575459]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9786152153913475 step loss 0.9794428293547269\n",
      "iter 0 sigmoid loss 1.0669045608887566 step loss 0.5983972407645848 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33318124606905813 step loss 0.3854177964486685 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3684217173033565 step loss 0.8204765893973324 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.27987095202144674 step loss 0.8822933899652167 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.29966499117265044 step loss 0.9240478917716214 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.32954169800155186 step loss 0.876664527690675 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31359021857634845 step loss 0.3360958672345581 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34050164612326583 step loss 0.3229324062558429 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.23454024657409522 step loss 0.32472663501047544 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.27863828994626744 step loss 0.3253870886276926 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30395973462434234 step loss 0.31874734258432524\n",
      "    beta 0.3013817790302852\n",
      "    rssi_w [-0.02964597 -0.00723343  0.06357961  0.32498458]\n",
      "    rssi_th [18.98797203 15.9879617  34.98770693]\n",
      "    infect_w [-0.00862474  0.04493073  0.33733376]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9544463217078807 step loss 0.9792354665788661\n",
      "iter 0 sigmoid loss 1.0404354182282909 step loss 0.64696648714194 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.34070062150823466 step loss 0.34770865527568123 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.40874321735420743 step loss 0.3453046412755628 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.27839820458634984 step loss 0.634447588869537 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.29600607370348214 step loss 0.7034770872398431 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.32839375928576586 step loss 0.6565211522569324 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.38445010795026796 step loss 0.4867650433511203 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4489346076791588 step loss 0.3692106762775337 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3252474315462943 step loss 0.3658628518973308 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3267543354740927 step loss 0.36295468380163853 sigmoid temp 1.0\n",
      "End sigmoid loss 0.35188881946630074 step loss 0.35800987868200324\n",
      "    beta 0.034852975558285926\n",
      "    rssi_w [-0.00366661  0.01793733  0.2103066   0.12058636]\n",
      "    rssi_th [28.00225065 35.00217277 14.99958845]\n",
      "    infect_w [0.3215021  0.19159309 0.38067324]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9424909059652559 step loss 0.9580765746650205\n",
      "iter 0 sigmoid loss 1.02361018444839 step loss 0.6490522457922852 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3401700509676124 step loss 0.34721779199225933 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4087034668862255 step loss 0.3466362643498007 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2800384019966229 step loss 0.5680180771727921 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.29683056268258956 step loss 0.6485383104878711 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3289864097379754 step loss 0.5942496496551237 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000 sigmoid loss 0.3314354943827539 step loss 0.30033253227540013 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3350205396755239 step loss 0.30194101077247143 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.23215721334821493 step loss 0.302532503440396 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3069407341937681 step loss 0.3014733324683184 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2932807054399969 step loss 0.29980743486513833\n",
      "    beta 0.2470613719689595\n",
      "    rssi_w [-0.00613804  0.02256573  0.19495839  0.17723609]\n",
      "    rssi_th [28.0084664  33.00839793 16.99841104]\n",
      "    infect_w [-0.00620999  0.03736829  0.2656186 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.978260629044742 step loss 0.9807382829668692\n",
      "iter 0 sigmoid loss 1.064646446431703 step loss 0.5715855998477232 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.34838659944976974 step loss 0.35236041432211956 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.47517605188874584 step loss 0.3970444533548932 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3777939919011915 step loss 0.3701118754657488 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.0856864543907967 step loss 1.0721274595945356 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.6982000927774817 step loss 0.7055016039169326 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3904644110731229 step loss 0.3961375999768722 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.4446908706051314 step loss 0.3536366337772732 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2869161457081573 step loss 0.34859547664356133 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3248866114726301 step loss 0.3455494051152873 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3413835437974398 step loss 0.34130006350781694\n",
      "    beta 0.05182453499387163\n",
      "    rssi_w [0.14070578 0.14209401 0.1354966  0.33130185]\n",
      "    rssi_th [12.99880172 13.99895686 37.99879138]\n",
      "    infect_w [-0.0441648   0.08184584  0.18030195]\n",
      "best loss 0.29980743486513833\n",
      "best scoring parameters\n",
      "    beta 0.2470613719689595\n",
      "    rssi_w [-0.00613804  0.01642769  0.21138608  0.38862218]\n",
      "    rssi_th [-91.9915336  -58.98313567 -41.98472463]\n",
      "    infect_w [-0.00620999  0.03115829  0.29677689]\n"
     ]
    }
   ],
   "source": [
    "# effect of varying bag size, given up to 3 positive exposures per bag\n",
    "bag_size = [1, 2, 4, 8, 16, 32]\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "idx = 0\n",
    "\n",
    "auc_train_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_train_true = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_true = np.zeros((len(bag_size),n_trials))\n",
    "for bs in bag_size:\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=bs,censor_prob_pos=0.,censor_prob_neg=0,max_pos_in_bag=3)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "BCg_QmvvqB4E",
    "outputId": "95acf06b-adf4-41dd-f92d-1c27e7e6f7c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1e89dd190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(bag_size, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(bag_size, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: No Censoring\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(bag_size, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(bag_size, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: No Censoring\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q07pCe8Mynaq"
   },
   "source": [
    "### 4. Robustness to Model Mismatch\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "Recognizing that the simulator may not be completely accurate or reflective of real data, slightly modifying the simulator by using a Taylor series approximation and then testing the performance of the model to see its robustness to slightly different generating models.\n",
    "\n",
    "This experiment is iterated over decreasing number of Taylor approximation terms to show the effect that increasing model mismatch has on the performance of the learning algorithm. Decreasing the number of terms used in the Taylor approximation of the generative function that directly calculates the probability of infection would increase model mismatch in that the approximation becomes less accurate to the original function. Hence, the probability of infection calculated should be less accurate as well.\n",
    "\n",
    "We expect the performance of the algorithm to decrease, i.e. the AUC score to fall, as model mismatch increases, or as the number of Taylor approximation terms decreases. \n",
    "\n",
    "This generic trend has been illustrated through the below two graphs. Reading from left to right following the x-axis (number of taylor terms increases), we do see a increase of AUC for the training set and testing sets. We can see they matche our expectation. However, for both sets the accuracy for using Inf terms (10 terms) is not clear that the infinity approximation seems to be less accurate than using less term. This phenomenon suggests two possibilities. First, our simulator may not be a very accruate model of how COVID is tranmitted, so that mismatch phenomenon is reflected not so completely. Second, our version of simulator is in general a robust one but with some noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making grid of 80 distances x 20 durations x 21 onsets = 33600 points\n",
      "Approx: [[[1.         1.         1.         ... 0.97538178 0.92937435 0.92675691]]\n",
      "\n",
      " [[0.49996063 0.49998181 0.46092688 ... 0.49969697 0.49750601 0.49731772]]\n",
      "\n",
      " [[0.67110338 0.66968263 0.81008083 ... 0.65435507 0.63129513 0.62997964]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.63518322 0.63414931 0.72087815 ... 0.62280329 0.60509304 0.60406036]]\n",
      "\n",
      " [[0.63539429 0.63435626 0.72199235 ... 0.62296994 0.60521186 0.60417686]]\n",
      "\n",
      " [[0.63536768 0.63433023 0.72181414 ... 0.62294962 0.60519806 0.60416337]]]\n",
      "Exact: [0.63537038 0.63433287 0.72183658 ... 0.62295162 0.60519936 0.60416464]\n",
      "(33600, 3) (33600,)\n",
      "total: 33600, positives: 5383, negatives: 28217\n"
     ]
    }
   ],
   "source": [
    "# initializing input data\n",
    "distances, durations, symptoms = uniform_input_data_grid(max_dur = 60,  max_dist = 5, ngrid_dist = 80, ngrid_dur=20, min_dist=0.1, min_dur=5)\n",
    "data = make_input_data(distances=distances, durations=durations, symptoms=symptoms)\n",
    "\n",
    "# initializing parameters\n",
    "ble_params = BleParams()\n",
    "rssi = atten_to_rssi(data['atten_grid'], ble_params)\n",
    "duration = data['duration_grid']\n",
    "symptom_day = data['symptom_grid']\n",
    "infectiousness = days_to_inf_levels(symptom_day)\n",
    "\n",
    "params = ModelParams()\n",
    "\n",
    "# Create model inputs\n",
    "X = np.concatenate([rssi[:,None], duration[:,None], infectiousness[:,None]], axis=1)\n",
    "approx_type = 'taylor'\n",
    "exp_taylor_terms = 8\n",
    "exp_temperature = np.arange(0.5,1.,0.09)\n",
    "\n",
    "# calculating probability of infection\n",
    "prob_infect_exact, prob_infect_approx = prob_infection_grid(data, params, approx=approx_type, exp_taylor_terms=exp_taylor_terms, temp=exp_temperature)\n",
    "prob_infect_approx = np.array(prob_infect_approx)\n",
    "# manually inspect the approximation error (higher error for larger values -- away from 0)\n",
    "ind = np.where(prob_infect_exact>0.6)\n",
    "print('Approx:', prob_infect_approx[:,ind])\n",
    "print('Exact:', prob_infect_exact[ind])\n",
    "\n",
    "\n",
    "print (X.shape, prob_infect_exact.shape)\n",
    "\n",
    "\n",
    "simdata = dict()\n",
    "simdata['X'] = X\n",
    "simdata['prob_infect_exact'] = prob_infect_exact\n",
    "simdata['prob_infect_approx'] = prob_infect_approx\n",
    "\n",
    "X_epi = simdata['X']  \n",
    "probabilities_true_epi = simdata['prob_infect_exact']\n",
    "probabilities_true_epi_approx = simdata['prob_infect_approx'].T  # N x num_approx_steps\n",
    "N = len(probabilities_true_epi)\n",
    "perm = np.random.permutation(N)\n",
    "X_epi = X_epi[perm, :]\n",
    "probabilities_true_epi = probabilities_true_epi[perm]\n",
    "probabilities_true_epi_approx = np.clip(probabilities_true_epi_approx[perm,:], 1e-5, 1-1e-5)\n",
    "  \n",
    "# generating labels for the exposure events\n",
    "def sample_labels(probabilities):\n",
    "  Y = bernoulli.rvs(probabilities)\n",
    "  N_pos = np.sum(Y)\n",
    "  N_neg = np.sum(1-Y)\n",
    "  print(\"total: {}, positives: {}, negatives: {}\".format(N, N_pos, N_neg))\n",
    "  pos_neg_ratio= float(N_neg) / N_pos\n",
    "  return Y, N_pos, N_neg, pos_neg_ratio\n",
    "\n",
    "Y_epi, N_pos, N_neg, pos_neg_ratio = sample_labels(probabilities_true_epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lir4dyO8yoeg",
    "outputId": "7f574726-7409-427a-c53e-3539cbe36f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5403, negatives: 28197\n",
      "total: 33600, positives: 5403, negatives: 28197\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.455, median size 2\n",
      "\t Negative bags: mean size 2.431, median size 2\n",
      "assign_mat size, X_shuff size: (4415, 10750) (10750, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3532, 10750) (812, 10750)\n",
      "Average positive samples per bag: 2.0126760563380284\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9761889113483901 step loss 0.9897511676748867\n",
      "iter 0 sigmoid loss 1.021417163339004 step loss 0.6673122182516648 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3314837361232751 step loss 0.31720750425336 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3065881446743266 step loss 0.3054449968220978 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2324311130960724 step loss 0.3040847644574086 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.24234261433300083 step loss 0.4116515374821078 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2565302668174569 step loss 0.41345551129455405 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2888643753801915 step loss 0.370467790100824 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.22727234067173363 step loss 0.3347709573328102 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.27341499163941485 step loss 0.2992725134475831 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3411246082128925 step loss 0.28775174730629194 sigmoid temp 1.0\n",
      "End sigmoid loss 0.277143018015852 step loss 0.2860360440538287\n",
      "    beta 0.30307344167358463\n",
      "    rssi_w [-0.00676378  0.01519916  0.07773313  0.29091915]\n",
      "    rssi_th [24.001531   30.0015014  19.99473793]\n",
      "    infect_w [-0.00799261  0.04951506  0.29049198]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.968023617363326 step loss 0.9728609858012375\n",
      "iter 0 sigmoid loss 1.0019860014155144 step loss 0.6730036255218729 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33419449577896804 step loss 0.3181235045856165 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.31082430858523574 step loss 0.3082808225301001 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2364051771117516 step loss 0.2955240472786161 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.24442268875306944 step loss 0.2889273655688886 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.24461151569188908 step loss 0.30457098414098405 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.28938259001506034 step loss 0.2733694808482109 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2102717666009019 step loss 0.27291754327663986 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.26802155851665466 step loss 0.2755259246007242 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.33567823229071964 step loss 0.27335155099550135 sigmoid temp 1.0\n",
      "End sigmoid loss 0.26304489213556775 step loss 0.2711930245305742\n",
      "    beta 0.33133395060659365\n",
      "    rssi_w [-0.06376326 -0.00828931  0.10052504  0.30415331]\n",
      "    rssi_th [33.99891328  9.99887129 22.99795373]\n",
      "    infect_w [-0.0052266   0.0459258   0.31978485]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9944521363498497 step loss 0.9940919170459281\n",
      "iter 0 sigmoid loss 1.0384083235417358 step loss 0.6439706823245083 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.333426529884174 step loss 0.322875030243759 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30902981955595704 step loss 0.3125074993805202 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.23420521370856184 step loss 0.3012790412153068 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.24356843531358588 step loss 0.28625936799066476 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.24583937628537794 step loss 0.2808984544305078 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2857821862178712 step loss 0.2787694550609737 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21190799338221397 step loss 0.27668228800121086 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.26873331200295086 step loss 0.27711588809784676 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3394647727301139 step loss 0.2763873465400654 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2657202410380499 step loss 0.2763207169382355\n",
      "    beta 0.3280076819016763\n",
      "    rssi_w [-0.07689807 -0.00524139  0.1178018   0.31517038]\n",
      "    rssi_th [36.99547059 10.99535589 20.99440692]\n",
      "    infect_w [-0.00657197  0.04959745  0.31825546]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.2678925249196522 step loss 1.2849863547911433\n",
      "iter 0 sigmoid loss 1.3133883282475813 step loss 0.5132591518544045 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33494757096069266 step loss 0.33178308448144983 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.31588530176053253 step loss 0.3279774439603084 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.24731791774147183 step loss 0.32728789901944394 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2770053358352593 step loss 0.3268131596752183 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2804388225749945 step loss 0.33889945843864927 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31972820714529376 step loss 0.3545033754665234 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2601006252869082 step loss 0.35035293433906095 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2987460512085311 step loss 0.34560161647766807 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.34932014788409305 step loss 0.3394198475696733 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3133551508624793 step loss 0.3389175187900252\n",
      "    beta 0.2687508768832835\n",
      "    rssi_w [-0.03529322 -0.01387122  0.16055963  0.20727079]\n",
      "    rssi_th [20.00218555 30.00217516 32.99673548]\n",
      "    infect_w [-0.00533397  0.04133009  0.25431648]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0021816309505907 step loss 1.003686388446959\n",
      "iter 0 sigmoid loss 1.0416508713322334 step loss 0.6862948741134087 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3332857461399182 step loss 0.3125781916197977 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3098418632510754 step loss 0.2991039513907258 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.23889016229828333 step loss 0.2880895023750064 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.25282917898125207 step loss 0.4359781078795731 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2509626681015402 step loss 0.5215287578032193 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3007228572258948 step loss 0.4114490334351089 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21733861623692013 step loss 0.28792875652661587 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2625391389284848 step loss 0.2811229370730432 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31146487878656487 step loss 0.2796410210849009 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27094713895255557 step loss 0.2788822506106673\n",
      "    beta 0.26602883623484386\n",
      "    rssi_w [-0.00745859  0.02697996  0.25108037  0.05160191]\n",
      "    rssi_th [28.00911343 32.00901453 38.99980941]\n",
      "    infect_w [-0.00158539  0.03502961  0.2535229 ]\n",
      "best loss 0.2711930245305742\n",
      "best scoring parameters\n",
      "    beta 0.33133395060659365\n",
      "    rssi_w [-0.06376326 -0.07205256  0.02847248  0.33262579]\n",
      "    rssi_th [-86.00108672 -76.00221543 -53.0042617 ]\n",
      "    infect_w [-0.0052266   0.0406992   0.36048405]\n",
      "total: 33600, positives: 5424, negatives: 28176\n",
      "total: 33600, positives: 5424, negatives: 28176\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.423, median size 2\n",
      "\t Negative bags: mean size 2.445, median size 2\n",
      "assign_mat size, X_shuff size: (4398, 10739) (10739, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3518, 10739) (814, 10739)\n",
      "Average positive samples per bag: 1.976056338028169\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0117763336022245 step loss 1.0069395557468184\n",
      "iter 0 sigmoid loss 0.8378884027795246 step loss 0.6708688470144607 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3368251838635717 step loss 0.41389610596273607 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3769180261606765 step loss 0.38656084659537177 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.38724303773576396 step loss 0.3750461597189268 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.39456613046966654 step loss 0.36778899923802166 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.34331204753007016 step loss 0.35566794951202285 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.37623683322062407 step loss 0.37753953648110766 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3246350333497148 step loss 0.34692035295846047 sigmoid temp 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4000 sigmoid loss 0.31200120179889007 step loss 0.3219838790594384 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.29435962872346266 step loss 0.3100061046808495 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2847850168481446 step loss 0.29039190893193384\n",
      "    beta 0.08023396979851541\n",
      "    rssi_w [-0.01629494 -0.00301911  0.02996466  0.28172842]\n",
      "    rssi_th [15.01033151 15.01037937 28.01033439]\n",
      "    infect_w [0.03016635 0.10810942 0.41291908]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9413684972978628 step loss 0.9413802128677877\n",
      "iter 0 sigmoid loss 0.7760000274832266 step loss 0.6944538456113053 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.360834658239298 step loss 0.3192338694126401 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3185238389103641 step loss 0.30709457505287957 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.28578073056880654 step loss 0.2848294022250643 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2795304098014802 step loss 0.44957906487704946 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3462518974083888 step loss 0.3632899113097922 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3875866454946115 step loss 0.3498637453110776 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.28848690559138807 step loss 0.3353688580834498 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3013910658129923 step loss 0.3250415467353563 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.29929572451848363 step loss 0.3160620315463743 sigmoid temp 1.0\n",
      "End sigmoid loss 0.299865179916498 step loss 0.3067889802694238\n",
      "    beta 0.04842205475187311\n",
      "    rssi_w [-0.02531797 -0.01565455  0.05511326  0.35444505]\n",
      "    rssi_th [15.00838154 22.00841588 23.0082105 ]\n",
      "    infect_w [0.17417672 0.05982531 0.44173847]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0849154183558944 step loss 1.0916862803444172\n",
      "iter 0 sigmoid loss 0.8872060700720827 step loss 0.6907112289596767 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.33469898108789153 step loss 0.33146786988992843 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34743972960285563 step loss 0.33400224023608577 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2809789448992591 step loss 0.2918311166841746 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2726317369474811 step loss 0.3314347481340991 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3383864402563415 step loss 0.2926073506850617 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29716423146324933 step loss 0.27055393461311317 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.23893931271230662 step loss 0.2688463944368926 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.24033934201229257 step loss 0.26358041453612013 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.24263334148932844 step loss 0.2631347961806768 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2537617687363822 step loss 0.2627025116961782\n",
      "    beta 0.3371289367641971\n",
      "    rssi_w [-0.040126   -0.03296138  0.0959144   0.34913089]\n",
      "    rssi_th [14.9927602  27.99276705 24.99180444]\n",
      "    infect_w [0.00380624 0.03530965 0.34326312]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9157290981238239 step loss 0.9228633898108545\n",
      "iter 0 sigmoid loss 0.7575297472053942 step loss 0.7152919746397449 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.35961934795340006 step loss 0.3208463532639252 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.2998315856349841 step loss 0.3082193754729461 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2755034381134897 step loss 0.2889302792889755 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3302262540013634 step loss 0.3411623436801547 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2480004507474863 step loss 0.3141264680829214 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3040772272513412 step loss 0.2660123679225523 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2308845594677694 step loss 0.2579944498190688 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3525520554856123 step loss 0.3234347422412836 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.22816617770281725 step loss 0.2582776618850651 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2518396800430423 step loss 0.2568939412738871\n",
      "    beta 0.26095541288674295\n",
      "    rssi_w [-0.06082025 -0.00910146  0.0928565   0.32482694]\n",
      "    rssi_th [33.00618059 15.00612352 15.00611034]\n",
      "    infect_w [0.00416903 0.03170053 0.27041203]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1001888447798418 step loss 1.1127444288838548\n",
      "iter 0 sigmoid loss 0.906393036539964 step loss 0.6914389309506154 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36205242434491663 step loss 0.33162999168572915 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3213998012702977 step loss 0.33121916061746826 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.29198747245155227 step loss 0.3192756752947955 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.29394620420100537 step loss 0.3407265647064768 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.25784585010464817 step loss 0.39982661886745574 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4330871047907751 step loss 0.44125337677557896 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3530396377931195 step loss 0.37438579795410154 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3858833679954858 step loss 0.3676571197269924 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.37151633191186745 step loss 0.3580590006084795 sigmoid temp 1.0\n",
      "End sigmoid loss 0.34516110389964894 step loss 0.3456239705596689\n",
      "    beta 0.025185407978478458\n",
      "    rssi_w [0.07754928 0.10957033 0.34185954 0.12549475]\n",
      "    rssi_th [21.00738585 31.00735547 37.99925797]\n",
      "    infect_w [ 0.12554983 -0.01694648  0.42556761]\n",
      "best loss 0.2568939412738871\n",
      "best scoring parameters\n",
      "    beta 0.26095541288674295\n",
      "    rssi_w [-0.06082025 -0.06992171  0.02293479  0.34776173]\n",
      "    rssi_th [-86.99381941 -71.98769589 -56.98158555]\n",
      "    infect_w [0.00416903 0.03586956 0.30628159]\n",
      "total: 33600, positives: 5476, negatives: 28124\n",
      "total: 33600, positives: 5476, negatives: 28124\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.444, median size 2\n",
      "\t Negative bags: mean size 2.403, median size 2\n",
      "assign_mat size, X_shuff size: (4356, 10498) (10498, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3484, 10498) (805, 10498)\n",
      "Average positive samples per bag: 1.9985915492957746\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.050700000937514 step loss 1.0723877280350322\n",
      "iter 0 sigmoid loss 1.1071175616223914 step loss 0.6711965237567618 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.43292503230846596 step loss 0.3246052990000388 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.2819879585152071 step loss 0.30853333476192896 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.31931761842551337 step loss 0.3941470826176214 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.31516532796343644 step loss 0.46906301056796923 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.38297519848429074 step loss 0.3840555453742579 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.0723347837356414 step loss 1.8769721079933666 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.554253587814231 step loss 1.8769721079933666 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.7845118971126357 step loss 1.8769721079933666 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 2.0723347837356414 step loss 1.8769721079933666 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8769721079933666 step loss 1.8769721079933666\n",
      "    beta -0.016475104006141827\n",
      "    rssi_w [0.27656435 0.28514252 0.28777727 0.18103096]\n",
      "    rssi_th [18.99926397 37.99922805 19.99945653]\n",
      "    infect_w [0.66316829 0.50897839 0.21102604]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0442526544167743 step loss 1.0503217830063107\n",
      "iter 0 sigmoid loss 1.1019051284865942 step loss 0.647633348613962 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.401343003586174 step loss 0.32537218980675503 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.28532678823428415 step loss 0.307952017425985 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2760122183108975 step loss 0.4003428641982124 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2841734466750489 step loss 0.4281370271624417 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.27573362407926766 step loss 0.3917739705524372 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31819746347630734 step loss 0.3741094015001002 sigmoid temp 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3500 sigmoid loss 0.26420561746554555 step loss 0.33854965358335554 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3908630070758818 step loss 0.43167301741075464 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3366091313470006 step loss 0.29782514793951603 sigmoid temp 1.0\n",
      "End sigmoid loss 0.28899084562041794 step loss 0.29483027977583626\n",
      "    beta 0.209520112423489\n",
      "    rssi_w [-0.00697141  0.01390192  0.12561576  0.27034064]\n",
      "    rssi_th [25.00654784 29.00651161 21.99549058]\n",
      "    infect_w [0.00590323 0.03780186 0.3228653 ]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0280442107310195 step loss 1.027821355378588\n",
      "iter 0 sigmoid loss 1.086722516859532 step loss 0.6408307967415539 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.40645069620904406 step loss 0.33775781913485664 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30281622381367584 step loss 0.324872342092692 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.31669410942781384 step loss 0.3188704638686423 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.292316601531116 step loss 0.3654068035556563 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3500026588068616 step loss 5.423202209409318 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 9.440600681288322 step loss 9.635963357030672 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 9.958681877209935 step loss 9.635963357030672 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 9.728423567911442 step loss 9.635963357030672 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 9.440600681288322 step loss 9.635963357030672 sigmoid temp 1.0\n",
      "End sigmoid loss 9.635963357030672 step loss 9.635963357030672\n",
      "    beta 0.15241782341514065\n",
      "    rssi_w [11.38831537 11.15463686  9.3498625   6.26207465]\n",
      "    rssi_th [20.93561675 20.93706243 10.95421027]\n",
      "    infect_w [1.16309737 0.31034784 0.26400701]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 0.9299611304686863 step loss 0.9388259102450195\n",
      "iter 0 sigmoid loss 0.9835188730115777 step loss 0.673397489057002 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4305610507039188 step loss 0.31813719189807527 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.28458566522726086 step loss 0.29950320718198026 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.31836648098289116 step loss 0.3473382807445598 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.5131856091263282 step loss 0.3561350332588275 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2888142112690752 step loss 0.3455049474178999 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32146520965113157 step loss 0.31521138626629097 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2461729084251161 step loss 0.2883847846687525 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.282378135117418 step loss 0.2664535353397077 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3079096690862176 step loss 0.25603379796681586 sigmoid temp 1.0\n",
      "End sigmoid loss 0.24999508072537502 step loss 0.25622885412740715\n",
      "    beta 0.26205820685399883\n",
      "    rssi_w [-0.01048998  0.02100763  0.01090489  0.34379186]\n",
      "    rssi_th [31.00800991 22.00796399 10.00617811]\n",
      "    infect_w [0.00698751 0.02754638 0.26711649]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1490020579454652 step loss 1.1836940977310941\n",
      "iter 0 sigmoid loss 1.207669691524286 step loss 0.6502496503305356 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4033087114580763 step loss 0.34174653270349653 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.2952713960601649 step loss 0.3304013412871835 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.31038383241224526 step loss 0.3269793162920554 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.2985578203428798 step loss 0.3313195293458792 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2612842539546426 step loss 0.4422703900669067 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29233201013374543 step loss 0.330149188738576 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.25863148473779174 step loss 0.3183658604498668 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2990754464358117 step loss 0.31598758512110586 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.36941847939679 step loss 0.31476244973852785 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30672246957309696 step loss 0.31471993724270686\n",
      "    beta 0.3733514797848002\n",
      "    rssi_w [-0.05910714  0.09629025  0.34586091  0.02998736]\n",
      "    rssi_th [38.98748061 36.98628123 30.99969989]\n",
      "    infect_w [0.00481378 0.03880205 0.36468845]\n",
      "best loss 0.25622885412740715\n",
      "best scoring parameters\n",
      "    beta 0.26205820685399883\n",
      "    rssi_w [-0.01048998  0.01051766  0.02142254  0.3652144 ]\n",
      "    rssi_th [-88.99199009 -66.9840261  -56.97784799]\n",
      "    infect_w [0.00698751 0.03453389 0.30165038]\n",
      "total: 33600, positives: 5306, negatives: 28294\n",
      "total: 33600, positives: 5306, negatives: 28294\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.386, median size 2\n",
      "\t Negative bags: mean size 2.420, median size 2\n",
      "assign_mat size, X_shuff size: (4496, 10856) (10856, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3596, 10856) (819, 10856)\n",
      "Average positive samples per bag: 2.0211267605633805\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.9856274873057295 step loss 0.9799558500979308\n",
      "iter 0 sigmoid loss 0.9682141608623652 step loss 0.6407077235801452 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3138466771710027 step loss 0.3335773062484404 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3587820237725902 step loss 0.3327384441805366 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2799087525103408 step loss 0.3334165477158871 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.362688340581696 step loss 0.33640447250538597 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3039664099378745 step loss 0.36043142845852716 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3906976024585985 step loss 0.3654248607770023 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3844056309556697 step loss 0.3681576779759489 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28805357498371115 step loss 0.36617920464128195 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31005291915413635 step loss 0.3584751442071529 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3246888608239906 step loss 0.35744582872938957\n",
      "    beta 0.25709955780707217\n",
      "    rssi_w [-0.07610314 -0.06083233  0.02919978  0.22351129]\n",
      "    rssi_th [11.00994355 26.00994433 13.00972668]\n",
      "    infect_w [-0.03124629  0.09417402  0.22495596]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 0.9829364712501389 step loss 0.98618475218312\n",
      "iter 0 sigmoid loss 0.9662277209094887 step loss 0.6600962774133198 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.294194812605973 step loss 0.32092538387292796 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.33961494459625585 step loss 0.2997224982076951 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2528868064926071 step loss 0.2872706243514301 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3185889303067503 step loss 0.4712468215508598 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.29065161402500966 step loss 0.4805581735732966 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.0723347837356414 step loss 1.8185127764890128 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8420764744372367 step loss 1.8185127764890128 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8185127764890128 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.8996410517618378 step loss 1.8185127764890128 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8185127764890128 step loss 1.8185127764890128\n",
      "    beta -0.005400460026494636\n",
      "    rssi_w [0.16806696 0.17362854 0.28914621 0.2071164 ]\n",
      "    rssi_th [10.9998679  31.99986855 20.99946768]\n",
      "    infect_w [0.41367786 0.21035569 0.27560763]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0124933124392304 step loss 1.0075727421875813\n",
      "iter 0 sigmoid loss 0.9964176686503966 step loss 0.6400888507595542 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3103329003081068 step loss 0.31635495291732163 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3437536465964445 step loss 0.3080610825829011 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2644132398566424 step loss 0.3870929268827619 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3315991686247593 step loss 0.3909501448255509 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3216697568217667 step loss 0.28588942709175796 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.326243343051572 step loss 0.34770304052909734 sigmoid temp 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3500 sigmoid loss 0.34072908004203994 step loss 0.32679764613498086 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2780862334392855 step loss 0.33468625264282315 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.32055760016884877 step loss 0.32741954611150187 sigmoid temp 1.0\n",
      "End sigmoid loss 0.32717552212168066 step loss 0.32705576593331515\n",
      "    beta 0.012731011575406376\n",
      "    rssi_w [0.78012639 0.79438412 0.87307363 0.39969228]\n",
      "    rssi_th [17.99373204 26.99373631 15.99400732]\n",
      "    infect_w [-0.01638142  0.05370691  0.16654075]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.042065751830827 step loss 1.0389162206987155\n",
      "iter 0 sigmoid loss 1.0246137928883052 step loss 0.6211005420099378 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3124142733881703 step loss 0.316827986625363 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3494799453971373 step loss 0.3094626189193042 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.26952435941887537 step loss 0.29834074814418965 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3365601036329474 step loss 0.4364273215589634 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.297895661128944 step loss 0.46096510727340156 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29763943093813083 step loss 0.27937365442941425 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3166030577767268 step loss 0.27497476695186146 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.20904497888318738 step loss 0.27295966439280767 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2696691992383976 step loss 0.26679099215124763 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2614414143350133 step loss 0.2670003280288604\n",
      "    beta 0.25172498285531686\n",
      "    rssi_w [-0.02143688 -0.00547803  0.0415944   0.27449505]\n",
      "    rssi_th [19.00737525 10.00739364 32.00726635]\n",
      "    infect_w [0.00449546 0.03511774 0.2705482 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9499185074485678 step loss 0.9581813036171953\n",
      "iter 0 sigmoid loss 0.9339534369822425 step loss 0.6777168556619347 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.31571482323281 step loss 0.3301980860112648 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3501386846036756 step loss 0.3273568987853008 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.27333509707911274 step loss 0.32137190893397194 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35043203630703135 step loss 0.3189594610745801 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2964463773065993 step loss 0.33420235305300644 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31171578312938125 step loss 0.3177740056257855 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3081400130181046 step loss 0.31678844769216075 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.30598571490326715 step loss 0.3393066545390774 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2908917434975597 step loss 0.30499014020940796 sigmoid temp 1.0\n",
      "End sigmoid loss 0.29699592431707067 step loss 0.3057341102288989\n",
      "    beta 0.31913771353077036\n",
      "    rssi_w [-0.05692075 -0.02750026  0.12061046  0.28065534]\n",
      "    rssi_th [20.99003246 19.99002813 32.98860046]\n",
      "    infect_w [0.00407876 0.05471382 0.35025388]\n",
      "best loss 0.2670003280288604\n",
      "best scoring parameters\n",
      "    beta 0.25172498285531686\n",
      "    rssi_w [-0.02143688 -0.02691492  0.01467949  0.28917453]\n",
      "    rssi_th [-100.99262475  -90.98523111  -58.97796476]\n",
      "    infect_w [0.00449546 0.03961321 0.3101614 ]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.383, median size 2\n",
      "\t Negative bags: mean size 2.414, median size 2\n",
      "assign_mat size, X_shuff size: (4329, 10429) (10429, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 10429) (789, 10429)\n",
      "Average positive samples per bag: 1.9873239436619718\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 0.914559364292362 step loss 0.9091070973149527\n",
      "iter 0 sigmoid loss 0.6850023965866596 step loss 0.7190299283274696 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36479061559759335 step loss 0.3712520841967357 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3449048437238417 step loss 0.3291356322202904 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35349849189350957 step loss 0.3248451688220629 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4377822961184613 step loss 0.3216665881138618 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.24492095451646712 step loss 0.3792241977634512 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.6417051195828881 step loss 0.5991868337975524 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.30798276440867883 step loss 0.33293130081862454 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28881082040451916 step loss 0.3334281096804071 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3068530629304855 step loss 0.3370259883995444 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3159388875485679 step loss 0.3411207548587151\n",
      "    beta 0.16738226254743294\n",
      "    rssi_w [-0.03851589 -0.02039665  0.02329896  0.29512235]\n",
      "    rssi_th [12.01273669 15.0127391  25.0126832 ]\n",
      "    infect_w [0.00407227 0.02596316 0.20988878]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.030716042412783 step loss 1.035925898694786\n",
      "iter 0 sigmoid loss 0.7712868179511585 step loss 0.726381537577717 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.32528117019690833 step loss 0.34107476985408036 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34133760287908177 step loss 0.33323405455534155 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.345129351517697 step loss 0.3292734190977211 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.42465129428966336 step loss 0.3251519246434918 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.23903897317542916 step loss 0.3376909178380343 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.27925366168525717 step loss 0.32589984700730656 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2762964176702473 step loss 0.3206407531473924 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.26100166659643276 step loss 0.31816899904342233 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.33302699075694114 step loss 0.31751599183779666 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3080449790704457 step loss 0.317803072845173\n",
      "    beta 0.3552943184630507\n",
      "    rssi_w [-0.06737212 -0.0377545   0.14518859  0.31105866]\n",
      "    rssi_th [24.9901867  17.99018529 31.98783453]\n",
      "    infect_w [0.00420138 0.03954365 0.35156071]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0951244122356836 step loss 1.1081920843845683\n",
      "iter 0 sigmoid loss 0.8166650632136219 step loss 0.7355853102254045 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.32418434374717703 step loss 0.32228506226247355 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3330378704731438 step loss 0.3033614402396351 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.325804793173278 step loss 0.28971548884964077 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3991380844121138 step loss 0.5507885516671671 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.22730013965960474 step loss 0.58603255241523 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4027272561509919 step loss 0.33919747116272164 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.31052814070000295 step loss 0.3380097357835549 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2922991940387058 step loss 0.33740914441950354 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.36262971186071014 step loss 0.3380808291700783 sigmoid temp 1.0\n",
      "End sigmoid loss 0.336763237504049 step loss 0.33648558349701463\n",
      "    beta 0.025690725710580842\n",
      "    rssi_w [0.63087279 0.64899758 0.49475422 0.10721602]\n",
      "    rssi_th [25.99609975 35.99597462 27.99930408]\n",
      "    infect_w [-0.02863309  0.0688601   0.18998051]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1926071529654718 step loss 1.1982968367787146\n",
      "iter 0 sigmoid loss 0.8876284227004276 step loss 0.6650696211720296 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.32354361474313026 step loss 0.3261414057305246 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3365137584796406 step loss 0.31061992798608995 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32957847173960403 step loss 0.2926619307057414 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.39726849559981003 step loss 0.5202248668725079 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2229864788795248 step loss 0.5946870570492035 sigmoid temp 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000 sigmoid loss 0.33247612536952936 step loss 0.3030209125891851 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.23208702161638065 step loss 0.2821031192524758 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2227167751714479 step loss 0.2757008941060939 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.28283699565496795 step loss 0.2733812515856558 sigmoid temp 1.0\n",
      "End sigmoid loss 0.25953788538864353 step loss 0.2753305669220631\n",
      "    beta 0.29792164768217544\n",
      "    rssi_w [-0.04110388 -0.02637844  0.08425594  0.32156406]\n",
      "    rssi_th [21.00020233 21.00021167 21.99968127]\n",
      "    infect_w [0.00377954 0.0294282  0.29818869]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.9575317925295885 step loss 0.9529882541240144\n",
      "iter 0 sigmoid loss 0.7126572249482631 step loss 0.7202464905665807 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3273336272570387 step loss 0.34275693519703276 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34734137935636483 step loss 0.34129994672236386 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.35779205047080465 step loss 0.3422998560445396 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4435500827620793 step loss 0.3468313689835928 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2506914868850697 step loss 0.3628676246018822 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.311011813653992 step loss 0.35217594302695915 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.32568904584998437 step loss 0.3463881643858418 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2999899123744154 step loss 0.3426121388213957 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3250710050167126 step loss 0.34309170517320714 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3280195082024813 step loss 0.3415167353341498\n",
      "    beta 0.28633403542356317\n",
      "    rssi_w [-0.09732455 -0.08028568  0.02757418  0.24728275]\n",
      "    rssi_th [15.00839724 23.00839426 11.00806361]\n",
      "    infect_w [0.00529761 0.03335168 0.27845284]\n",
      "best loss 0.2753305669220631\n",
      "best scoring parameters\n",
      "    beta 0.29792164768217544\n",
      "    rssi_w [-0.04110388 -0.06748233  0.01677362  0.33833768]\n",
      "    rssi_th [-98.99979767 -77.99958601 -55.99990474]\n",
      "    infect_w [0.00377954 0.03320773 0.33139643]\n"
     ]
    }
   ],
   "source": [
    "# parameters of training\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# taylor series approximation parameters\n",
    "approx_type = 'taylor'\n",
    "exp_taylor_terms = [np.inf, 8, 6, 4, 2]\n",
    "exp_temperature = np.arange(0.5,1.,0.09)\n",
    "\n",
    "auc_train_learned = np.zeros((len(exp_taylor_terms),n_trials))\n",
    "auc_train_true = np.zeros((len(exp_taylor_terms),n_trials))\n",
    "auc_test_learned = np.zeros((len(exp_taylor_terms),n_trials))\n",
    "auc_test_true = np.zeros((len(exp_taylor_terms),n_trials))\n",
    "\n",
    "for num_taylor_term in exp_taylor_terms:\n",
    "  # generate probability of infection using taylor approximation\n",
    "  prob_infect_exact, prob_infect_approx = prob_infection_grid(data, params, approx=approx_type, exp_taylor_terms=num_taylor_term, temp=exp_temperature)\n",
    "  prob_infect_approx = np.array(prob_infect_approx)\n",
    "\n",
    "  # store data\n",
    "  simdata = dict()\n",
    "  simdata['X'] = X\n",
    "  simdata['prob_infect_exact'] = prob_infect_exact\n",
    "  simdata['prob_infect_approx'] = prob_infect_approx\n",
    "\n",
    "  # shuffle data points\n",
    "  X_epi = simdata['X']  \n",
    "  probabilities_true_epi = simdata['prob_infect_exact']\n",
    "  probabilities_true_epi_approx = simdata['prob_infect_approx'].T  # N x num_approx_steps\n",
    "  N = len(probabilities_true_epi)\n",
    "  perm = np.random.permutation(N)\n",
    "  X_epi = X_epi[perm, :]\n",
    "  probabilities_true_epi = probabilities_true_epi[perm]\n",
    "  probabilities_true_epi_approx = np.clip(probabilities_true_epi_approx[perm], 1e-5, 1-1e-5)\n",
    "    \n",
    "  # sample labels (exposure outcome) based on probability of infection\n",
    "  Y_epi, N_pos, N_neg, pos_neg_ratio = sample_labels(probabilities_true_epi)\n",
    "\n",
    "  # initialize bag simulator and run the training\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=4,censor_prob_pos=0.,censor_prob_neg=0,max_pos_in_bag=3)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "kFoNf8sdngUU",
    "outputId": "8d103c16-ee0b-458e-9a6c-31bf865a8104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1985eb0d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "taylor_terms = [10, 8, 6, 4, 2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(taylor_terms, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(taylor_terms, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Number of Taylor Approx Terms\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: No Censoring\")\n",
    "axs[0].set_xticks([10, 8, 6, 4, 2])\n",
    "axs[0].set_xticklabels(['Inf', '8', '6', '4', '2'])\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(taylor_terms, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(taylor_terms, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Number of Taylor Approx Terms\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: No Censoring\")\n",
    "axs[1].set_xticks([10, 8, 6, 4, 2])\n",
    "axs[1].set_xticklabels(['Inf', '8', '6', '4', '2'])\n",
    "#axs[1].set_xticks(['Inf', 8, 6, 4, 2])\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlokEQqwyrSs"
   },
   "source": [
    "### 5. Reordering of data\n",
    "\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "Testing the robustness of the optimization routine to a reordering of the data points in the bagging simulation.\n",
    "\n",
    "This experiment is iterated over increasing bag sizes, but the difference between this and experiment 1 is the shuffling of training data. We would expect the same trends as Experiment 1, and it acts more like a check to make sure that the learning model previously was not learning to a very specific sequence of the data and being biased. This is confirmed by two graphs below, where we can see the decreasing AUC following the increase of bag size. \n",
    "\n",
    "\n",
    "More discussion on this can be found in III.2 Future Work and Potential Modifications, under optimization of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJDRxSf4ysKp",
    "outputId": "621ed3fd-e80d-4322-cedc-395958b39ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4329, 4329) (4329, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 4329) (866, 4329)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.2200283407728685 step loss 1.2257949275655553\n",
      "iter 0 sigmoid loss 7.437925088221127 step loss 0.3781165192511059 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.13565879062049044 step loss 0.3403220812006198 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12980633434680391 step loss 0.3493532206783222 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.11756154614373092 step loss 0.3355314300042378 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.15763043394481963 step loss 0.3303848238483888 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.15141073263349017 step loss 0.32850620685633036 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.15310144867096384 step loss 0.3316717097813872 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5488375933827389 step loss 0.3335557576191651 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.1832488143492679 step loss 0.32570398439821235 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.15688955796286805 step loss 0.33923525007776595 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2796809976995267 step loss 0.2888149890934166\n",
      "    beta 0.20610362977114816\n",
      "    rssi_w [-0.00625257  0.01440334  0.07741165  0.32711023]\n",
      "    rssi_th [19.00433372 33.00432231 10.00463654]\n",
      "    infect_w [0.01095765 0.08261399 0.31108047]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.187653006200411 step loss 1.185981082804295\n",
      "iter 0 sigmoid loss 7.23677397155167 step loss 0.4032773586123913 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.13127321835355438 step loss 0.3503931164071771 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14039694176189152 step loss 0.3392882549437097 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13085767183433986 step loss 0.3324028931185966 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1479991022642534 step loss 0.3319405336090666 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.14228501915834177 step loss 0.3310834296880523 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.15167023573118898 step loss 0.33533224040927584 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5772666690633678 step loss 0.33641974655095774 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.3174278032773215 step loss 0.3283139935015345 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.11642461866792911 step loss 0.32816102787902374 sigmoid temp 1.0\n",
      "End sigmoid loss 0.31222306629165697 step loss 0.3279148078630854\n",
      "    beta 0.2634509662340466\n",
      "    rssi_w [-0.04058278 -0.03244996  0.19453353  0.29935388]\n",
      "    rssi_th [13.99820566 35.99820547 22.99244015]\n",
      "    infect_w [0.00976668 0.08358958 0.31789537]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.2186466610692883 step loss 1.2136226664199206\n",
      "iter 0 sigmoid loss 7.426016870892952 step loss 0.383282987866722 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.13725768052210113 step loss 0.34490241092863017 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13974890050734948 step loss 0.3206979099346905 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.12708760857211782 step loss 0.3039755876309025 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.14532817467251036 step loss 0.29978108008325205 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.15147548724336812 step loss 0.3159555108690549 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.16088648121713944 step loss 0.2939009866519037 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5421093409389842 step loss 0.29639571167587764 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.1687981456740553 step loss 0.2912369320397565 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.08634742874395535 step loss 0.2985765060197714 sigmoid temp 1.0\n",
      "End sigmoid loss 0.28743839026259366 step loss 0.29291809962000903\n",
      "    beta 0.23225000095040682\n",
      "    rssi_w [-0.05374608 -0.03261321  0.11852488  0.31084829]\n",
      "    rssi_th [16.01035263 29.01034867 14.00972893]\n",
      "    infect_w [0.00934486 0.07836315 0.31068516]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1647577933003832 step loss 1.1753679680412414\n",
      "iter 0 sigmoid loss 7.096151377311994 step loss 0.4131697207886723 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.1316490790517923 step loss 0.3513190026454029 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14089396029414517 step loss 0.3414178263660852 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13308366930729265 step loss 0.336070213864582 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.15211849798533358 step loss 0.331907536438449 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.14022018584905463 step loss 0.33562144394177273 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.15347037820147105 step loss 0.3397330232532275 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.575227606622392 step loss 0.3444048118702118 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.3786838771619745 step loss 0.33296339251747575 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.10362854754961504 step loss 0.3339916968033601 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3182217784048862 step loss 0.33211950193108114\n",
      "    beta 0.2647550748606896\n",
      "    rssi_w [-0.07892412 -0.00086087  0.21155644  0.2745972 ]\n",
      "    rssi_th [35.00178431 15.00165248 24.99447006]\n",
      "    infect_w [0.0099931  0.08278793 0.31404121]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.2214960561005235 step loss 1.2365344510250085\n",
      "iter 0 sigmoid loss 7.450523334888487 step loss 0.3781633948964664 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.1367412654293796 step loss 0.33994617633016927 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13075185547902415 step loss 0.31447523496164587 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.11779435747072049 step loss 0.3968234568180847 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1556889295723245 step loss 0.3897232232824614 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.1527833944540057 step loss 0.38696735212836436 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.6503643711312154 step loss 0.5883087875513432 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5614569276286376 step loss 0.3061276639359147 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.1765473992613467 step loss 0.2868906830962544 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.09369361482751451 step loss 0.289139120189195 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27741837163815253 step loss 0.28472352617500724\n",
      "    beta 0.1977112001032676\n",
      "    rssi_w [-0.00839605  0.02810527  0.12311457  0.30931161]\n",
      "    rssi_th [27.00463747 28.0045912  10.00153689]\n",
      "    infect_w [0.01053864 0.09296143 0.36112924]\n",
      "best loss 0.28472352617500724\n",
      "best scoring parameters\n",
      "    beta 0.1977112001032676\n",
      "    rssi_w [-0.00839605  0.01970922  0.14282379  0.4521354 ]\n",
      "    rssi_th [-92.99536253 -64.99077133 -54.98923445]\n",
      "    infect_w [0.01053864 0.10350008 0.46462931]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.551, median size 2\n",
      "\t Negative bags: mean size 1.542, median size 2\n",
      "assign_mat size, X_shuff size: (4329, 6683) (6683, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 6683) (866, 6683)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0815387297397805 step loss 1.091542172964426\n",
      "iter 0 sigmoid loss 6.631021374131824 step loss 0.4208986051654761 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.12022072428383755 step loss 0.37008517042190153 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12624030563455102 step loss 0.35081647635693897 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13325796343572321 step loss 0.4687466287244836 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.14700192963770298 step loss 0.47869314701454907 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.1960937825518546 step loss 0.47371785982954395 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.17319054864833042 step loss 0.3654403572949135 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5669620557041499 step loss 0.3379619090188579 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.4162690121617356 step loss 0.33056933399222826 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.09461132365252571 step loss 0.3387567309100637 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3218828624437763 step loss 0.33170778241367754\n",
      "    beta 0.1771504102270448\n",
      "    rssi_w [-0.00140747  0.02011427  0.20230656  0.25473526]\n",
      "    rssi_th [22.00489773 34.00487007 17.99651483]\n",
      "    infect_w [0.0100112  0.06349208 0.31908927]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.102654276088049 step loss 1.104151060205447\n",
      "iter 0 sigmoid loss 6.78115741327719 step loss 0.4170991766068726 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.1231701906409905 step loss 0.3788378432461293 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12997458136883897 step loss 0.36335639811675 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14176886896335167 step loss 0.352392209629956 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1484857388510517 step loss 0.3472376022782935 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.19322398247445466 step loss 0.35544639503900416 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.1986605311180117 step loss 0.3334681954166262 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.542676897333064 step loss 0.343733458488395 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.5321427376832872 step loss 0.32915745232378363 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.0757049895687735 step loss 0.34267096788356616 sigmoid temp 1.0\n",
      "End sigmoid loss 0.32294523863290703 step loss 0.3314322563234409\n",
      "    beta 0.23393175755938012\n",
      "    rssi_w [-0.01468774  0.00262154  0.06894324  0.39081965]\n",
      "    rssi_th [14.98947235 17.98950006 35.98911853]\n",
      "    infect_w [0.00964724 0.05892993 0.32698059]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1122002293684359 step loss 1.1238060591398036\n",
      "iter 0 sigmoid loss 6.826740257969773 step loss 0.41714821828379006 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.12349190538643207 step loss 0.3762454720975128 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13124726379717433 step loss 0.36407116311170934 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.1418816785194037 step loss 0.35535553589010344 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.14588684316961234 step loss 0.38175534995474775 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.1920186472724076 step loss 0.39212014583125865 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.20840575667741873 step loss 0.38407257102131986 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5793158478500376 step loss 0.39127544077240467 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.5244664077406542 step loss 0.35558156386784895 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.10566924324664198 step loss 0.3899346437747572 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3424496208532756 step loss 0.3708216717381274\n",
      "    beta 0.17508987778792606\n",
      "    rssi_w [-0.01737704  0.01239242  0.19208603  0.25850809]\n",
      "    rssi_th [25.00392251 27.0038956  24.99636256]\n",
      "    infect_w [0.01014915 0.06059425 0.29385709]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.03226391012608 step loss 1.0363902006419898\n",
      "iter 0 sigmoid loss 6.331416726942313 step loss 0.4333569737718021 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.12258099971479136 step loss 0.37346098400546307 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12948957599296906 step loss 0.3570488071671687 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13928551743668968 step loss 0.34328578315214925 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.14547749936613086 step loss 0.34004437382303637 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.21186681514792297 step loss 0.33236146781286713 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.17012581088502465 step loss 0.3241231745920087 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.539953494298237 step loss 0.33226935817708214 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.4627264568194618 step loss 0.3210606571388934 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.07158360202716256 step loss 0.3359382779059328 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3118067440231877 step loss 0.3240426154377944\n",
      "    beta 0.21889059597924854\n",
      "    rssi_w [-0.02841116 -0.01683682  0.09036948  0.37302917]\n",
      "    rssi_th [13.99807141 25.99808497 24.99749394]\n",
      "    infect_w [0.00931241 0.0563715  0.32713936]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1430836117465435 step loss 1.1547010670553903\n",
      "iter 0 sigmoid loss 7.022288367140208 step loss 0.4109904410733168 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.12311626911806352 step loss 0.374892981868913 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.1297957647204809 step loss 0.3590699872593152 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13940153723414608 step loss 0.34563125181341386 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.14445977458723988 step loss 0.33404184880546717 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.19075409973112037 step loss 0.3270383267217231 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.1832427950473994 step loss 0.3264126270301285 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5361472034251533 step loss 0.3352463411393033 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.494600391960139 step loss 0.3211834458901376 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.07122057835017419 step loss 0.3364612309104921 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3151485206826657 step loss 0.3239716406839413\n",
      "    beta 0.23310899840690558\n",
      "    rssi_w [-0.05275615 -0.02889128  0.13080762  0.372498  ]\n",
      "    rssi_th [16.99610808 27.99610909 21.99427465]\n",
      "    infect_w [0.00946331 0.0579735  0.33028183]\n",
      "best loss 0.3239716406839413\n",
      "best scoring parameters\n",
      "    beta 0.23310899840690558\n",
      "    rssi_w [-0.05275615 -0.08164743  0.04916019  0.42165819]\n",
      "    rssi_th [-103.00389192  -75.00778283  -53.01350818]\n",
      "    infect_w [0.00946331 0.06743682 0.39771865]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 2.415, median size 2\n",
      "\t Negative bags: mean size 2.414, median size 2\n",
      "assign_mat size, X_shuff size: (4329, 10451) (10451, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 10451) (866, 10451)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.053705570186742 step loss 1.063177002850041\n",
      "iter 0 sigmoid loss 6.457958942867378 step loss 0.43494164175266936 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.1052512653155061 step loss 0.4049795872938209 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13729552781125115 step loss 0.3905812967906323 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14858454367340446 step loss 0.3791825439199998 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1705019023612972 step loss 0.36958294815990594 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.18346907482288388 step loss 0.3638620583275751 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2294095585731209 step loss 0.36145680020141974 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6052969488029238 step loss 0.3772082210354107 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.704354199931724 step loss 0.35629297592315184 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.06769340409147469 step loss 0.37247188224537564 sigmoid temp 1.0\n",
      "End sigmoid loss 0.35022485737088627 step loss 0.35872971662725833\n",
      "    beta 0.16818359543636632\n",
      "    rssi_w [-0.0217611  -0.01176912  0.09029338  0.37345858]\n",
      "    rssi_th [10.99009885 26.99011401 30.98945498]\n",
      "    infect_w [0.00961328 0.04960597 0.31470109]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1522173221272205 step loss 1.159812242752954\n",
      "iter 0 sigmoid loss 7.050833279996359 step loss 0.4675979654811026 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.11796345627286767 step loss 0.40914757921474243 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13922431100286542 step loss 0.3915949126748629 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14912610700165047 step loss 0.3844306660350423 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.17171346213672883 step loss 0.4066601417310028 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.18108716227771438 step loss 0.43398166071091643 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.24019000626626522 step loss 0.44256681340265197 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6680054096412081 step loss 0.4314620719541576 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.6603400034995806 step loss 0.41147310466422055 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.11245254334407456 step loss 0.41772398717129133 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3798510567585645 step loss 0.4090799689780174\n",
      "    beta 0.11477311661890469\n",
      "    rssi_w [-0.02792297  0.00383463  0.23981065  0.13318216]\n",
      "    rssi_th [22.0088452  30.00881967 35.99897639]\n",
      "    infect_w [0.01247098 0.05057298 0.28247723]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1147611771318993 step loss 1.1297721113912131\n",
      "iter 0 sigmoid loss 6.821063003990839 step loss 0.45636408640060205 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.11076888560794645 step loss 0.40993654920150696 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.1368534662202971 step loss 0.39916566305188816 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14638575816314914 step loss 0.39464656352550787 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16855088635795043 step loss 0.39353721949737125 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.18441817881046998 step loss 0.40033207041983254 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.23489595227256885 step loss 0.4135334064245925 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6628736949191318 step loss 0.42562206941008585 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.7892378051079532 step loss 0.39393937570918175 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.09102796400082257 step loss 0.39808651268490647 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3802015544206742 step loss 0.39108184676363444\n",
      "    beta 0.166866380210584\n",
      "    rssi_w [-0.08163297 -0.06640081  0.2459255   0.2642401 ]\n",
      "    rssi_th [12.00689424 37.00689314 27.99450928]\n",
      "    infect_w [0.01220533 0.05270049 0.30375292]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.0321304595484966 step loss 1.0685276575825207\n",
      "iter 0 sigmoid loss 6.3138065809062525 step loss 0.4236786614568939 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10773165012460705 step loss 0.40073600564240824 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.1322643619161537 step loss 0.38399135786986366 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13641654365518893 step loss 0.6521141740238319 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16510297080112551 step loss 0.7404199119257422 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.20415910047841748 step loss 0.6919693545513369 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.20264389321840198 step loss 0.3455845853841482 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.5795823038893281 step loss 0.3543706336030514 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.5407849239336588 step loss 0.3424476821283038 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.08154156013944641 step loss 0.35712833929803384 sigmoid temp 1.0\n",
      "End sigmoid loss 0.333913177363113 step loss 0.34569519566777007\n",
      "    beta 0.1549323143916869\n",
      "    rssi_w [0.00408048 0.02858292 0.28380265 0.18495712]\n",
      "    rssi_th [25.99934943 35.99927492 11.99832912]\n",
      "    infect_w [0.00964887 0.04464384 0.32383084]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1207373014341917 step loss 1.128642341921689\n",
      "iter 0 sigmoid loss 6.858635372002917 step loss 0.4737856140168961 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.1166860134798031 step loss 0.41341071682124475 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13505894581692893 step loss 0.3879721289861054 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14192391074241725 step loss 0.3753317772802448 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16791142359780842 step loss 0.36782302456470595 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.18671282687995192 step loss 0.3606128446287236 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.22890201120157216 step loss 0.36123675091219404 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6022678791333866 step loss 0.377242459402326 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.7007338947227912 step loss 0.3561180663513722 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.06775587555775699 step loss 0.3722365196207162 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3499993900423281 step loss 0.35862146010544993\n",
      "    beta 0.16662135454416074\n",
      "    rssi_w [-0.04016042 -0.01761184  0.1149367   0.38200887]\n",
      "    rssi_th [22.99037791 17.99039994 27.9894151 ]\n",
      "    infect_w [0.0096112  0.04939867 0.31517772]\n",
      "best loss 0.34569519566777007\n",
      "best scoring parameters\n",
      "    beta 0.1549323143916869\n",
      "    rssi_w [0.00408048 0.0326634  0.31646605 0.50142317]\n",
      "    rssi_th [-94.00065057 -58.00137564 -46.00304652]\n",
      "    infect_w [0.00964887 0.05429271 0.37812355]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.417, median size 3\n",
      "\t Negative bags: mean size 3.479, median size 3\n",
      "assign_mat size, X_shuff size: (4329, 15018) (15018, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 15018) (866, 15018)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.164021126924725 step loss 1.1739256520186758\n",
      "iter 0 sigmoid loss 7.127573798655419 step loss 0.6338654598158889 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.11540265879746801 step loss 0.43796685242844086 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.1447836993212887 step loss 0.4215727750071143 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.1547527149002575 step loss 0.41027740273474744 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.15824797422568862 step loss 0.4424844703171529 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.23234243519770106 step loss 0.46744954623081636 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 5.4110802685625075 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 11.512925464970227 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta -0.007476832919561888\n",
      "    rssi_w [0.20842493 0.23653634 0.33238415 0.09566679]\n",
      "    rssi_th [23.99945807 29.99942584 36.99968422]\n",
      "    infect_w [0.07419803 0.09326014 0.22126786]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1218073117927598 step loss 1.1209626608635979\n",
      "iter 0 sigmoid loss 6.86967328396954 step loss 0.7377151892029055 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.101350771405699 step loss 0.45655238789902175 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14039947502666558 step loss 0.4403171318896573 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.1600722655572786 step loss 0.43486622616026605 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16679906117220497 step loss 0.4344580005334438 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.24225653609378528 step loss 0.438308646506774 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.24056182924176636 step loss 0.4439873156763693 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7945908387123688 step loss 0.457551100223333 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.1659694804084615 step loss 0.4341830927352767 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.1117362213374855 step loss 0.4470468071969657 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43731340118035145 step loss 0.43731344565298685\n",
      "    beta 0.08266689088533\n",
      "    rssi_w [0.01120219 0.014841   0.03104297 0.10816683]\n",
      "    rssi_th [11.00062897 11.00066366 16.0008078 ]\n",
      "    infect_w [0.01171971 0.05159829 0.22834407]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.0051416290783688 step loss 1.0049762239395061\n",
      "iter 0 sigmoid loss 6.152564907338749 step loss 0.4878570993439965 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.11192465555203544 step loss 0.43678619014399345 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13490492429844764 step loss 0.41334825937508507 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14360708181963666 step loss 0.39433581949533597 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1626454118588886 step loss 0.3856371259314096 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2241067146674925 step loss 0.3881315636811367 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3875860919915975 step loss 0.4891066314654366 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 5.4110802685625075 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 11.512925464970227 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta 3.854231831611978\n",
      "    rssi_w [0.03898314 0.11924252 0.21172367 0.25933113]\n",
      "    rssi_th [36.00123681 13.0011738  13.00137246]\n",
      "    infect_w [-0.15446207 -0.04588106  0.19135725]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.0156343766376088 step loss 1.0157415786427302\n",
      "iter 0 sigmoid loss 6.21106125529065 step loss 0.4859054213430398 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.113698877633251 step loss 0.43975992474233044 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.137735760093946 step loss 0.42276106841005795 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14553928392405577 step loss 0.4096883462317898 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1603300045055186 step loss 0.40284929561354255 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.23506008715940863 step loss 0.4058910479758703 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2520211278937669 step loss 0.4413631871209033 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7130446512466945 step loss 0.43146512615272226 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.9621165339583422 step loss 0.4023719644033792 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.09830064513632376 step loss 0.40972063430022265 sigmoid temp 1.0\n",
      "End sigmoid loss 0.390137005806932 step loss 0.3986132616348588\n",
      "    beta 0.12242582786138315\n",
      "    rssi_w [-0.05770491 -0.0549296   0.18288553  0.3356233 ]\n",
      "    rssi_th [12.99844323 35.9984498  21.99200472]\n",
      "    infect_w [0.01019742 0.05365196 0.3116923 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.295637732281315 step loss 1.2946348438927167\n",
      "iter 0 sigmoid loss 7.981763621626328 step loss 1.8156209029856776 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 5.4110802685625075 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 11.512925464970227 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta -0.012438766509265203\n",
      "    rssi_w [0.05955461 0.05888137 0.06438315 0.03392044]\n",
      "    rssi_th [13.00002489 11.00002723 39.0000268 ]\n",
      "    infect_w [0.4311061  0.26423096 0.07645602]\n",
      "best loss 0.3986132616348588\n",
      "best scoring parameters\n",
      "    beta 0.12242582786138315\n",
      "    rssi_w [-0.05770491 -0.11263451  0.07025102  0.40587432]\n",
      "    rssi_th [-107.00155677  -71.00310696  -49.01110224]\n",
      "    infect_w [0.01019742 0.06384938 0.37554168]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.930, median size 3\n",
      "\t Negative bags: mean size 3.899, median size 3\n",
      "assign_mat size, X_shuff size: (4329, 16900) (16900, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 16900) (866, 16900)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.191777071502548 step loss 1.2032794846638821\n",
      "iter 0 sigmoid loss 7.31237126254042 step loss 0.9047164958485454 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 5.4110802685625075 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 11.512925464970227 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta -0.003721064827200221\n",
      "    rssi_w [0.18039048 0.17101061 0.1356006  0.04739228]\n",
      "    rssi_th [33.00013938 18.00014615 26.00004293]\n",
      "    infect_w [0.05783596 0.03033331 0.05701104]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0403572920423185 step loss 1.0438179841220039\n",
      "iter 0 sigmoid loss 6.378760460648703 step loss 0.5464244957322628 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10270788363073141 step loss 0.4517292973496229 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13295988518962623 step loss 0.4408528160976033 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14519283636646405 step loss 0.4346829555735615 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1637853620788405 step loss 0.4306796745931068 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.21338299033910574 step loss 0.4306082280917841 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.28221213127929184 step loss 0.43918183003804057 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7201638509340893 step loss 0.4537544556298907 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.127182608071928 step loss 0.4289046892330713 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.11304817429579903 step loss 0.4418766706805477 sigmoid temp 1.0\n",
      "End sigmoid loss 0.43096742108453473 step loss 0.43246904545676546\n",
      "    beta 0.09287520216495473\n",
      "    rssi_w [-0.02847775 -0.00633366  0.14212326  0.26930627]\n",
      "    rssi_th [19.99721371 21.99723596 36.99562863]\n",
      "    infect_w [0.0117103  0.06518763 0.23658242]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.107450235319392 step loss 1.1186480106757377\n",
      "iter 0 sigmoid loss 6.791028917227407 step loss 0.6444695920408058 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.108283480992493 step loss 0.44986472233270897 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.1326399080912921 step loss 0.4396917225902593 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.14306690823519946 step loss 0.43669193442616167 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1624274352011393 step loss 0.44133114096370996 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.211975819044941 step loss 0.4587017015738552 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.27496416314716343 step loss 0.46434679146888397 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7131239518644469 step loss 0.47769306355674257 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.113147247867269 step loss 0.43817150342426603 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.11275414343433042 step loss 0.4431118638638064 sigmoid temp 1.0\n",
      "End sigmoid loss 0.4295825590484723 step loss 0.43603093308792634\n",
      "    beta 0.09961272784121392\n",
      "    rssi_w [-0.08407973 -0.0556397   0.25369392  0.20153168]\n",
      "    rssi_th [24.00872691 25.00872452 34.99716243]\n",
      "    infect_w [0.01185002 0.0679517  0.24316877]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.0114586895928706 step loss 1.013411702008754\n",
      "iter 0 sigmoid loss 6.2007695415077 step loss 0.5170427611252294 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10260860017214529 step loss 0.4424322583611398 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12683262411535043 step loss 0.42253249169755314 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13633376119958324 step loss 0.41187437686694456 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16067200512307056 step loss 0.42647709293227815 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2090241423766638 step loss 0.44152922942766565 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25770875104750535 step loss 0.3880295185937732 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6495941917746005 step loss 0.4095078345497042 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.9498525054706488 step loss 0.3840016938580607 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.10342300877529037 step loss 0.4035864628182036 sigmoid temp 1.0\n",
      "End sigmoid loss 0.38171495944033906 step loss 0.3875661696787256\n",
      "    beta 0.09934252134514701\n",
      "    rssi_w [-0.01820138 -0.00172222  0.05710198  0.37194003]\n",
      "    rssi_th [14.99997395 16.9999949  31.9997962 ]\n",
      "    infect_w [0.0101385  0.05239699 0.28148766]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0818744158036557 step loss 1.0784272013508163\n",
      "iter 0 sigmoid loss 6.637407803253684 step loss 0.5865157792280238 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10695061989092451 step loss 0.44546700607240614 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.12907723340553778 step loss 0.4318019642272524 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13791713784064769 step loss 0.42561908890733907 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.16087578167368025 step loss 0.4337746281326564 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.20984413193998655 step loss 0.4526791759030183 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.24763150641347145 step loss 0.43486795037324655 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7131791053756638 step loss 0.44540832898113536 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.9955212953265964 step loss 0.4251568280273782 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.12281231563921245 step loss 0.44183163933961483 sigmoid temp 1.0\n",
      "End sigmoid loss 0.42038136219357364 step loss 0.4234471025523617\n",
      "    beta 0.06432331769713465\n",
      "    rssi_w [-0.01494058  0.02605825  0.20803604  0.2441002 ]\n",
      "    rssi_th [26.0023819  25.0023565  27.99719324]\n",
      "    infect_w [0.01618152 0.05375587 0.23206551]\n",
      "best loss 0.3875661696787256\n",
      "best scoring parameters\n",
      "    beta 0.09934252134514701\n",
      "    rssi_w [-0.01820138 -0.0199236   0.03717838  0.4091184 ]\n",
      "    rssi_th [-105.00002605  -88.00003115  -56.00023494]\n",
      "    infect_w [0.0101385  0.06253549 0.34402315]\n",
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 3.845, median size 3\n",
      "\t Negative bags: mean size 4.032, median size 3\n",
      "assign_mat size, X_shuff size: (4329, 17321) (17321, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 17321) (866, 17321)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0580479987438371 step loss 1.0620600328450887\n",
      "iter 0 sigmoid loss 6.3882477690610555 step loss 0.5955776146276555 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10098631391410401 step loss 0.4539695398560876 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14084775438578742 step loss 0.425134120073966 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13531202626375768 step loss 0.4075293958074853 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.17476631138161067 step loss 0.39855198444644374 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.20279853030090286 step loss 0.39905918375343635 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.22263443424284574 step loss 0.41516859608945467 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6889018432843236 step loss 0.4327690608072723 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.9276338275759628 step loss 0.39707176561955776 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.08674491111617876 step loss 0.4151265905246291 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3882652447434063 step loss 0.39789662552064126\n",
      "    beta 0.11011825553898227\n",
      "    rssi_w [-0.08757291 -0.03548074  0.17159975  0.37234285]\n",
      "    rssi_th [31.99867138 15.99865913 19.99376357]\n",
      "    infect_w [0.00802257 0.05318134 0.29495052]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0296810494099158 step loss 1.0348304679499734\n",
      "iter 0 sigmoid loss 6.210073205206389 step loss 0.5243384118694412 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10382402381447736 step loss 0.4438346017240038 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.13982582392519383 step loss 0.4364088081290756 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13485922016598356 step loss 0.47003859425454786 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.17497068655452813 step loss 0.47041181880136235 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.20029877913182362 step loss 0.4607855141521135 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2318718342306692 step loss 0.43001248472554987 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6848364755088311 step loss 0.41757436925419905 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.8468070498780318 step loss 0.4008325942469534 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.09254963511065568 step loss 0.42518250007687014 sigmoid temp 1.0\n",
      "End sigmoid loss 0.38406147030609905 step loss 0.4039466357414682\n",
      "    beta 0.09643303008611817\n",
      "    rssi_w [-0.02188888  0.01399088  0.10405083  0.33952002]\n",
      "    rssi_th [24.00095163 29.00092657 14.99708551]\n",
      "    infect_w [0.00910659 0.05545171 0.28167103]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 0.9748760796835905 step loss 0.9827458499435262\n",
      "iter 0 sigmoid loss 5.870143356136887 step loss 0.49425481555851597 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10434969581940613 step loss 0.45121872492943144 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14395923344635142 step loss 0.43451923260832653 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13541503531947657 step loss 0.42167351271235304 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.17484542380496174 step loss 0.41618235783235025 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.20618008467183288 step loss 0.42132573156916414 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.23182686523794974 step loss 0.44250139649972803 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.7008383489014542 step loss 0.4803683379057979 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.982900125901864 step loss 0.41684198525864724 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.09189806965085945 step loss 0.42573889294106926 sigmoid temp 1.0\n",
      "End sigmoid loss 0.40485450503207115 step loss 0.41438436025588954\n",
      "    beta 0.11031200957957168\n",
      "    rssi_w [-0.07140993 -0.05607187  0.19893517  0.3449718 ]\n",
      "    rssi_th [15.99846453 32.99846658 22.99118566]\n",
      "    infect_w [0.0086931  0.05723526 0.28413378]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.265710826846673 step loss 1.283553266874576\n",
      "iter 0 sigmoid loss 7.654549736372495 step loss 1.080420630741337 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 5.4110802685625075 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 11.512925464970227 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.0000050000287821e-05 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta -0.0018059424262585055\n",
      "    rssi_w [0.07238158 0.06592042 0.03560238 0.01609686]\n",
      "    rssi_th [36.00006136 28.00005124 21.00002046]\n",
      "    infect_w [0.23672299 0.18271265 0.08036453]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 0.98091570736716 step loss 0.9842510926503855\n",
      "iter 0 sigmoid loss 5.905366200100873 step loss 0.49997594693830166 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.10408831239662106 step loss 0.45084133386652364 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.14611772224721808 step loss 0.4317680049269966 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.13683802872506226 step loss 0.41750885499357554 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.1743689888169363 step loss 0.40702879603267533 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2064813620716287 step loss 0.397817663410085 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.21712041834722792 step loss 0.40262683164040314 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.6981024161250547 step loss 0.4234940527162012 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.9389320658791565 step loss 0.3947969798878027 sigmoid temp 0.7749999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500 sigmoid loss 0.08977456255523972 step loss 0.41761568631159013 sigmoid temp 1.0\n",
      "End sigmoid loss 0.39059263325953814 step loss 0.3994417856858591\n",
      "    beta 0.10664200541788951\n",
      "    rssi_w [-0.0385116  -0.01404745  0.10043055  0.39078713]\n",
      "    rssi_th [22.99330113 17.99333782 26.99247926]\n",
      "    infect_w [0.00805373 0.05262691 0.28860174]\n",
      "best loss 0.39789662552064126\n",
      "best scoring parameters\n",
      "    beta 0.11011825553898227\n",
      "    rssi_w [-0.08757291 -0.12305364  0.04854611  0.42088896]\n",
      "    rssi_th [-88.00132862 -72.00266949 -52.00890592]\n",
      "    infect_w [0.00802257 0.06120391 0.35615443]\n"
     ]
    }
   ],
   "source": [
    "# effect of reordering data points\n",
    "bag_size = [1, 2, 4, 8, 16, 32]\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "\n",
    "\n",
    "idx = 0\n",
    "\n",
    "auc_train_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_train_true = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_learned = np.zeros((len(bag_size),n_trials))\n",
    "auc_test_true = np.zeros((len(bag_size),n_trials))\n",
    "for bs in bag_size:\n",
    "  bag_sim = Bag_Simulator(p_pos=0.6,r_pos=2,p_neg=0.6,r_neg=2,max_bag_size=bs,censor_prob_pos=0.,censor_prob_neg=0,max_pos_in_bag=1)\n",
    "  auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(bag_sim, X_epi,\n",
    "                                                                       probabilities_true_epi, n_trials=n_trials,\n",
    "                                                                       n_random_restarts=n_random_restarts_train,order=False)\n",
    "  for i in range(n_trials):\n",
    "    auc_train_learned[idx, i] = dict(auc_train_trials[i])['Learned']\n",
    "    auc_train_true[idx, i] = dict(auc_train_trials[i])['True']\n",
    "    auc_test_learned[idx, i] = dict(auc_test_trials[i])['Learned']\n",
    "    auc_test_true[idx, i] = dict(auc_test_trials[i])['True']\n",
    "  \n",
    "  idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "Hm2a-h6RHVAy",
    "outputId": "83d3c08c-909c-4b29-b9fc-17cf8a8af725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1f87d1fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(bag_size, np.mean(auc_train_learned, axis=1), '+-', label='train, learned')\n",
    "axs[0].plot(bag_size, np.mean(auc_train_true, axis=1), '+-', label='train, true')\n",
    "axs[0].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[0].set_ylabel(\"AUC\")\n",
    "axs[0].set_title(\"Train: No Censoring\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(bag_size, np.mean(auc_test_learned, axis=1), '+-', label='test, learned')\n",
    "axs[1].plot(bag_size, np.mean(auc_test_true, axis=1), '+-', label='test, true')\n",
    "axs[1].set_xlabel(\"Maximum Bag Size\")\n",
    "axs[1].set_ylabel(\"AUC\")\n",
    "axs[1].set_title(\"Test: No Censoring\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPYyTOcQTaYm"
   },
   "source": [
    "### 6. Examples of failure mode\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "Here we test out two scenarios of \"failure\". First is using a large learning rate in the traning process. The learning rate problem is briefly mentioned in II.2.2 when we introduce SGD alogirthm that SGD is sensitive to the learning rate. We show that if the learning rate is large (0.1 vs 0.001 original), the algorithm could not converge to a descent accuracy. Compared to original implementation, the accuracy is only 0.5 for training and 0.17 for testing, where the original hits 0.87 for both traning and testing, with truth being 0.9. The large learning rate is thus considered an example of failure becuase it results in tremedous drop in accuracy and it isnot better than randomlly guessing half of the population is infected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kiwRpi3WTZt4"
   },
   "outputs": [],
   "source": [
    "# failed experiment- learning rate edge \n",
    "def train_and_eval_with_bag_config_fail1(bag_sim: Bag_Simulator, X_epi, probabilities_true_epi, n_trials=1, n_random_restarts=5, order=True):\n",
    "    \"\"\"Train the risk score model on data generated according to the given bag configuration. \"\"\"\n",
    "    auc_train_trials = []\n",
    "    auc_test_trials = []\n",
    "    for j in range(n_trials):\n",
    "        # Y_epi, N_pos, N_neg, pos_neg_ratio = sample_labels(probabilities_true_epi)\n",
    "        X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi, visualize=False, order=order)\n",
    "        best_loss = np.inf\n",
    "        best_model_params = None\n",
    "        best_final_probs = None\n",
    "        for i in range(n_random_restarts):\n",
    "            # here we replace learning rate by 0.1\n",
    "            print('----------- Trial {}/{}: Training run {}/{} ----------------'.format(j+1, n_trials, i+1, n_random_restarts))\n",
    "            model_params, loss_st_step, final_probs = train(X, bag_labels_trn, assign_mat_trn,\n",
    "                                               sigmoid_temp_init=0.1, sigmoid_temp_target=1,\n",
    "                                               batch_size=200, num_iters=5000, lr=0.1)\n",
    "            if loss_st_step < best_loss:\n",
    "                best_loss = loss_st_step\n",
    "                best_model_params = model_params\n",
    "                best_final_probs = final_probs\n",
    "\n",
    "        print(\"best loss\", best_loss)\n",
    "        print(\"best scoring parameters\")\n",
    "        print_params(residual_to_scoring(best_model_params))\n",
    "\n",
    "        probs_bags_true_trn = 1 - np.exp(np.dot(assign_mat_trn, np.log(1-probabilities_true)))\n",
    "        auc_train_trials.append({\n",
    "            'Learned': auc(best_final_probs, bag_labels_trn),\n",
    "            'True': auc(probs_bags_true_trn, bag_labels_trn),\n",
    "        })\n",
    "\n",
    "        scores_learned = scores_learned = loss_fn_stepbins_ce(best_model_params, X, None, None, return_scores=True)\n",
    "        scores_learned_bags_tst = np.dot(assign_mat_tst, scores_learned)\n",
    "        probs_learned_bags_tst = 1 - np.exp(-1*scores_learned_bags_tst)\n",
    "        probs_bags_true_tst = 1 - np.exp(np.dot(assign_mat_tst, np.log(1-probabilities_true)))\n",
    "        auc_test_trials.append({\n",
    "            'Learned': auc(probs_learned_bags_tst, bag_labels_tst),\n",
    "            'True': auc(probs_bags_true_tst, bag_labels_tst),\n",
    "        })\n",
    "\n",
    "    return auc_train_trials, auc_test_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cprc_VxihZFG",
    "outputId": "9fe38377-e177-44d1-ce09-e73908dd3a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4329, 4329) (4329, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 4329) (866, 4329)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.1219768761036102 step loss 1.1134305430934133\n",
      "iter 0 sigmoid loss 0.9663027166935403 step loss 0.837428210184174 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36271120524836975 step loss 0.3381407657863293 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.36149079470830925 step loss 0.3275225543097697 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3758836523867663 step loss 0.31989607206949394 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3699628598627152 step loss 0.3107080338066055 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30020568475549064 step loss 0.4402199868299871 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.30898606758406016 step loss 0.4403082210510724 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.23457354990714596 step loss 0.3240916607391936 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.28006727480966737 step loss 0.31263913187145137 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3460729894233397 step loss 0.3077482800674307 sigmoid temp 1.0\n",
      "End sigmoid loss 0.2983593624725792 step loss 0.307999892932363\n",
      "    beta 0.2841002950087045\n",
      "    rssi_w [-0.01118165  0.00219091  0.02885734  0.26976416]\n",
      "    rssi_th [14.01189321 10.0119251  32.01188942]\n",
      "    infect_w [0.01068561 0.06612105 0.26178645]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.1457434532965907 step loss 1.1491762523270734\n",
      "iter 0 sigmoid loss 0.9902394561477001 step loss 0.8487481950915559 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36180377961950355 step loss 0.340671065651749 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.35957862172517835 step loss 0.32983856362839264 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3720004789111704 step loss 0.32216890203338233 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.36085029049721595 step loss 0.314755913536715 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2891252498941304 step loss 0.34557096692299266 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29112235898332783 step loss 0.3160033305059157 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2638199614556918 step loss 0.3053500017175401 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2796526439865578 step loss 0.3028299446152391 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3343538784399405 step loss 0.3014388400608691 sigmoid temp 1.0\n",
      "End sigmoid loss 0.29101660087516673 step loss 0.30134850584165435\n",
      "    beta 0.36188739982169194\n",
      "    rssi_w [-0.01955325 -0.00111057  0.07568212  0.34122789]\n",
      "    rssi_th [16.9902897  17.99031881 32.98982456]\n",
      "    infect_w [0.01152648 0.08235205 0.34068951]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.2020121659779306 step loss 1.2010239727032244\n",
      "iter 0 sigmoid loss 1.036905700820247 step loss 0.8143162087154622 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36210924011377926 step loss 0.3367130158210591 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3591718675515695 step loss 0.3238694607180305 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3714104558996327 step loss 0.3138481536287908 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3588939342904793 step loss 0.30365594350010133 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2861934782124144 step loss 0.7864866695189553 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2842112350200353 step loss 0.3169160178920426 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2257839986385018 step loss 0.2973027849812792 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.27447369839729996 step loss 0.29382376549418915 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.33888263356816517 step loss 0.29213685251488386 sigmoid temp 1.0\n",
      "End sigmoid loss 0.280771949612669 step loss 0.29230298359724893\n",
      "    beta 0.3451541150638725\n",
      "    rssi_w [-0.01504015 -0.00227843  0.06091929  0.32659457]\n",
      "    rssi_th [ 9.99768374 22.99769922 30.99740691]\n",
      "    infect_w [0.01085689 0.07305679 0.32561762]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1399673984433492 step loss 1.1446680057496375\n",
      "iter 0 sigmoid loss 0.9831912052486962 step loss 0.8394458189424857 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.35998843337571296 step loss 0.3368872131987415 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3546987657773586 step loss 0.32399037660499697 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36513790651788824 step loss 0.31271755682816343 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.34184305558628286 step loss 0.29808727913382205 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2692442831098379 step loss 0.30201994734881726 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.2880803291450651 step loss 0.2902010907477397 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.21727002293537295 step loss 0.28970412024695635 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.26733486823414665 step loss 0.28913330754522154 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.33862514731809507 step loss 0.28915179176908673 sigmoid temp 1.0\n",
      "End sigmoid loss 0.27705788092489003 step loss 0.2892166535870432\n",
      "    beta 0.3250073971588051\n",
      "    rssi_w [-0.05463631  0.00118022  0.10047149  0.30548269]\n",
      "    rssi_th [33.00310092 16.00303798 13.00321009]\n",
      "    infect_w [0.01062478 0.0709289  0.3043646 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.1626413073218032 step loss 1.1921772548396574\n",
      "iter 0 sigmoid loss 1.0003442518942394 step loss 0.8947840177353745 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.36092287211641577 step loss 0.34760736404978787 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3549934667427918 step loss 0.3376806412691283 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36625373990167553 step loss 0.33317912434326447 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35670347973880573 step loss 0.33202686881685917 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28792205612352056 step loss 0.3524469092536834 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31282368970016683 step loss 0.3336261396993105 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2817749232613088 step loss 0.32795175923211173 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2838234533596232 step loss 0.3258442208140686 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3552570275075493 step loss 0.32461680818996236 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3145478356617899 step loss 0.32447164045091903\n",
      "    beta 0.36140357933801537\n",
      "    rssi_w [-0.02323071  0.09388365  0.33166136  0.05052908]\n",
      "    rssi_th [37.9868323  33.98588074 26.99952635]\n",
      "    infect_w [0.01204858 0.08662959 0.33868893]\n",
      "best loss 0.2892166535870432\n",
      "best scoring parameters\n",
      "    beta 0.3250073971588051\n",
      "    rssi_w [-0.05463631 -0.05345609  0.0470154   0.35249809]\n",
      "    rssi_th [-86.99689908 -70.9938611  -57.99065101]\n",
      "    infect_w [0.01062478 0.08155368 0.38591828]\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "n_trials = 1\n",
    "n_random_restarts_train = 5\n",
    "auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                n_trials=n_trials, n_random_restarts=n_random_restarts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFRRseUthd0M",
    "outputId": "9b77dc5c-e22e-4ad2-e48d-67322b418ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4329, 4329) (4329, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 4329) (866, 4329)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.1040380849114508 step loss 1.1100025411852166\n",
      "iter 0 sigmoid loss 0.7472514014654863 step loss 6.940842522733289 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.5542535878142312 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 2.0147702064110398 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.7269473197880345 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8420764744372367 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 2.3025930930340457 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.1874639383848433 step loss 1.8883542056736473 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8996410517618378 step loss 1.8883542056736473 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8883542056736473 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.6118181651388321 step loss 1.8883542056736473 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8883542056736473 step loss 1.8883542056736473\n",
      "    beta -1.2151600919414363\n",
      "    rssi_w [0.28376745 0.28725701 0.26474128 0.12409393]\n",
      "    rssi_th [19.00227773 31.0023046  25.00145728]\n",
      "    infect_w [0.34213929 0.57401231 0.47431262]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.3234328330105503 step loss 1.322511194138947\n",
      "iter 0 sigmoid loss 0.8758795055776042 step loss 9.29335749463293 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.5542535878142312 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 2.0147702064110398 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.7269473197880345 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8420764744372367 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 2.3025930930340457 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.1874639383848433 step loss 1.8883542056736473 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8996410517618378 step loss 1.8883542056736473 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8883542056736473 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.6118181651388321 step loss 1.8883542056736473 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8883542056736473 step loss 1.8883542056736473\n",
      "    beta -0.08411561164555223\n",
      "    rssi_w [0.89211075 0.861778   0.72944081 0.27047382]\n",
      "    rssi_th [31.00032396 19.00035183 26.00016742]\n",
      "    infect_w [1.4491814  1.47404627 0.75252978]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.274822916393252 step loss 1.277161912970972\n",
      "iter 0 sigmoid loss 0.869610925271254 step loss 9.122991174765934 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.5542535878142312 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 2.0147702064110398 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.7269473197880345 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8420764744372367 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 2.3025930930340457 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.1874639383848433 step loss 1.8883542056736473 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8996410517618378 step loss 1.8883542056736473 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8883542056736473 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.6118181651388321 step loss 1.8883542056736473 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8883542056736473 step loss 1.8883542056736473\n",
      "    beta -0.1941047405613517\n",
      "    rssi_w [0.5955551  0.59215407 0.56512745 0.29579467]\n",
      "    rssi_th [18.00038447 15.00040068 35.00034628]\n",
      "    infect_w [1.5123345  1.54451291 1.25238962]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1777957735059572 step loss 1.173851115715025\n",
      "iter 0 sigmoid loss 0.7901091955609181 step loss 8.519828197340118 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.5542535878142312 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 2.0147702064110398 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.7269473197880345 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8420764744372367 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 2.3025930930340457 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.1874639383848433 step loss 1.8883542056736473 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8996410517618378 step loss 1.8883542056736473 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8883542056736473 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.6118181651388321 step loss 1.8883542056736473 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8883542056736473 step loss 1.8883542056736473\n",
      "    beta -0.7011228347371002\n",
      "    rssi_w [0.30010918 0.29708958 0.2979463  0.25745628]\n",
      "    rssi_th [10.00080402 11.00083999 30.00092892]\n",
      "    infect_w [1.08725984 1.16024169 0.77224845]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0889428309813716 step loss 1.0860534926777339\n",
      "iter 0 sigmoid loss 0.7383080390097849 step loss 7.006774104063675 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.5542535878142312 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 2.0147702064110398 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 1.7269473197880345 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8420764744372367 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 2.3025930930340457 step loss 1.8883542056736473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.1874639383848433 step loss 1.8883542056736473 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 1.8996410517618378 step loss 1.8883542056736473 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.439124433165029 step loss 1.8883542056736473 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.6118181651388321 step loss 1.8883542056736473 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8883542056736473 step loss 1.8883542056736473\n",
      "    beta -1.011445935286781\n",
      "    rssi_w [0.32110423 0.32314634 0.32081075 0.23235056]\n",
      "    rssi_th [10.00105593 17.00111999 34.00119171]\n",
      "    infect_w [0.33296816 0.52918572 0.43347082]\n",
      "best loss 1.8883542056736473\n",
      "best scoring parameters\n",
      "    beta -1.2151600919414363\n",
      "    rssi_w [0.28376745 0.57102446 0.83576575 0.95985968]\n",
      "    rssi_th [-100.99772227  -69.99541767  -44.99396039]\n",
      "    infect_w [0.34213929 0.9161516  1.39046422]\n"
     ]
    }
   ],
   "source": [
    "## Failure due to large learning rate \n",
    "# lr = 0.1\n",
    "auc_train_trials_fail1, auc_test_trials_fail1 = train_and_eval_with_bag_config_fail1(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                                  censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                    n_trials=n_trials, n_random_restarts=n_random_restarts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0umbO6fyhh1f",
    "outputId": "de5a0c89-a49e-4b35-83c7-5ce650d48d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Learned': 0.8815031379989784, 'True': 0.9200731591622273}] [{'Learned': 0.8578709828028948, 'True': 0.898470936113921}]\n",
      "[{'Learned': 0.5, 'True': 0.9113959230338856}] [{'Learned': 0.17880417866313908, 'True': 0.9314790677768267}]\n"
     ]
    }
   ],
   "source": [
    "print(auc_train_trials, auc_test_trials)\n",
    "print(auc_train_trials_fail1, auc_test_trials_fail1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II3ct6_nhwNa"
   },
   "source": [
    "The second senario we want to test is using a very large bag size (128), compared to the largest bag size of 32 that we used in our main experiment. To save time, we do not incorporate random restart in this part but we compare the large bag size implementation with the original one. Notice that the train accuracy has dropoed to 0.5, and test accuracy has dropped to 0.34 The scenario is considered another example of failure because it is not better than randomlly guessing half of the population is infected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eOPLe35l3tt",
    "outputId": "ddd0035c-e5f7-4434-9599-5e995662ef1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4329, 4329) (4329, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 4329) (866, 4329)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/1 ----------------\n",
      "Start sigmoid loss 1.2482413242950465 step loss 1.257868839025658\n",
      "iter 0 sigmoid loss 1.3089002228609719 step loss 0.8105117319211484 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3601045326008543 step loss 0.3504521219930372 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.32678743194839205 step loss 0.3402750577323596 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.2814995703769667 step loss 0.33519728759973993 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.28184545917561143 step loss 0.3321994978236391 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.28785784219343813 step loss 0.3420697972974473 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.32177173272940907 step loss 0.3327175800784395 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.30848847742173413 step loss 0.32835832822378885 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.34054734348224164 step loss 0.32629551911146465 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.2714758048137344 step loss 0.32536195273038765 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3159437624260285 step loss 0.3252708364415301\n",
      "    beta 0.3589744596548371\n",
      "    rssi_w [-0.02972732  0.09826311  0.32823586  0.0456899 ]\n",
      "    rssi_th [38.98712193 32.98602841 28.99960241]\n",
      "    infect_w [0.013443   0.08319367 0.33627621]\n",
      "best loss 0.3252708364415301\n",
      "best scoring parameters\n",
      "    beta 0.3589744596548371\n",
      "    rssi_w [-0.02972732  0.06853579  0.39677166  0.44246156]\n",
      "    rssi_th [-81.01287807 -48.02684966 -19.02724725]\n",
      "    infect_w [0.013443   0.09663667 0.43291288]\n"
     ]
    }
   ],
   "source": [
    "# failed experiment- very large bag size\n",
    "\n",
    "# original with only one restarts\n",
    "auc_train_trials2, auc_test_trials2 = train_and_eval_with_bag_config(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                n_trials=n_trials, n_random_restarts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytSbw7gUl48x",
    "outputId": "0cae2016-48ea-473c-9025-f5bc976f1774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5510, negatives: 28090\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 5.424, median size 5\n",
      "\t Negative bags: mean size 5.676, median size 5\n",
      "assign_mat size, X_shuff size: (4329, 24393) (24393, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3463, 24393) (853, 24393)\n",
      "Average positive samples per bag: 2.0225352112676056\n",
      "----------- Trial 1/1: Training run 1/1 ----------------\n",
      "Start sigmoid loss 1.1043613021850538 step loss 1.106081606826197\n",
      "iter 0 sigmoid loss 0.9274505669663057 step loss 0.5786967530300093 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.40193995281904654 step loss 0.36923183783669455 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3345641027245263 step loss 0.35555054529122104 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3523339335787491 step loss 0.65126397413235 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 2.01477020641104 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 1.49668901048963 step loss 1.888354205673647 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 1.6693827424634333 step loss 1.888354205673647 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 2.53285140233245 step loss 1.888354205673647 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 1.8996410517618378 step loss 1.888354205673647 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 1.7845118971126357 step loss 1.888354205673647 sigmoid temp 1.0\n",
      "End sigmoid loss 1.888354205673647 step loss 1.888354205673647\n",
      "    beta -0.07472037844301627\n",
      "    rssi_w [1.00751246 1.00079449 0.95560032 0.45536793]\n",
      "    rssi_th [ 9.99592727 18.99598599 36.99606376]\n",
      "    infect_w [0.1331177  0.08801679 0.22843648]\n",
      "best loss 1.888354205673647\n",
      "best scoring parameters\n",
      "    beta -0.07472037844301627\n",
      "    rssi_w [1.00751246 2.00830695 2.96390727 3.4192752 ]\n",
      "    rssi_th [-110.00407273  -91.00808674  -54.01202298]\n",
      "    infect_w [0.1331177  0.22113449 0.44957097]\n"
     ]
    }
   ],
   "source": [
    "# failure due to large bag size\n",
    "# bag size = 128\n",
    "auc_train_trials_fail2, auc_test_trials_fail2 = train_and_eval_with_bag_config(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=128,\n",
    "                                                                                                  censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                    n_trials=n_trials, n_random_restarts=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiLUbURBl5FK",
    "outputId": "ac183fa4-501b-44c3-ad5c-e134a540ca1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Learned': 0.8425077233695785, 'True': 0.9157514169646549}] [{'Learned': 0.8392002567893548, 'True': 0.916810948564314}]\n",
      "[{'Learned': 0.5, 'True': 0.8811789997324186}] [{'Learned': 0.3443671876547612, 'True': 0.8843277668825895}]\n"
     ]
    }
   ],
   "source": [
    "print(auc_train_trials2, auc_test_trials2)\n",
    "print(auc_train_trials_fail2, auc_test_trials_fail2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eHEAC-Ms5Qh"
   },
   "source": [
    "# III. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2AU4FGtCYxr"
   },
   "source": [
    "## 1. Evaluation\n",
    "\n",
    "Overall, this seems like a pretty generic workflow similar to what we are used to in classâ€“define a generative model, get data, use data to predict optimal parameter values for the model and check the accuracy with a predefined loss function. We do not see any major standout issues. However, we do see room to question some of their design choices.\n",
    "\n",
    "### 1.1 Choice of Loss Function\n",
    "For example, we find that the author's choice of loss function does not reflect real world considerations fully. In the context of the pandemic, we would prefer a model that is more liberal in its estimates of risk because we would ideally want to err on the side of caution in the short runâ€”we would rather isolate individuals who may not be infected, than to let individuals who are infected roam freely. In this sense, we would prefer a model with a higher false positive rate than a lower false negative rate. The loss function that is being minimized over right now does not seem to take this into account by weighing equally the risk scores for those infected and those not infected. From a practical, policy standpoint, we might want to adjust this to be more liberal in predictions. Alternatively, recognizing this feature of the model can also lead to a practical implementation where the threshold for alerting users of their close contact status is lower to compensate for the predictions.\n",
    "\n",
    "### 1.2 Robustness to Model Mismatch\n",
    "This discussion continues from Experiment 4 in the previous section. In the paper, the authors tested the robustness of their model to a potential model mismatch, by using a Taylor series approximation to the generative function for calculating the probability of infection. Poorer approximations (with fewer terms) would therefore mean a greater mismatch. However, this still assumes that the underlying model is fixed and is correct, while the approximations just appear to mimic noise in the real world. We find that the authors have not sufficiently proven that the model is truly robust to model mismatches where the generative model they have assumed is almost fundamentally flawed in design to begin with. This may potentially disadvantage their model in favor of an expert-driven model, since it is less flexible to change the whole structure of the learning model and re-learn parameters, compared to an expert changing guidance on thresholds immediately in response to the pandemic.\n",
    "\n",
    "### 1.3 Comparison with Expert Guided Fixed Thresholds\n",
    "Another consideration we had was the author's claim that their model performs better than fixed models with expert-guided thresholds. While they did perform a comparison with the Swiss model, we are hard pressed to accept this comparison as sufficient evidence that their claim holds. For one, the data they used to test both models originated from their data generator with specific parameters. They have not provided us with enough evidence to show that the data generated is reflective of the real world (apart from the fact that the equations are based on real world physics, there is no explicit justification for the parameters used in generating data) and less that it is reflective of the situation in Switzerland, which the Swiss model was presumably being tested in. Second, since we (and the authors) did not have access to the data from the GAEN app, we can only take the author's word that the data generated from their simulation was similar enough to that collected by GAEN, that the model can be easily ported over for integration and use with GAEN.\n",
    "\n",
    "### 1.4 Accuracy and Uncertainty of Model\n",
    "Nonetheless, it seems promising that we are seeing the right trends in the other experiments they conducted to test their own algorithm. For example, increasing bag size and censoring probability increases the difficulty of the optimization since there is potentially more noise or less signal, and we see a corresponding decrease in accuracy. Increasing signal through increasing positive bag constructions increases accuracy to a certain point where the benefit of the increased signal is trumped by the drawbacks of noise. These experiments give us confidence that the model is being uncertain in the right conditions as we expect. \n",
    "\n",
    "However, further work would need to be done to properly quantify the level of uncertainty and decide if it is practical. For example, we see that the inaccuracy drops as bag size increases, but the main experiments only tested until a bag size of 32. It is unclear if this is a realistic bag size to work with for real data, especially at the height of the pandemic when a single person may have many exposure events that it presents a separate problem just deciding which exposure events go in one bag and how to aggregate risk scores across different bags meaningfully. Moreover, if an ideal bag size is larger than 32, like in the demonstration of failure mode No.2, then we know for a fact that the accuracy will drop which begs the question of whether any potential gains in accuracy over an expert determined model is significant enough to justify an ML model. If anything, the experiments definitely highlighted that a lot of the parameters (e.g. maximum bag size, maximum number of positive exposures, censoring probability) can and probably should be tuned according to what we know of the real world constraints. For example, we would want to adjust the maximum bag size to make sure that there is substantive signal from positive exposures, given the average number of positive exposures a person may receive daily, but also make sure the bag size is still realistic and efficient to run. Censoring probability in real life would also come with knowledge of take-up rates of the contact tracing app and how well people use them. Depending on that, we would have a gauge of where the model could fail and compensate or guard against it.\n",
    "\n",
    "Last but no the least, through the failure mode No.1, we acknowledge that every algorithm has its advantages and drawbacks and some uncareful choice of parameters can result in decreasing model accuracy (increasing model uncertainty). For example, the SGD algorithm is sensitive to the learning rate, if the rate is set to be extremely low or high, the gradient search will stuck in a place or jump frantically. Other ML algorithms have issues, too, such as too easy to overfit. To conclude, the story warns us that we have to be aware of the inherited problems associated with specific alogorithm to choose in the problem. Some algorithm needs to be carefully tuned before experiments. As an extension, we actually optimize the algorithm in III.2 Future Work and Potential Modifications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kHNDek3CbFF"
   },
   "source": [
    "## 2. Future Work and Potential Modifications\n",
    "\n",
    "### 2.1 Possible Modification 1: Optimization for SGD\n",
    "\n",
    "Based on the common tricks mentioned in LeCun et al.'s Backprop chapter [11], we could consider the following methods for improving the performance of the stochastic gradient descent.\n",
    "\n",
    "1. **Shuffling and Curriculum Learning.** Generally, we want to avoid providing the training examples in a meaningful, consistent order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.\n",
    "\n",
    "2. **Batch Normalization.** To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.\n",
    "\n",
    "  According to Ioffe and Szegedy's paper on Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [12], batch normalization reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.\n",
    "\n",
    "3. **Early Stopping.** According to Geoff Hinton: \"Early stopping is beautiful free lunch\" (NIPS 2015 Tutorial slides, slide 63). We should thus always monitor error on a validation set during training and stop (with some patience) if our validation error does not improve enough.\n",
    "\n",
    "We also recognize that even though these tricks have their advantages, we cannot blindly assume that their benefits hold true for all machine learning models. There is definitely room to understand our model better and see if there is even a need to be concerned about certain things, like batch normalizations, if the model was actually simple enough that the learning and update of parameters need not be complicated further.\n",
    "\n",
    "### 2.2 Possible Modification 2: Other Parameter Learning Methods\n",
    "\n",
    "Here, we explore specific learning methods that can enhance SGD for more efficient learning.\n",
    "\n",
    "1. **Momentum.** SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum. Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector: \n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\ \n",
    "\\theta &= \\theta - v_t \n",
    "\\end{split} \n",
    "\\end{align}\n",
    "\n",
    "  This could potentially help speed up the efficiency of the SGD algorithm, though careful tuning would be needed to avoid too big a momentum such that the SGD falls into a local optimum too quickly and the algorithm terminates prematurely.\n",
    "\n",
    "2. **Adagrad.** Adagrad is an algorithm for gradient-based optimization. It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited to dealing with sparse data. Dean et al. found in their paper, Large Scale Distributed Deep Networks [13], that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, whichâ€”among other thingsâ€”learned to recognize cats in YouTube videos.\n",
    "\n",
    "  In its update rule, Adagrad modifies the general learning rate $\\eta$ at each time step $t$ for every parameter $\\theta_i$ based on the past gradients that have been computed for $\\theta_i$:\n",
    "$$\\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i}$$\n",
    "\\\n",
    "$G_{t, ii}$ here is a diagonal matrix where each diagonal element $i$, $i$ is the sum of the squares of the gradients w.r.t. $\\theta_i$ up to time step $t$, while $\\epsilon$ is a smoothing term that avoids division by zero (usually on the order of 1eâˆ’8).\n",
    "\n",
    "  While this method may have its benefits, for our particular model, it may not be completely relevant since we are not exactly working with sparse data (and also assume that our data is complete, save for the censoring). Nonetheless, in a real world setting, as the authors of our paper have mentioned, it is likely that the contact tracing app may miss picking up on a lot of exposure events. Hence, the problem may be reformulated to be one of determining risk score with sparse data, and this method may come in handy then.\n",
    "\n",
    "3. **Adadelta.** Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size $w$. With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.\n",
    "\n",
    "4. **Adam.** Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ \n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \n",
    "\\end{split} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ultimately, it is a balance we need to strike between overcomplicating the model and optimizing methods for greater efficiency.\n",
    "\n",
    "\n",
    "### 2.3 Implementation of Optimization\n",
    "\n",
    "In this part, we implement several proposed optimization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmbXs716acLG"
   },
   "outputs": [],
   "source": [
    "# Implementation of Optimization 1 - Shuffling and Curriculum Learning.\n",
    "\n",
    "def train_and_eval_with_bag_config_mod_v1(bag_sim: Bag_Simulator, X_epi, probabilities_true_epi, n_trials=1, n_random_restarts=5):\n",
    "    '''\n",
    "    first modification - Shuffling and Curriculum Learning.\n",
    "    '''\n",
    "    auc_train_trials = []\n",
    "    auc_test_trials = []\n",
    "    for j in range(n_trials):\n",
    "        # Since there is alrealy randomness inside the bag_sim.simulate_bagged_data function, we\n",
    "        # don't implement shuffling here for each trial\n",
    "        X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = \\\n",
    "            bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi, visualize=False)\n",
    "        best_loss = np.inf\n",
    "        best_model_params = None\n",
    "        best_final_probs = None\n",
    "        for i in range(n_random_restarts):\n",
    "            print('----------- Trial {}/{}: Training run {}/{} ----------------'.format(j+1, n_trials, i+1, n_random_restarts))\n",
    "            # Modification here, shuffling here before each iteration\n",
    "            # Since our three input variables for training input are dependent of each other\n",
    "            # need to wisely shuffle the data\n",
    "            idx1 = np.arange(assign_mat_trn.shape[0])\n",
    "            np.random.shuffle(idx1)\n",
    "            bag_labels_trn = bag_labels_trn[idx1]\n",
    "            assign_mat_trn = assign_mat_trn[idx1, :]\n",
    "\n",
    "            model_params, loss_st_step, final_probs = train(X, bag_labels_trn, assign_mat_trn,\n",
    "                                               sigmoid_temp_init=0.1, sigmoid_temp_target=1,\n",
    "                                               batch_size=200, num_iters=5000, lr=0.001)\n",
    "            if loss_st_step < best_loss:\n",
    "                best_loss = loss_st_step\n",
    "                best_model_params = model_params\n",
    "                best_final_probs = final_probs\n",
    "\n",
    "        print(\"best loss\", best_loss)\n",
    "        print(\"best scoring parameters\")\n",
    "        print_params(residual_to_scoring(best_model_params))\n",
    "\n",
    "        probs_bags_true_trn = 1 - np.exp(np.dot(assign_mat_trn, np.log(1-probabilities_true)))\n",
    "        auc_train_trials.append({\n",
    "            'Learned': auc(best_final_probs, bag_labels_trn),\n",
    "            'True': auc(probs_bags_true_trn, bag_labels_trn),\n",
    "        })\n",
    "\n",
    "        scores_learned = loss_fn_stepbins_ce(best_model_params, X, None, None, return_scores=True)\n",
    "        scores_learned_bags_tst = np.dot(assign_mat_tst, scores_learned)\n",
    "        probs_learned_bags_tst = 1 - np.exp(-1*scores_learned_bags_tst)\n",
    "        probs_bags_true_tst = 1 - np.exp(np.dot(assign_mat_tst, np.log(1-probabilities_true)))\n",
    "        auc_test_trials.append({\n",
    "            'Learned': auc(probs_learned_bags_tst, bag_labels_tst),\n",
    "            'True': auc(probs_bags_true_tst, bag_labels_tst),\n",
    "        })\n",
    "\n",
    "    return auc_train_trials, auc_test_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LI3jYJCgay2Z",
    "outputId": "5b3791b4-3030-4d97-f32f-96610406b552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5468, negatives: 28132\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4362, 4362) (4362, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3489, 4362) (873, 4362)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.175454359499679 step loss 1.1832132363026815\n",
      "iter 0 sigmoid loss 1.117649362414514 step loss 0.8203591934372848 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.35707710172401136 step loss 0.35665064280738146 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30134682717362193 step loss 0.3477959750554159 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.320967402585495 step loss 0.341930252024401 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3601606781223854 step loss 0.33620355160992454 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3108253803566541 step loss 0.33951994942820707 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.28368279314657735 step loss 0.3331322280170496 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.314742110941194 step loss 0.33052413621735105 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35939014189468965 step loss 0.3285581887256153 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.307463261052493 step loss 0.3271980722345064 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3202877715184831 step loss 0.3271534472272605\n",
      "    beta 0.3674340512931472\n",
      "    rssi_w [-0.07016319 -0.00715545  0.14010975  0.31979209]\n",
      "    rssi_th [31.99115258 10.99110705 28.98919164]\n",
      "    infect_w [0.01315177 0.09385931 0.34281078]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.0771338394573606 step loss 1.0800334238163982\n",
      "iter 0 sigmoid loss 1.0200397895368374 step loss 0.8348038708842997 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3565983926027269 step loss 0.3479807873602735 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3017770675288302 step loss 0.33726327930362854 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32205472714417366 step loss 0.32825676561026407 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.35812455437549146 step loss 0.3163929406285835 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30078649114517314 step loss 0.4756833347562294 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.29517980847367464 step loss 0.3158733075551057 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2980187283820313 step loss 0.3176714310632307 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.32633096066560213 step loss 0.31526712682861296 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.314697135741725 step loss 0.3138260052644261 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3071843972833988 step loss 0.31400121918974294\n",
      "    beta 0.2893414872111698\n",
      "    rssi_w [-0.05872754 -0.01510624  0.09957395  0.25147242]\n",
      "    rssi_th [29.013529   17.0134983  11.01355341]\n",
      "    infect_w [0.01139964 0.07363368 0.26459577]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.2658655464677038 step loss 1.2648647336092953\n",
      "iter 0 sigmoid loss 1.2005092034804057 step loss 0.7671637316484229 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3587234401166512 step loss 0.36318192563046303 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3084965281169524 step loss 0.3587846285745685 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.33365306558763136 step loss 0.3585461112881062 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.371849249413504 step loss 0.357881401684053 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.33610950155419106 step loss 0.3582193351114027 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31548939534678383 step loss 0.36399485480579796 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3237759781793778 step loss 0.3636256410313402 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3677576877697471 step loss 0.3637510573117591 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.34098157136074947 step loss 0.36377856631507416 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3458668904556869 step loss 0.3631699033681599\n",
      "    beta 0.2869283741684491\n",
      "    rssi_w [-0.0643455  -0.03937188  0.01172672  0.26042512]\n",
      "    rssi_th [21.01031292 11.01035528 18.01034316]\n",
      "    infect_w [0.01277739 0.08036489 0.26069394]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.2491117478102614 step loss 1.2466017246337888\n",
      "iter 0 sigmoid loss 1.184584318792444 step loss 0.7927295298619816 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3581990012990465 step loss 0.3645337493112961 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.30545623206629335 step loss 0.3594676458345833 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3295871888669818 step loss 0.35813917691487246 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.36932011049499297 step loss 0.35696019385549377 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.33271993195063154 step loss 0.3559995648781122 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3255832476667148 step loss 0.35519173695117756 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.34447349539030125 step loss 0.35455609826491874 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.39078485255185486 step loss 0.35422522159283587 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.34867665841306467 step loss 0.3540481928463866 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3514194889026039 step loss 0.3539206206837834\n",
      "    beta 0.30841154746431554\n",
      "    rssi_w [-0.05587615  0.00691536  0.16507394  0.23582111]\n",
      "    rssi_th [30.99863468 12.99861645 37.99584031]\n",
      "    infect_w [0.01422222 0.08892061 0.28151565]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.0898781162622277 step loss 1.0885482448899388\n",
      "iter 0 sigmoid loss 1.0333147144612085 step loss 0.8341307296189877 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3565955658199472 step loss 0.34559490782027014 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3014121175502259 step loss 0.33297535313989085 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3215791386731428 step loss 0.3219910995186105 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.359036105438005 step loss 0.3093666977728371 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30517396146357795 step loss 0.5853857857179227 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.27697318350476435 step loss 0.32823649557537626 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.2852165098662003 step loss 0.31077106128836196 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.3070650714220685 step loss 0.30553376024950113 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.31609073052363934 step loss 0.3032528576032665 sigmoid temp 1.0\n",
      "End sigmoid loss 0.29446381922897863 step loss 0.3038440922688811\n",
      "    beta 0.3131636109292678\n",
      "    rssi_w [-0.0339953  -0.0240882   0.08959544  0.28331788]\n",
      "    rssi_th [13.00798561 28.00799278 19.00756412]\n",
      "    infect_w [0.01091791 0.0735068  0.29041621]\n",
      "best loss 0.3038440922688811\n",
      "best scoring parameters\n",
      "    beta 0.3131636109292678\n",
      "    rssi_w [-0.0339953  -0.0580835   0.03151194  0.31482982]\n",
      "    rssi_th [-106.99201439  -78.9840216   -59.97645748]\n",
      "    infect_w [0.01091791 0.08442472 0.37484093]\n"
     ]
    }
   ],
   "source": [
    "## Original Implementation for Comparison\n",
    "auc_train_trials, auc_test_trials = train_and_eval_with_bag_config(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                n_trials=n_trials, n_random_restarts=n_random_restarts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGKmvGebatDV",
    "outputId": "fe16f130-e3a8-4b2e-b526-0a31f3d78608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5468, negatives: 28132\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4362, 4362) (4362, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3489, 4362) (873, 4362)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.1401424218642982 step loss 1.1436560013052968\n",
      "iter 0 sigmoid loss 1.3462001999257598 step loss 0.7672636596642491 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3431212433022248 step loss 0.36417474510526776 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.33504211084725966 step loss 0.36078466496277367 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.34733871864549487 step loss 0.36087523781564873 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.31419139476353386 step loss 0.3608011422532609 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.309317508597252 step loss 0.3612132033152587 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.327192316094813 step loss 0.36042844923895306 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3423636813162942 step loss 0.36033438327150497 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35162458931183965 step loss 0.36033896587576525 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3557900969785965 step loss 0.36034482315468946 sigmoid temp 1.0\n",
      "End sigmoid loss 0.36023190285979095 step loss 0.36033316958899464\n",
      "    beta 0.220563615202407\n",
      "    rssi_w [-0.0063548   0.01677183  0.05805691  0.18891131]\n",
      "    rssi_th [22.00230957 10.00237039 13.00250901]\n",
      "    infect_w [0.01045464 0.05911262 0.19295159]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.236647737918061 step loss 1.2465425193048665\n",
      "iter 0 sigmoid loss 1.1536670363636998 step loss 0.8154070671807274 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4602245433499331 step loss 0.35758811583294886 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3191523933281198 step loss 0.35025708933914 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.29336166486401233 step loss 0.3457561530116548 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4458199516811221 step loss 0.3409417831217647 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.29865361630729026 step loss 0.3379677414587886 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.25533930902344104 step loss 0.33586579630965396 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.45194367471393737 step loss 0.3352630417980278 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.35785893021515025 step loss 0.33436850390959016 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.277396723816177 step loss 0.3339110348509307 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3233467556374652 step loss 0.3337275972575485\n",
      "    beta 0.3663136913771957\n",
      "    rssi_w [-0.06384534 -0.05667536  0.18342005  0.29154415]\n",
      "    rssi_th [12.99664157 33.99664624 25.99059173]\n",
      "    infect_w [0.01511239 0.10140753 0.33903767]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.1308300541177252 step loss 1.125942609666351\n",
      "iter 0 sigmoid loss 0.8777181184952721 step loss 0.8504996054781335 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.30895914326320795 step loss 0.3463762477876531 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.34878746974823116 step loss 0.33588911093376983 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32853164737299084 step loss 0.3274769000651727 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.32923107953929753 step loss 0.31739036468188997 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.22810447808858847 step loss 0.4742797673096794 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.31538002947587784 step loss 0.4738937423059239 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.27214589953959456 step loss 0.3197845896602318 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2334844273737094 step loss 0.31478297859036886 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3478845236790072 step loss 0.31316407982935235 sigmoid temp 1.0\n",
      "End sigmoid loss 0.30677726632444346 step loss 0.3128142361174993\n",
      "    beta 0.2831805127028296\n",
      "    rssi_w [-0.00559402  0.00310695  0.03106268  0.26663911]\n",
      "    rssi_th [15.01309386 10.0131303  32.01308958]\n",
      "    infect_w [0.01202266 0.06888319 0.25878685]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1603819678263663 step loss 1.1608008536832246\n",
      "iter 0 sigmoid loss 1.093479832574556 step loss 0.8138793917262754 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.382255474248154 step loss 0.35030401488695684 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3900769964429307 step loss 0.34174994807147796 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.32566082577075733 step loss 0.33584345197604465 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.29702984542444344 step loss 0.32898255475952765 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.30598455330526536 step loss 0.3279480882684482 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.27407759329779025 step loss 0.44600418283862 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.249496871951325 step loss 0.44482881978410244 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.2726846599319101 step loss 0.32926202146846145 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.30229395271218457 step loss 0.3265693588043742 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3191916094531024 step loss 0.32577036956249406\n",
      "    beta 0.26831623900204205\n",
      "    rssi_w [-0.00338974  0.00707756  0.02132021  0.25065161]\n",
      "    rssi_th [10.01414259 12.01416872 33.01416666]\n",
      "    infect_w [0.0124663  0.06827428 0.24245894]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.2593495159768548 step loss 1.259782486120562\n",
      "iter 0 sigmoid loss 1.3756625129626878 step loss 0.7627413587588253 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.42494978611750867 step loss 0.36498363532509065 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3231099660663862 step loss 0.3605020882182618 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.39396145834816776 step loss 0.3599727421033583 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4012764525315393 step loss 0.35938847314509625 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2847547649145002 step loss 0.35943776540407124 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.3766327337415516 step loss 0.3586734458044755 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3811660763568456 step loss 0.35784678700403966 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.29049872288161893 step loss 0.3576404112663247 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.3930488448769449 step loss 0.35749610175527313 sigmoid temp 1.0\n",
      "End sigmoid loss 0.350913458149417 step loss 0.357345054774594\n",
      "    beta 0.32195640186906116\n",
      "    rssi_w [-0.06749454 -0.05558537  0.22927979  0.18732512]\n",
      "    rssi_th [13.00790314 35.00790573 35.9971166 ]\n",
      "    infect_w [0.01709291 0.09846075 0.29220741]\n",
      "best loss 0.3128142361174993\n",
      "best scoring parameters\n",
      "    beta 0.2831805127028296\n",
      "    rssi_w [-0.00559402 -0.00248707  0.02857561  0.29521471]\n",
      "    rssi_th [-104.98690614  -94.97377583  -62.96068625]\n",
      "    infect_w [0.01202266 0.08090585 0.3396927 ]\n"
     ]
    }
   ],
   "source": [
    "## Implementation for Optimization\n",
    "auc_train_trials_mod1, auc_test_trials_mod1 = train_and_eval_with_bag_config_mod_v1(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                                  censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                    n_trials=n_trials, n_random_restarts=n_random_restarts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DF9elljjgIOk",
    "outputId": "a10c7add-63d7-4ea5-ad16-5cb8433b32b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Learned': 0.8707116027214296, 'True': 0.905479866532299}] [{'Learned': 0.8590778597714879, 'True': 0.8955896803529798}]\n",
      "[{'Learned': 0.5027758557507317, 'True': 0.9036879613869455}] [{'Learned': 0.8552725381013854, 'True': 0.9029546636866341}]\n"
     ]
    }
   ],
   "source": [
    "print(auc_train_trials, auc_test_trials)\n",
    "print(auc_train_trials_mod1, auc_test_trials_mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWzVZDDGmzjn"
   },
   "source": [
    "Analysis of optimization 1 - Shuffling and Curriculum Learning.\n",
    "\n",
    "The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
    "\n",
    "As we could observe from the result, although the AUC in training data is far less than the true AUC, the optimization method achieves similar result in test data. We don't make significant improvement after implementing shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfPFdbobburG"
   },
   "outputs": [],
   "source": [
    "# Implementation of Optimization 2 - Batch Normalization.\n",
    "\n",
    "def get_init_parameters_mod_v2():\n",
    "    beta = 0.1\n",
    "\n",
    "    # rssi weights\n",
    "    rssi_w_residual = np.random.rand(n_rssi_buckets) * 0.01\n",
    "    # modification: normalises input by subtracting the mean and dividing it by the standard deviation\n",
    "    rssi_w_residual = (rssi_w_residual - np.mean(rssi_w_residual)) / np.std(rssi_w_residual)\n",
    "\n",
    "    # rssi thresholds\n",
    "    rssi_th_residual = np.random.randint(low=10, high=40, size=n_rssi_th)\n",
    "\n",
    "    # infectiousness weights\n",
    "    infect_w_residual = np.random.rand(n_infect_levels) * 0.01\n",
    "    # modification: normalises input by subtracting the mean and dividing it by the standard deviation\n",
    "    infect_w_residual = (infect_w_residual - np.mean(infect_w_residual)) / np.std(infect_w_residual)\n",
    "\n",
    "    return np.concatenate([[beta], rssi_w_residual, rssi_th_residual, infect_w_residual])\n",
    "\n",
    "\n",
    "def train_mod_v2(features, bag_labels, assign_mat, sigmoid_temp_init, sigmoid_temp_target, batch_size, num_iters, lr):\n",
    "    num_samples = len(bag_labels)\n",
    "    batch_size = np.minimum(batch_size, num_samples)\n",
    "\n",
    "    model_params = get_init_parameters_mod_v2()\n",
    "    loss_start, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_init)\n",
    "    loss_start_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"Start\", \"sigmoid loss\", loss_start, \"step loss\", loss_start_step)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        l, r = i*batch_size%num_samples, (i+1)*batch_size%num_samples\n",
    "        if l <= r:\n",
    "            batch_y = bag_labels[l:r]\n",
    "            batch_assign_mat = assign_mat[l:r]\n",
    "        else:\n",
    "            batch_y = np.concatenate([bag_labels[l:], bag_labels[:r]])\n",
    "            batch_assign_mat = np.concatenate([assign_mat[l:], assign_mat[:r]], axis=0)\n",
    "\n",
    "        sigmoid_temp = sigmoid_temp_fn(sigmoid_temp_init, sigmoid_temp_target, i, num_iters)\n",
    "\n",
    "        loss, _ = loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp)\n",
    "        grad = grad_loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp, loss)\n",
    "\n",
    "        model_params -= lr * grad\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            loss_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "            print(\"iter\", i, \"sigmoid loss\", loss, \"step loss\", loss_step, \"sigmoid temp\", sigmoid_temp)\n",
    "\n",
    "    loss_final, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_target)\n",
    "    loss_final_step, probs_final = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"End\", \"sigmoid loss\", loss_final, \"step loss\", loss_final_step)\n",
    "    print_params(model_params)\n",
    "\n",
    "    return model_params, loss_final_step, probs_final\n",
    "\n",
    "\n",
    "def train_and_eval_with_bag_config_mod_v2(bag_sim: Bag_Simulator, X_epi, probabilities_true_epi, n_trials=1, n_random_restarts=5):\n",
    "    '''\n",
    "    second modification - Batch Normalization.\n",
    "    '''\n",
    "    auc_train_trials = []\n",
    "    auc_test_trials = []\n",
    "    for j in range(n_trials):\n",
    "        X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = \\\n",
    "            bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi, visualize=False)\n",
    "        best_loss = np.inf\n",
    "        best_model_params = None\n",
    "        best_final_probs = None\n",
    "        for i in range(n_random_restarts):\n",
    "            print('----------- Trial {}/{}: Training run {}/{} ----------------'.format(j+1, n_trials, i+1, n_random_restarts))\n",
    "            model_params, loss_st_step, final_probs = train_mod_v2(X, bag_labels_trn, assign_mat_trn,\n",
    "                                               sigmoid_temp_init=0.1, sigmoid_temp_target=1,\n",
    "                                               batch_size=200, num_iters=5000, lr=0.001)\n",
    "            if loss_st_step < best_loss:\n",
    "                best_loss = loss_st_step\n",
    "                best_model_params = model_params\n",
    "                best_final_probs = final_probs\n",
    "\n",
    "        print(\"best loss\", best_loss)\n",
    "        print(\"best scoring parameters\")\n",
    "        print_params(residual_to_scoring(best_model_params))\n",
    "\n",
    "        probs_bags_true_trn = 1 - np.exp(np.dot(assign_mat_trn, np.log(1-probabilities_true)))\n",
    "        auc_train_trials.append({\n",
    "            'Learned': auc(best_final_probs, bag_labels_trn),\n",
    "            'True': auc(probs_bags_true_trn, bag_labels_trn),\n",
    "        })\n",
    "\n",
    "        scores_learned = loss_fn_stepbins_ce(best_model_params, X, None, None, return_scores=True)\n",
    "        scores_learned_bags_tst = np.dot(assign_mat_tst, scores_learned)\n",
    "        probs_learned_bags_tst = 1 - np.exp(-1*scores_learned_bags_tst)\n",
    "        probs_bags_true_tst = 1 - np.exp(np.dot(assign_mat_tst, np.log(1-probabilities_true)))\n",
    "        auc_test_trials.append({\n",
    "            'Learned': auc(probs_learned_bags_tst, bag_labels_tst),\n",
    "            'True': auc(probs_bags_true_tst, bag_labels_tst),\n",
    "        })\n",
    "\n",
    "    return auc_train_trials, auc_test_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8QnXWjmcCk2",
    "outputId": "de05f9da-ad7e-4f34-93b4-37c6361bf139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5468, negatives: 28132\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4362, 4362) (4362, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3489, 4362) (873, 4362)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 2.0576858184833804 step loss 2.224771594043257\n",
      "iter 0 sigmoid loss 2.610820801456132 step loss 2.0280994003958766 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.585181008814583 step loss 0.6693544360189062 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.47957867281029437 step loss 0.6258575181357663 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.6236749701357784 step loss 0.6205437282138629 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.074574248161077 step loss 1.4310497748259916 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.5070326492003061 step loss 0.5888809538616244 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.5328514023324504 step loss 1.8742822798077217 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 2.129899361060242 step loss 1.8742822798077217 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.0147702064110398 step loss 1.8742822798077217 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 2.3601576703586473 step loss 1.8742822798077217 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8742822798077217 step loss 1.8742822798077217\n",
      "    beta -0.5623678566741731\n",
      "    rssi_w [-0.42769027  0.63430937  1.61618956 -0.79318106]\n",
      "    rssi_th [19.02500314 20.02450286 27.01693086]\n",
      "    infect_w [ 0.90693189 -0.90111566  0.7014179 ]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 2.4829643871711484 step loss 2.5368917380905316\n",
      "iter 0 sigmoid loss 3.1327683089723894 step loss 2.2527256285289865 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.5555271725318686 step loss 1.1485436655340089 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.4453909705115127 step loss 0.7452029626223868 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.5917840638121035 step loss 0.6340292273068193 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.6364060756860055 step loss 0.7421289857951209 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.4650025450865789 step loss 0.568683977217117 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.6340334296328577 step loss 0.5560497975408962 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.46603434198709975 step loss 0.5216466258236958 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.5405592992519042 step loss 0.4975847035456155 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.5522115015920683 step loss 0.49427274502654617 sigmoid temp 1.0\n",
      "End sigmoid loss 0.46855340073767543 step loss 0.47300561258150925\n",
      "    beta -0.013385554834088959\n",
      "    rssi_w [-1.2221151  -0.72692072  1.25054845 -0.43165211]\n",
      "    rssi_th [35.01440891 34.01357142 20.9965786 ]\n",
      "    infect_w [-0.99936992  1.32445744  0.12416327]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 2.0106128557687963 step loss 1.962986581975038\n",
      "iter 0 sigmoid loss 2.548620417037077 step loss 1.6856846906943277 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.4893322655108115 step loss 0.45725725896987224 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3771756651596943 step loss 0.444471115144588 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4966903431780011 step loss 0.43492468423272296 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.4012244993761624 step loss 0.42862686608649586 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.39972187300883105 step loss 0.428982673156327 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.5143167090043073 step loss 0.5506648720189085 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3673409718445901 step loss 0.4197440189623897 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.4496141298414909 step loss 0.40789374211165996 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4676322102100709 step loss 0.4039461139572588 sigmoid temp 1.0\n",
      "End sigmoid loss 0.396224980631607 step loss 0.40216821363707095\n",
      "    beta 0.021998757312913148\n",
      "    rssi_w [-1.54368523  0.56501408  1.03599983  0.46107105]\n",
      "    rssi_th [18.00728505 17.00778995 21.00894507]\n",
      "    infect_w [-0.55322778  1.6341344  -0.08281342]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.8742822798077217 step loss 1.8742822798077217\n",
      "iter 0 sigmoid loss 2.360157670358647 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 1.8996410517618374 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 1.4391244331650288 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 2.2450285157094445 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 1.8996410517618378 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 1.8420764744372367 step loss 1.8742822798077217 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 2.5328514023324504 step loss 1.8742822798077217 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 2.129899361060242 step loss 1.8742822798077217 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 2.0147702064110398 step loss 1.8742822798077217 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 2.3601576703586473 step loss 1.8742822798077217 sigmoid temp 1.0\n",
      "End sigmoid loss 1.8742822798077217 step loss 1.8742822798077217\n",
      "    beta 0.1\n",
      "    rssi_w [ 0.5720815  -1.64496397  0.09413032  0.97875215]\n",
      "    rssi_th [10. 23. 27.]\n",
      "    infect_w [ 0.69943048  0.7147554  -1.41418588]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.9785554399737868 step loss 1.9464938025210503\n",
      "iter 0 sigmoid loss 2.510710457132315 step loss 1.70607455926438 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.44077658315824664 step loss 0.4155803336641432 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3347406346811367 step loss 0.40990011574159985 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.4440817550010436 step loss 0.40274066985249457 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3619856077368651 step loss 0.3970030112478793 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.3382904849891028 step loss 0.39239590117132583 sigmoid temp 0.1\n",
      "iter 3000 sigmoid loss 0.4620923613688032 step loss 0.37223409219556036 sigmoid temp 0.325\n",
      "iter 3500 sigmoid loss 0.3208670158362542 step loss 0.368690018014641 sigmoid temp 0.55\n",
      "iter 4000 sigmoid loss 0.37165814713420375 step loss 0.3626144523012853 sigmoid temp 0.7749999999999999\n",
      "iter 4500 sigmoid loss 0.4245134174766849 step loss 0.3611950105922687 sigmoid temp 1.0\n",
      "End sigmoid loss 0.3547046180405894 step loss 0.36091263274430396\n",
      "    beta 0.18390467681751446\n",
      "    rssi_w [ 1.05992852 -1.57824209  0.55747086  0.32115715]\n",
      "    rssi_th [19.01044504 23.0100067  14.01074522]\n",
      "    infect_w [-1.16731969  1.25861033  0.27849697]\n",
      "best loss 0.36091263274430396\n",
      "best scoring parameters\n",
      "    beta 0.18390467681751446\n",
      "    rssi_w [ 1.05992852 -0.51831357  0.03915729  0.36031445]\n",
      "    rssi_th [-100.98955496  -77.97954826  -63.96880304]\n",
      "    infect_w [-1.16731969  0.09129064  0.36978761]\n"
     ]
    }
   ],
   "source": [
    "auc_train_trials_mod2, auc_test_trials_mod2 = train_and_eval_with_bag_config_mod_v2(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                                  censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                    n_trials=n_trials, n_random_restarts=n_random_restarts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YolhBdX-cXGz",
    "outputId": "916be0da-25e6-4ef6-9e1f-7232cf920a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Learned': 0.8708758456249307, 'True': 0.9058276395793452}] [{'Learned': 0.8575027456118378, 'True': 0.8938700603071232}]\n",
      "[{'Learned': 0.8492964376467638, 'True': 0.898958067129239}] [{'Learned': 0.8668089246835321, 'True': 0.9220149900772625}]\n"
     ]
    }
   ],
   "source": [
    "print(auc_train_trials, auc_test_trials)\n",
    "print(auc_train_trials_mod2, auc_test_trials_mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZQhdBS7nViv"
   },
   "source": [
    "Analysis of optimization 2 - Batch Normalization: Based on the training performance of our optimized method, we found that the loss doesn't decrease well as expected for some random trials. Also, the best loss is greater than that without batch normalization. This shows that batch normalization might not be useful in stochatic gradient descent method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEhWEe7cgZYg"
   },
   "outputs": [],
   "source": [
    "# # Implementation of Optimization 3 - Early Stopping\n",
    "# previously we only have training & testing data, need further splitting for validation data\n",
    "def check_validation_loss(validation_loss_history, num_early_stop):\n",
    "    if num_early_stop and len(validation_loss_history) > num_early_stop * 2:\n",
    "        recent_best = min(validation_loss_history[-num_early_stop:])\n",
    "        previous_best = min(validation_loss_history[:-num_early_stop])\n",
    "        if recent_best > previous_best:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def train_mod_v3(features, bag_labels, assign_mat, sigmoid_temp_init, sigmoid_temp_target, batch_size, num_iters, lr,\n",
    "                 bag_labels_val, assign_mat_val, patient_early_stopping):\n",
    "    num_samples = len(bag_labels)\n",
    "    batch_size = np.minimum(batch_size, num_samples)\n",
    "\n",
    "    model_params = get_init_parameters()\n",
    "    loss_start, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_init)\n",
    "    loss_start_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"Start\", \"sigmoid loss\", loss_start, \"step loss\", loss_start_step)\n",
    "\n",
    "    validation_loss_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        l, r = i*batch_size%num_samples, (i+1)*batch_size%num_samples\n",
    "        if l <= r:\n",
    "            batch_y = bag_labels[l:r]\n",
    "            batch_assign_mat = assign_mat[l:r]\n",
    "        else:\n",
    "            batch_y = np.concatenate([bag_labels[l:], bag_labels[:r]])\n",
    "            batch_assign_mat = np.concatenate([assign_mat[l:], assign_mat[:r]], axis=0)\n",
    "\n",
    "        sigmoid_temp = sigmoid_temp_fn(sigmoid_temp_init, sigmoid_temp_target, i, num_iters)\n",
    "\n",
    "        loss, _ = loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp)\n",
    "        grad = grad_loss_fn_ce(model_params, features, batch_y, batch_assign_mat, sigmoid_temp, loss)\n",
    "\n",
    "        model_params -= lr * grad\n",
    "\n",
    "        # add early stopping\n",
    "        loss_val, _ = loss_fn_ce(model_params, features, bag_labels_val, assign_mat_val, sigmoid_temp)\n",
    "        validation_loss_history.append(loss_val)\n",
    "\n",
    "        if check_validation_loss(validation_loss_history, patient_early_stopping):\n",
    "            print(\"Early Stopping in Iteration, \", i)\n",
    "            break\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            loss_step, _ = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "            print(\"iter\", i, \"sigmoid loss\", loss, \"step loss\", loss_step, \"sigmoid temp\", sigmoid_temp)\n",
    "\n",
    "    loss_final, _ = loss_fn_ce(model_params, features, bag_labels, assign_mat, sigmoid_temp_target)\n",
    "    loss_final_step, probs_final = loss_fn_stepbins_ce(model_params, features, bag_labels, assign_mat)\n",
    "    print(\"End\", \"sigmoid loss\", loss_final, \"step loss\", loss_final_step)\n",
    "    print_params(model_params)\n",
    "\n",
    "    return model_params, loss_final_step, probs_final\n",
    "\n",
    "\n",
    "def train_and_eval_with_bag_config_mod_v3(bag_sim: Bag_Simulator, X_epi, probabilities_true_epi, n_trials=1, n_random_restarts=5, patient_early_stopping=30):\n",
    "    '''\n",
    "    third modification - Early Stopping.\n",
    "    '''\n",
    "    auc_train_trials = []\n",
    "    auc_test_trials = []\n",
    "    for j in range(n_trials):\n",
    "        X, probabilities_true, assign_mat_trn, assign_mat_tst, bag_labels_trn, bag_labels_tst = \\\n",
    "            bag_sim.simulate_bagged_data(X_epi, Y_epi, probabilities_true_epi, visualize=False)\n",
    "        ## Split the validation data for early stopping evaluation\n",
    "        assign_mat_trn, assign_mat_val, bag_labels_trn, bag_labels_val = \\\n",
    "            train_test_split(assign_mat_trn, bag_labels_trn, train_size=0.8)\n",
    "        best_loss = np.inf\n",
    "        best_model_params = None\n",
    "        best_final_probs = None\n",
    "        for i in range(n_random_restarts):\n",
    "            print('----------- Trial {}/{}: Training run {}/{} ----------------'.format(j+1, n_trials, i+1, n_random_restarts))\n",
    "            model_params, loss_st_step, final_probs = train_mod_v3(X, bag_labels_trn, assign_mat_trn,\n",
    "                                               sigmoid_temp_init=0.1, sigmoid_temp_target=1,\n",
    "                                               batch_size=200, num_iters=5000, lr=0.001, assign_mat_val=assign_mat_val,\n",
    "                                               bag_labels_val=bag_labels_val, patient_early_stopping=patient_early_stopping)\n",
    "            if loss_st_step < best_loss:\n",
    "                best_loss = loss_st_step\n",
    "                best_model_params = model_params\n",
    "                best_final_probs = final_probs\n",
    "\n",
    "        print(\"best loss\", best_loss)\n",
    "        print(\"best scoring parameters\")\n",
    "        print_params(residual_to_scoring(best_model_params))\n",
    "\n",
    "        probs_bags_true_trn = 1 - np.exp(np.dot(assign_mat_trn, np.log(1-probabilities_true)))\n",
    "        auc_train_trials.append({\n",
    "            'Learned': auc(best_final_probs, bag_labels_trn),\n",
    "            'True': auc(probs_bags_true_trn, bag_labels_trn),\n",
    "        })\n",
    "\n",
    "        scores_learned = loss_fn_stepbins_ce(best_model_params, X, None, None, return_scores=True)\n",
    "        scores_learned_bags_tst = np.dot(assign_mat_tst, scores_learned)\n",
    "        probs_learned_bags_tst = 1 - np.exp(-1*scores_learned_bags_tst)\n",
    "        probs_bags_true_tst = 1 - np.exp(np.dot(assign_mat_tst, np.log(1-probabilities_true)))\n",
    "        auc_test_trials.append({\n",
    "            'Learned': auc(probs_learned_bags_tst, bag_labels_tst),\n",
    "            'True': auc(probs_bags_true_tst, bag_labels_tst),\n",
    "        })\n",
    "\n",
    "    return auc_train_trials, auc_test_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DmUdGGqnjsw",
    "outputId": "5a30a787-2be8-485e-9cce-9889ac59a036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 33600, positives: 5468, negatives: 28132\n",
      "Empirical Bag sizes:\n",
      "\t Positive bags: mean size 1.000, median size 1\n",
      "\t Negative bags: mean size 1.000, median size 1\n",
      "assign_mat size, X_shuff size: (4362, 4362) (4362, 3)\n",
      "assign_mat_trn size, assign_mat_tst size (3489, 4362) (873, 4362)\n",
      "Average positive samples per bag: 1.0\n",
      "----------- Trial 1/1: Training run 1/5 ----------------\n",
      "Start sigmoid loss 1.0720328100187655 step loss 1.0779339026915968\n",
      "iter 0 sigmoid loss 1.2420871342086806 step loss 0.8217326385528234 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3405446915232291 step loss 0.35552467003539606 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3747062675032058 step loss 0.3434560007590296 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36744567088141017 step loss 0.33300717598266677 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3123300817613699 step loss 0.32083193866737864 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2623574702836697 step loss 0.31499328352848865 sigmoid temp 0.1\n",
      "Early Stopping in Iteration,  2826\n",
      "End sigmoid loss 0.30281256362246667 step loss 0.31375909865081475\n",
      "    beta 0.31954446927097135\n",
      "    rssi_w [-0.06863074  0.00311495  0.14251487  0.26133037]\n",
      "    rssi_th [37.00149179 12.00138963 18.00026373]\n",
      "    infect_w [0.01568274 0.10734184 0.28510776]\n",
      "----------- Trial 1/1: Training run 2/5 ----------------\n",
      "Start sigmoid loss 1.3629694683181406 step loss 1.4387220184582845\n",
      "iter 0 sigmoid loss 1.573063902066644 step loss 0.7160343995122965 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3360930213589836 step loss 0.3692667593372829 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3716686374809008 step loss 0.36199502195967426 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36436528149483477 step loss 0.35987625189523215 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3275896944202663 step loss 0.3613010902057594 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.27467378919278035 step loss 0.3720045867645901 sigmoid temp 0.1\n",
      "Early Stopping in Iteration,  2588\n",
      "End sigmoid loss 0.35919049440447465 step loss 0.36820270226217\n",
      "    beta 0.2814950784106983\n",
      "    rssi_w [-0.01581254  0.09027835  0.2336265   0.10522025]\n",
      "    rssi_th [38.99962747 36.99874658 10.99880576]\n",
      "    infect_w [0.02330598 0.09264656 0.24716957]\n",
      "----------- Trial 1/1: Training run 3/5 ----------------\n",
      "Start sigmoid loss 1.2062706330640343 step loss 1.2020663634238067\n",
      "iter 0 sigmoid loss 1.3924612040712918 step loss 0.8041389397904805 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3418397643821111 step loss 0.3492138537432713 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.377264334743824 step loss 0.3354951305851122 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.3680721180686166 step loss 0.3257878977794353 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3284298549916748 step loss 0.3229556406232951 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.27238727098296783 step loss 0.5441717024991753 sigmoid temp 0.1\n",
      "Early Stopping in Iteration,  2784\n",
      "End sigmoid loss 0.47255945538278143 step loss 0.5451207765267403\n",
      "    beta 0.30743119999444557\n",
      "    rssi_w [-0.03667396  0.01352073  0.28143558  0.06825052]\n",
      "    rssi_th [28.00181649 30.00168424 36.99972077]\n",
      "    infect_w [0.01700541 0.10693578 0.27226173]\n",
      "----------- Trial 1/1: Training run 4/5 ----------------\n",
      "Start sigmoid loss 1.1987813934708635 step loss 1.2008472384560758\n",
      "iter 0 sigmoid loss 1.392568152273663 step loss 0.782357827034807 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.3434168717819195 step loss 0.36953015725709654 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.38147955258237914 step loss 0.363540179863 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36873111136408726 step loss 0.3613075130407875 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.336574490685103 step loss 0.3594748611263109 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2816990688285593 step loss 0.35826733388141324 sigmoid temp 0.1\n",
      "Early Stopping in Iteration,  2727\n",
      "End sigmoid loss 0.351964261905139 step loss 0.35831071075348847\n",
      "    beta 0.28977674154374566\n",
      "    rssi_w [-0.0366212  -0.01887151  0.16452663  0.21554626]\n",
      "    rssi_th [19.00075415 28.00075908 29.99902396]\n",
      "    infect_w [0.02473776 0.10142327 0.2531886 ]\n",
      "----------- Trial 1/1: Training run 5/5 ----------------\n",
      "Start sigmoid loss 1.212299884281231 step loss 1.214254490360228\n",
      "iter 0 sigmoid loss 1.415410057940195 step loss 0.7787554910983174 sigmoid temp 0.1\n",
      "iter 500 sigmoid loss 0.34313478430573136 step loss 0.3564871172527192 sigmoid temp 0.1\n",
      "iter 1000 sigmoid loss 0.3796552443286441 step loss 0.3444288498825448 sigmoid temp 0.1\n",
      "iter 1500 sigmoid loss 0.36829851998973395 step loss 0.3350212464435135 sigmoid temp 0.1\n",
      "iter 2000 sigmoid loss 0.3272691370052284 step loss 0.32519908914318557 sigmoid temp 0.1\n",
      "iter 2500 sigmoid loss 0.2673572972598275 step loss 0.34010995163915964 sigmoid temp 0.1\n",
      "Early Stopping in Iteration,  2826\n",
      "End sigmoid loss 0.30730715959937455 step loss 0.32137454983371894\n",
      "    beta 0.31875908529089825\n",
      "    rssi_w [-0.04539322 -0.01999173  0.10096825  0.28298262]\n",
      "    rssi_th [22.00064079 20.00065298 24.9998323 ]\n",
      "    infect_w [0.02195661 0.11924217 0.27954828]\n",
      "best loss 0.31375909865081475\n",
      "best scoring parameters\n",
      "    beta 0.31954446927097135\n",
      "    rssi_w [-0.06863074 -0.06551579  0.07699908  0.33832945]\n",
      "    rssi_th [-82.99850821 -70.99711858 -52.99685485]\n",
      "    infect_w [0.01568274 0.12302458 0.40813234]\n"
     ]
    }
   ],
   "source": [
    "auc_train_trials_mod3, auc_test_trials_mod3 = train_and_eval_with_bag_config_mod_v3(Bag_Simulator(p_pos=0.7, r_pos=2, p_neg=0.7, r_neg=2, max_bag_size=1,\n",
    "                                                                                                  censor_prob_pos=0, censor_prob_neg=0), X_epi, probabilities_true_epi,\n",
    "                                                                                    n_trials=n_trials, n_random_restarts=n_random_restarts_train, patient_early_stopping=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZSEvVpenorD",
    "outputId": "48621564-ee11-4595-b0c7-914f87dfcd62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Learned': 0.8707116027214296, 'True': 0.905479866532299}] [{'Learned': 0.8590778597714879, 'True': 0.8955896803529798}]\n",
      "[{'Learned': 0.854169753863936, 'True': 0.9041179815657276}] [{'Learned': 0.8653783164100884, 'True': 0.9093755418970734}]\n"
     ]
    }
   ],
   "source": [
    "print(auc_train_trials, auc_test_trials)\n",
    "print(auc_train_trials_mod3, auc_test_trials_mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvw6eh_Zn2fS"
   },
   "source": [
    "Analysis of optimization 3 - Early Stopping. \n",
    "\n",
    "With early stopping, we can partially solve the problem of slow running, without sacrificing too much accuracy. Also, compared the AUC and loss between test dataset, we achieve better result and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4ujStJACfRa"
   },
   "source": [
    "## 3. Broader Impact\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "+ **The Tracing App**\n",
    "\n",
    "The world has been fighting COVID-19 for two years. From a perspective, we have witnessed how technological advances could aid in preventing the spread of the virus. The covid tracing app is an innovative example that benefits lots of people. Each terminal user who agrees to share information with the platform can get timely feedback from the app about the risk of infection. For larger parties like medical providers who need to monitor close contacts or relevant public agencies that need to enact policies, the existence of a tracing app saves time from endless searching.\n",
    "\n",
    "\n",
    "It is necessary to obtain great accuracy for risk score modeling since it will provide more helpful information and saves time. The inacurrate contact tracing, on the contrary, will falsely occupy the medical resources greatly. As we mentioned before, one of the main contributions from original authors is to build off the model from real-world, bio-medically sound laws. With more realistic constraints considered, the original authors' parameters are more helpful in improving the tracing content. As for us, we manage to replicate their approach for optimizing the risk score model parameters and experiment with more optimization methods on top of their data management and fundamental SGD algorithm. We hope that we have made positive impacts throughout the process and contributed to smoother future iterations of this application.\n",
    "\n",
    "+ **The Inspiration for ML Community**\n",
    "\n",
    "The successful application for using machine learning technologies on covid-19 risk score modeling could give confidence to the ML community and encourage engineers/scientists to explore more. Essentially, we provide several possible modifications and enhancements that can be applied to the original learning algorithm. It shows that others are incentivized to improve the performance based on a good starting point (the original work done by paper authors). Combining with the background of the study, we incentivize more ML engineers to contribute to real-world problems.\n",
    "\n",
    "Furthermore, the work could have broadened the communityâ€™s perspective on what ML can achieve. Now it is only an example for making an accurate covid tracing app. Still, in the future, the procedure can be used for other infectious diseases or be prepared for future public health events. Other fields are possible, too, since we have shown the capability of ML to model some real-life scenarios with actual physical law captured. All in all, we can propose a bright future for ML applications, but there are caveats that we have to be careful about as well.  \n",
    "\n",
    "\n",
    "+ **Privacy Issue in Data Usage**\n",
    "\n",
    "The trade-off between data privacy and prediction accuracy has bothered data scientists for a long time. In the original work and our replicated version, we use simulated data for exposure events and interactions between human users. However, companies in the real world will finally adopt the tracing app and similar applications, and they will feed in real data. It could eventually become a privacy issue, and such an issue has already shown its lousy influence in some countries. In China, the government strictly controls the covid cases, and the media will keep publishing the case information over the internet (Weibo, Weixin official account, etc.). Several internet violence events have happened: a young lady from Chengdu has been reported as PCR test positive, and then not only her itineraries, but also her name, ID number, family background were extensively searched for and reported on the internet. The impact is detrimental for her and further warns us of the problem of using real data in public health scenarios. \n",
    " \n",
    "It seems that maybe the higher-level administrators must come up with a legislative solution to improve the situation. Meanwhile, for our end, as engineers and data scientists, we recommend adding privacy filters onto the dataset manually. We could incorporate methods for the de-identification process, such as suppressing, adding synthetic records, or applying a differential privacy algorithm. \n",
    " \n",
    "However, when manipulating de-identification on the dataset, we potentially result in some level of inaccuracy. One aspect is misrepresentation associated with a biased and distorted dataset. Like in crime analysis, misrepresentation and underrepresentation cause a big problem of prioritizing some races. The other aspect is information loss. In this specific scenario of the covid tracing app, using too much de-identification processing harms the integrity of information, which is one of the bedrocks that we want to base on improving the algorithm. Moreover, we are alternatively creating more censorship, potentially increasing the learning complexity for machines. \n",
    " \n",
    "Although we have a good intention of protecting privacy while using public data by some means, we shall think of the consequences. Our goal of contributing to the world needs to be reassessed with the abovementioned ethical issues in a delicate balance. In conclusion, albeit no more action we could take during this step of the project, we urge researchers to be aware of the tradeoff of data privacy and data accuracy and incorporate methods to mitigate the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJn1b4Xzxq4r"
   },
   "source": [
    "# References\n",
    "\n",
    "[Return to top](#Notebook-contents)\n",
    "\n",
    "[1] Google-Apple. Exposure notifications: Using technology to help public health authorities fight covid-19, 2020. URL https://www.google.com/covid19/exposurenotifications/.\n",
    "\n",
    "[2] LFPH. Configuring Exposure Notification Risk Scores for COVID-19 (Linux Foundation Public Health). URL https://github.com/lfph/gaen-risk-scoring/blob/main/risk-scoring.md.\n",
    "\n",
    "[3] Luca Ferretti, Chris Wymant, Michelle Kendall, Lele Zhao, Anel Nurtay, Lucie Abeler-D Ìˆorner, Michael Parker, David Bonsall, and Christophe Fraser. Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing. Science, 368(6491), May 2020b. URL http://dx.doi.org/10.1126/science.abb6936.\n",
    "\n",
    "[4] Matthew Abueg, Robert Hinch, Neo Wu, Luyang Liu, William J M Probert, Austin Wu, Paul Eastham, Yusef Shafi, Matt Rosencrantz, Michael Dikovsky, Zhao Cheng, Anel Nur- tay, Lucie Abeler-D Ìˆorner, David G Bonsall, Michael V McConnell, Shawn Oâ€™Banion, and Christophe Fraser. Modeling the combined effect of digital exposure notification and non- pharmaceutical interventions on the COVID-19 epidemic in washington state. NPJ Digital Medicine, (medrxiv;2020.08.29.20184135v1), 2021. URL https://www.medrxiv.org/content/ 10.1101/2020.08.29.20184135v1.abstract.\n",
    "\n",
    "[5] Marcel Salathe, Christian Althaus, Nanina Anderegg, Daniele Antonioli, Tala Ballouz, Edouard Bugnon, Srdjan CË‡apkun, Dennis Jackson, Sang-Il Kim, Jim Larus, Nicola Low, Wouter Lueks, Dominik Menges, C Ìedric Moullet, Mathias Payer, Julien Riou, Theresa Stadler, Carmela Troncoso, Effy Vayena, and Viktor von Wyl. Early evidence of effectiveness of digital contact tracing for SARS-CoV-2 in switzerland. Swiss Med. Wkly, 150:w20457, December 2020. URL http: //dx.doi.org/10.4414/smw.2020.20457.\n",
    "\n",
    "[6] Felix Sattler, Jackie Ma, Patrick Wagner, David Neumann, Markus Wenzel, Ralf Sch Ìˆafer, Wojciech Samek, Klaus-Robert Mu Ìˆller, and Thomas Wiegand. Risk estimation of SARS-CoV-2 transmission from bluetooth low energy measurements. npj Digital Medicine, 3(1):129, October 2020. URL https://doi.org/10.1038/s41746-020-00340-0.\n",
    "\n",
    "[7] Mark Briers, Marcos Charalambides, and Chris Holmes. Risk scoring calculation for the current NHSx contact tracing app. May 2020. URL http://arxiv.org/abs/2005.11057.\n",
    "\n",
    "[8] Luca Ferretti, Alice Ledda, Chris Wymant, Lele Zhao, Virginia Ledda, Lucie Abeler-Dorner, Michelle Kendall, Anel Nurtay, Hao-Yuan Cheng, Ta-Chou Ng, Hsien-Ho Lin, Rob Hinch, Joanna Masel, A Marm Kilpatrick, and Christophe Fraser. The timing of COVID-19 trans- mission. September 2020a. URL https://www.medrxiv.org/content/10.1101/2020.09.04. 20188516v1.abstract.\n",
    "\n",
    "[9] Timo Smieszek. A mechanistic model of infection: why duration and intensity of contacts should be included in models of disease spread. Theor. Biol. Med. Model., 6:25, November 2009. URL http://dx.doi.org/10.1186/1742-4682-6-25.\n",
    "\n",
    "[10] Charles N Haas. Quantitative Microbial Risk Assessment (2nd edn). Wiley, July 2014. URL https://openlibrary.org/books/OL27557844M/Quantitative_Microbial_Risk_Assessment.\n",
    "\n",
    "[11] Yann A. LeCun, LÃ©on Bottou, Genevieve B. Orr and Klaus-Robert MÃ¼lle. Efficient BackProp. Neural Networks: Tricks of the Trade, 1524: 9â€“50, 1998. \n",
    "\n",
    "[12] Sergey Ioffe and Christian Szegedy. Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. March 2015. URL https://arxiv.org/abs/1502.03167\n",
    "\n",
    "[13] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marcâ€™Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang and Andrew Y. Ng. Large Scale Distributed Deep Networks. Neural Information Processing Systems, 1â€“11, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#Notebook-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_Implementation_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
