{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1 (Due 09/16/2021, 11:59pm)\n",
    "## Review of Stastistical Modeling and Scientific Computing\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Jiahui Tang**\n",
    "\n",
    "**Students collaborators: Yujie Cai**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus1.png) |  ![alt](toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus2.png) |  ![alt](toysmith2.png)\n",
    "\n",
    "\n",
    "In the following, we will ask you to build statistical models to compare these two products using the observed rating. Larger versions of the images are available in the data set accompanying this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: A Maximum Likelihood Model\n",
    "1. **(Model Building)** Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "  where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars. That is, each new rating (a value between 1 and 5) has a categorical distribution $Cat(\\theta)$. Represent the observed ratings of an Amazon product as a vector $R = [r_1, r_2, r_3, r_4, r_5]$ where, for example, $r_4$ is the number of $4$-star reviews out of a total of $N$ ratings. Write down the likelihood of $R$. That is, what is $p(R| \\theta)$?\n",
    "\n",
    "  **Note:** The observed ratings for each product should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "For given fixed categorical distribution $Cat(\\theta)$ for possibility of the value each new ratings is, \n",
    "and N independent ratings each of which leads to exactly one of the 5 ratings, \n",
    "the distribution of $R = [r_1, r_2, r_3, r_4, r_5]$ follows a multinomial distribution.\n",
    "\n",
    "Thus, the likelihood of $R$ follows multinomial distribution PDF, and would look like:\n",
    "\n",
    "$$\\mathcal{L} = p(R| \\theta) = \\frac{N!}{r_1!\\ldots r_5!} \\theta_1^{r_1}\\ldots \\theta_5^{r_5}$$\n",
    "\n",
    "where \n",
    "$$\\sum_{i=1}^{5} r_{i} = N $$\n",
    "subject to\n",
    "$$\\sum_{i=1}^{5} \\theta_{i} = 1 $$\n",
    "    \n",
    "With the observed ratings for each product that we could read off the image files included in the dataset,\n",
    "\n",
    "1. For Lotus World, $R = [10, 6, 10, 28, 109]$ when round up. The likelihood could be written as:\n",
    "   $$\n",
    "\\mathcal{L}_{\\text{lotus}} = \\frac{N!}{10!*6!*10!*28!*109!} \\theta_1^{10}*\\theta_2^{6}*\\theta_3^{10}*\\theta_4^{28}*\\theta_5^{109}\n",
    "$$ \n",
    "    \n",
    "    \n",
    "2. For Toysmith,  $R = [57, 33, 29, 45, 246]$ when round up. Thus, the likelihood could be written as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{toysmith}}= \\frac{N!}{57!*33!*29!*45!*246!} \\theta_1^{57}*\\theta_2^{33}*\\theta_3^{29}*\\theta_4^{45}*\\theta_5^{246}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Model Fitting)** Find the maximum likelihood estimator of $\\theta$ for the Lotus World model; find the MLE of $\\theta$ for the Toysmith model. You need to make a reasonably mathematical argument for why your estimate actually maximizes the likelihood (i.e. recall the criteria for a point to be a global optima of a function).\n",
    "\n",
    "  *Note:* I recommend deriving the MLE using the general expression of the likelihood. That is, derive the posterior using the variable $R$, then afterwards plug in your specific values of $R$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "    \n",
    "First, we take log-likehood for our likehood function as ***maximizing the likelihood is equivalent to maximizing the log likelihood***.\n",
    "    \n",
    "Thus, given the likelihood function, we first derive the log likelihood function as below:\n",
    "    \n",
    "$$\\log \\mathcal{L} = \\log p(R| \\theta) = \\log [\\frac{N!}{r_1!\\ldots r_5!} \\theta_1^{r_1}\\ldots \\theta_5^{r_5}]$$\n",
    "$$ = \\log(\\frac{N!}{r_1!\\ldots r_5!}) + \\sum_{i=1}^{5}r_i *\\log(\\theta_i)$$\n",
    "    \n",
    "To obtain maximum likelihood estimator of $\\theta$ for any model, we want to find:\n",
    "    \n",
    "$$\\underset{\\theta}{\\mathrm{argmax}}\\log\\mathcal{L} = \\underset{\\theta}{\\mathrm{argmax}} \\log(\\frac{N!}{r_1!\\ldots r_5!}) + \\sum_{i=1}^{5}r_i *\\log(\\theta_i)  = \\underset{\\theta}{\\mathrm{argmax}} \\sum_{i=1}^{5}r_i *\\log(\\theta_i) $$\n",
    " \n",
    " \n",
    "where \n",
    "$$\\sum_{i=1}^{5} r_{i} = N $$\n",
    "subject to\n",
    "$$\\sum_{i=1}^{5} \\theta_{i} = 1 $$\n",
    "    \n",
    "Since it is a constrainted optimization problem, we introduce $\\lambda$ as our Lagrange multiplier, and derive augmented objective $J$ as Lagrangian of the constrained optimization problem, where\n",
    "$$ J = f(x) - \\lambda g(x) = \\sum_{i=1}^{5}r_i *\\log(\\theta_i) - \\lambda * (\\sum_{i=1}^{5} \\theta_{i} - 1) $$\n",
    "    \n",
    "Next, we take gradient of Lagrangian $J$ with respect to $(\\theta, \\lambda)$, we get vector \n",
    "    \n",
    "$$\n",
    "\\nabla J = \\left[\\frac{\\partial J}{\\partial \\theta_1}, \\frac{\\partial J}{\\partial \\theta_2}, \\ldots \\frac{\\partial J}{\\partial \\theta_5}, \\frac{\\partial J}{\\partial \\lambda}\\right]\n",
    "$$\n",
    "    \n",
    "Thus, we have \n",
    "    \n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{r_i}{\\theta_i} - \\lambda = 0   &\\Rightarrow \\theta_i = \\frac{r_i}{\\lambda}&\\\\\n",
    "1-\\sum_{i=1}^{5} \\theta_{i} = 0 &\\Rightarrow 1 = \\sum_{i=1}^{5} \\frac{r_i}{\\lambda} &\\Rightarrow \\lambda = \\sum_{i=1}^{5} r_i \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "Thus, we get the solution as below\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\lambda = \\sum_{i=1}^{5} r_i \\\\\n",
    "\\\\\n",
    "\\theta_i = \\frac{r_i}{\\sum_{i=1}^{5} r_i}&\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "The $\\theta_{MLE}$ is thus $$\\theta_{MLE} = [\\frac{r_1}{\\sum_{i=1}^{5} r_i}, \\frac{r_2}{\\sum_{i=1}^{5} r_i}, \\ldots ,\\frac{r_5}{\\sum_{i=1}^{5} r_i}]$$\n",
    "    \n",
    "In order to check that this MLE estimate actually maximizes the likelihood (i.e. for a point to be a global optima of a function), we need to check under this constraint optimization problem, \n",
    "1. The constraint $\\sum_{i=1}^{5} \\theta_{i} = 1 $ is affine.\n",
    "2. log-likelihood $\\mathcal{L} = \\sum_{i=1}^{5}r_i *\\log(\\theta_i) $ is concave, as its second derivative w.r.t $\\theta$ is $-r_i * \\theta^{-2}$, and is negative with nonnegative $r_i$ and $\\theta_i$. Thus $\\mathcal{L}$ is a concave function.\n",
    "    \n",
    "So that we know $\\mathcal{L}$ is maximized on the line at the stationary point.\n",
    "    \n",
    "Thus, plugging in specific value for $R$ for each of the product, we get:\n",
    "    \n",
    "1. Lotus World, $R = [10, 6, 10, 28, 109]$ \n",
    "    $$\\theta_{MLE} = [\\frac{r_1}{\\sum_{i=1}^{5} r_i}, \\frac{r_2}{\\sum_{i=1}^{5} r_i}, \\ldots ,\\frac{r_5}{\\sum_{i=1}^{5} r_i}]$$\n",
    "    \n",
    "    $$ = [\\frac{10}{163}, \\frac{6}{163}, \\frac{10}{163}, \\frac{28}{163}, \\frac{109}{163}] = [0.061, 0.037, 0.061, 0.172, 0.669]$$\n",
    "    \n",
    "    \n",
    "2. Toysmith, $R = [57, 33, 29, 45, 246]$\n",
    "    $$\\theta_{MLE} = [\\frac{r_1}{\\sum_{i=1}^{5} r_i}, \\frac{r_2}{\\sum_{i=1}^{5} r_i}, \\ldots ,\\frac{r_5}{\\sum_{i=1}^{5} r_i}]$$\n",
    "    \n",
    "    $$ = [\\frac{57}{410}, \\frac{33}{410}, \\frac{29}{410}, \\frac{45}{410}, \\frac{246}{410}] = [0.139, 0.080, 0.071, 0.110, 0.6]$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Model Interpretation)** Based on your MLE of $\\theta$'s for both models, do you feel confident deciding if one product is superior to another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:** \n",
    "I don't feel very confident to decide if one movie is superior to another.\n",
    "    \n",
    "Lotus World has only 162 comments, while Toysmith has 410 comments. It seems at beginning Lotus World is superior to Toysmith as it has higher MLE of $\\theta$ of its rating to fall in 4 or 5 stars, while Toysmith has less MLE of $\\theta$ with 4 or 5 stars, and plenty more 1 star ratings likelihood. However, given the comparable size of observed data, Toysmith is twice as large as the size of Lotus World. It may be the case that current observed data on Lotus World's high ratings could be outliers, and they also have not accumulated enough low ratings to have a fair reflection of their rating distribution comparing to Toysmith. I would say it is indecisive to conclude which product is better than the other, from the MLE of $\\theta$.\n",
    "    \n",
    "The model also make implicit assumption that observed ratings are independent from each other, and thus $R$ follows a multinomial distribution. It assumes there's no correlation between each ratings, meaning it can't be more likely for one people to rate 5 stars if we've already find one person rate it 5 stars. This could not be the real case, thus based on this reason, I would also argue it's confident to conclude if one is better than the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part II: A Bayesian Model\n",
    "\n",
    "1. **(Model Building)** Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). What would be an appropriate $\\alpha$ for the Dirichlet prior on $\\theta$? Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "p_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "Dirichelet distribution, also known as multivariate beta distribution, suppose we denote the prior by $Dir(\\alpha)$\n",
    "    \n",
    "The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other, so $\\alpha$ are uniformly distributed, e.g. $Dir(\\alpha) = [1,1,1,1,1]$; indicating reviews at star 1 to 5 are all equally likely to occur in rubber chickens.\n",
    "\n",
    "The single value $\\alpha$ to which all parameters are set is called the **concentration parameter**. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how \"concentrated\" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much greater than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much less than 1, the mass will be dispersed almost equally among all the components.\n",
    "\n",
    "If we hold prior belief that most reviews will be 1 stars or 5 stars, with little middle ground, then there should be heavy emphasis on **concentration parameter** in $\\alpha_1$ and $\\alpha_5$.\n",
    "    \n",
    "Thus, an appropriate $\\alpha$ for the Dirichlet prior to $\\theta$ could be:\n",
    "\n",
    "$$Dir(\\alpha) = [1000, 1, 1, 1, 1000]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Inference)** Analytically derive the posterior distribution (using the likelihoods you derived in Part I) for each product.\n",
    "\n",
    "  *Note:* I recommend deriving the posterior using the general expression of a Dirichelet pdf. That is, derive the posterior using the variable $\\alpha$, then afterwards plug in your specific values of $\\alpha$ when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "Based on likelihoods we derived in Part I for $p(R|\\theta)$, the posterior distribution could be written as:\n",
    "    \n",
    "$$\n",
    "p(\\theta|R) = \\frac{p(R|\\theta) p(\\theta)}{p(R)} = \\frac{\\frac{N!}{r_1!\\ldots r_5!} \\theta_1^{r_1}\\ldots \\theta_5^{r_5} * \\frac{1}{B(\\alpha)} \\prod_{i=1}^5 \\theta_i^{\\alpha_i - 1}}{p(R)}\n",
    "$$\n",
    "\n",
    "As Dirichelet distribution is a conjugate prior, we try to simplify the formula and re-arrange it into a posterior with the form of another Dirichelet distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|R) = \\frac{p(R|\\theta)p(\\theta)}{p(R)} =\\frac{N!}{r_1!\\ldots r_5!} * \\frac{1}{{p(R)} * B(\\alpha)} * \\prod_{i=1}^5 \\theta_i^{\\alpha_i + r_i - 1} = const * \\prod_{i=1}^5 \\theta_i^{\\alpha_i + r_i - 1}\n",
    "$$\n",
    "    \n",
    "It seems to update our posterior distribution to another Dirichelet distribution with \n",
    "    \n",
    "$$Dir(\\alpha) = [\\alpha_1 + r_1, \\alpha_2 + r_2, \\alpha_3 + r_3, \\alpha_4 + r_4, \\alpha_5 + r_5]$$\n",
    "    \n",
    "Thus, the updated posterior of each product is\n",
    "1. Lotus World:\n",
    "$$Dir(\\alpha) = [1000+10, 1+6, 1+10, 1+28, 1000+109] = [1010, 7, 11, 29, 1109]$$\n",
    "2. Toysmith:\n",
    "$$Dir(\\alpha) = [1000+57, 1+33, 1+29, 1+45, 1000+246] = [1057, 34, 30, 46, 1246]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(The Maximum A Posterior Estimate)** Analytically or empirically compute the MAP estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MLE? Just for this problem, compute the MAP estimate of $\\theta$ for each product using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$. Make a conjecture about the effect of the prior on the difference between the MAP estimates and the MLE's of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "**Part I.**\n",
    " \n",
    "MAP estimate is a point estimate, which is the posterior mode, or maximum a posterior (MAP).\n",
    "    \n",
    "The mode of Dirichelet posterior is calculated as \n",
    "$$\n",
    "\\theta_i = x_i = \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5}, \\alpha_i > 1\n",
    "$$\n",
    "Thus, the MAP estimate is \n",
    "$$\\theta_{MAP} =  \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5}$$ \n",
    "    on posterior $\\alpha$ and $\\alpha_i > 1$\n",
    "    \n",
    "1. For Lotus World, posterior $\\alpha = [1010, 7, 11, 29, 1109]$\n",
    "    \n",
    "    $$\\theta_{MAP} =  \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5} = [\\frac{1009}{2161}, \\frac{6}{2161}, \\frac{10}{2161},\\frac{28}{2161},\\frac{1108}{2161}] = [0.467, 0.003, 0.005, 0.013, 0.513]$$ \n",
    "    \n",
    "\n",
    "2. For Toysmith, posterior $\\alpha = [1057, 34, 30, 46, 1246]$\n",
    "    \n",
    "      $$\\theta_{MAP} =  \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5} = [\\frac{1056}{2408}, \\frac{33}{2408}, \\frac{29}{2408},\\frac{45}{2408},\\frac{1245}{2408}] = [0.439, 0.014, 0.012, 0.019, 0.517]$$ \n",
    "    \n",
    "\n",
    "From Question 1, we have the below MLE estimators:\n",
    "1. Lotus World, $R = [10, 6, 10, 28, 109]$ \n",
    "    $$\\theta_{MLE} = [0.061, 0.037, 0.061, 0.172, 0.669]$$\n",
    "    \n",
    "    \n",
    "2. Toysmith, $R = [57, 33, 29, 45, 246]$\n",
    "    $$\\theta_{MLE} = [0.139, 0.080, 0.071, 0.110, 0.6]$$\n",
    "    \n",
    "These MAP estimator, comparing to MLE, gives way more weightage on 1 star rating and 5 star ratings for both retail stores, and is quite close to its prior belief. It make the estimation of $\\theta_5$ slightly lower but $\\theta_1$ a lot more higher than MLE estimation. It also makes the estimation of $\\theta_{2..4}$ a lot lower than MLE estimation.\n",
    "    \n",
    "In the prior, we assume most reviews from customers will be extreme ratings with little middle ground. Thus, the result given by MAP makes sense. However, the distribution then will look less similar to what we observe in real world (mostly five/four star ratings). Besides, there is no obvious superior store if we compare between two MAP estimators, both of them have $\\theta_5 \\approx 0.5$ and $\\theta_1 \\approx 0.4$;\n",
    "while based on MLE estimators, we still could see that Lotus world has higher $\\theta$ estimations for 4 and 5 star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "   \n",
    "**Part II.**\n",
    "    \n",
    "using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$\n",
    "    \n",
    "1. For Lotus World, updated posterior $\\alpha = [11, 7, 11, 29, 110]$\n",
    "    \n",
    "    $$\\theta_{MAP} =  \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5} = [\\frac{10}{163}, \\frac{6}{163}, \\frac{10}{163},\\frac{28}{163},\\frac{109}{163}] = [ 0.061, 0.037,  0.061, 0.172, 0.669]$$ \n",
    "\n",
    "\n",
    "2. For Toysmith, updated posterior $\\alpha = [58, 34, 30, 46, 247]$\n",
    "    \n",
    "      $$\\theta_{MAP} =  \\frac{\\alpha_i - 1}{\\sum_{i=1}^5 \\alpha_i - 5} = [\\frac{57}{410}, \\frac{33}{410}, \\frac{29}{410},\\frac{45}{410},\\frac{246}{410}] = [0.139, 0.080, 0.071, 0.110, 0.6]$$ \n",
    "\n",
    "By using a different set of prior $\\alpha = [1, 1, 1, 1, 1]$, assume no previous information about ratings, we could see the MAP estimators are quite different from the one we obtained above using $\\alpha = [1000, 1, 1, 1, 1000]$.\n",
    "\n",
    "The new MAP estimator is now converged and equal to MLE estimator. \n",
    "    \n",
    "We can see that MAP estimator is a point estimate that depends a lot on our prior belief and assumption. If we assume a uniform prior with no previous information, then the MAP estimator will be similar to MLE estimator.\n",
    "Being affected heavily by prior, we can see that point estimates can be misleading. And it may not always be appropriate to summarize the entire posterior using a point estimate. The computation of MAP posterior estimator is thus a process of choosing the right prior.\n",
    "\n",
    "However, for MLE estimator, it is an optimization problem, to find local optima and global optima. The change of prior won't affect our MLE estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lotus MAP estimate is [4.15427741e-01 3.07305167e-04 5.08174668e-04 5.59027060e-03\n",
      " 4.66062767e-01]\n",
      "toysmith MAP estimate is [0.39556811 0.00565786 0.00508172 0.00947454 0.47222228]\n"
     ]
    }
   ],
   "source": [
    "# emprically\n",
    "from scipy import stats\n",
    "\n",
    "samples = 100000\n",
    "#lotus posterior samples\n",
    "lotus = np.random.dirichlet([1010, 7, 11, 29, 1109], size=samples)\n",
    "#toysmith posterior samples\n",
    "toysmith = np.random.dirichlet([1057, 34, 30, 46, 1246], size=samples)\n",
    "\n",
    "# MAP\n",
    "lotus_map = stats.mode(lotus).mode[0]\n",
    "toysmith_map = stats.mode(toysmith).mode[0]\n",
    "\n",
    "print(\"lotus MAP estimate is\", lotus_map)\n",
    "print(\"toysmith MAP estimate is\", toysmith_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(The Posterior Mean Estimate)** Analytically or empirically compute the posterior mean estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MAP estimates and the MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "Posterior mean estimate is another point estimate, which is the posterior mean, or expectation.\n",
    "    \n",
    "The mean of Dirichelet posterior is calculated as \n",
    "$$\n",
    "\\theta_i = x_i = \\frac{\\alpha_i}{\\sum_{i=1}^5 \\alpha_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, the posterior mean estimate of $\\theta$ is \n",
    "$$\\theta_{postmean} =  \\frac{\\alpha_i}{\\sum_{i=1}^5 \\alpha_i}$$ \n",
    "    on posterior $\\alpha$\n",
    "    \n",
    "1. For Lotus World, posterior $\\alpha = [1010, 7, 11, 29, 1109]$\n",
    "    \n",
    "    $$\\theta_{post_mean} =  \\frac{\\alpha_i}{\\sum_{i=1}^5 \\alpha_i} = [\\frac{1010}{2166}, \\frac{7}{2166}, \\frac{11}{2166},\\frac{29}{2166},\\frac{1109}{2166}] = [0.466, 0.003, 0.005, 0.013, 0.512]$$ \n",
    "    \n",
    "\n",
    "2. For Toysmith, posterior $\\alpha = [1057, 34, 30, 46, 1246]$\n",
    "    \n",
    "     $$\\theta_{post_mean} =  \\frac{\\alpha_i}{\\sum_{i=1}^5 \\alpha_i} = [\\frac{1057}{2413}, \\frac{34}{2413}, \\frac{30}{2413},\\frac{46}{2413},\\frac{1246}{2413}] = [0.438, 0.014, 0.012, 0.019, 0.516]$$ \n",
    "    \n",
    "\n",
    "From Question 1, we have the below MLE estimators:\n",
    "1. Lotus World, $R = [10, 6, 10, 28, 109]$ \n",
    "    $$\\theta_{MLE} = [0.061, 0.037, 0.061, 0.172, 0.669]$$\n",
    "    \n",
    "    \n",
    "2. Toysmith, $R = [57, 33, 29, 45, 246]$\n",
    "    $$\\theta_{MLE} = [0.139, 0.080, 0.071, 0.110, 0.6]$$\n",
    "    \n",
    "The posterior mean estimates are almost the same and quite similar to MAP estimates, where comparing to MLE, gives way more weightage on 1 star rating and 5 star ratings for both retail stores, and is quite close to its prior belief. It make the estimation of $\\theta_5$ slightly lower but $\\theta_1$ a lot more higher than MLE estimation. It also makes the estimation of $\\theta_{2..4}$ a lot lower than MLE estimation.\n",
    "    \n",
    "In the prior, we assume most reviews from customers will be extreme ratings with little middle ground. Thus, the result given by MAP makes sense. However, the distribution then will look less similar to what we observe in real world (mostly five/four star ratings). Besides, there is no obvious superior store if we compare between two mean posterior estimators, both of them have $\\theta_5 \\approx 0.5$ and $\\theta_1 \\approx 0.4$;\n",
    "while based on MLE estimators, we still could see that Lotus world has higher $\\theta$ estimations for 4 and 5 star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus mean posterior estimate is [0.46619771 0.00322459 0.00508383 0.01339342 0.51210045]\n",
      "Toysmith mean posterior estimate is [0.43801539 0.01410089 0.01244081 0.01907461 0.51636829]\n"
     ]
    }
   ],
   "source": [
    "# emprically\n",
    "\n",
    "samples = 100000\n",
    "\n",
    "# lotus posterior samples\n",
    "lotus = np.random.dirichlet([1010, 7, 11, 29, 1109], size=samples)\n",
    "# toysmith posterior samples\n",
    "toysmith = np.random.dirichlet([1057, 34, 30, 46, 1246], size=samples)\n",
    "\n",
    "# MAP\n",
    "lotus_mean = lotus.mean(axis = 0)\n",
    "toysmith_mean = toysmith.mean(axis = 0)\n",
    "\n",
    "print(\"Lotus mean posterior estimate is\", lotus_mean)\n",
    "print(\"Toysmith mean posterior estimate is\", toysmith_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **(The Posterior Predictive Estimate)** Sample 1000 rating vectors from the posterior predictive for each product, using the $\\alpha$'s you chose in Problem 1. Use the average of the posterior predictive samples to estimate $\\theta$. How do these estimates compare with the MAP, MLE, posterior mean estimate of $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior predictive sample average of theta estimate for Lotus is  [0.46415432 0.0032716  0.00526543 0.01346914 0.51383951]\n",
      "posterior predictive sample average of theta estimate for ToySmith is  [0.43697805 0.01377561 0.01220976 0.01916341 0.51787317]\n"
     ]
    }
   ],
   "source": [
    "samples = 1000\n",
    "\n",
    "# lotus posterior samples\n",
    "lotus = np.random.dirichlet([1010, 7, 11, 29, 1109], size=samples)\n",
    "\n",
    "# toysmith posterior samples\n",
    "toysmith = np.random.dirichlet([1057, 34, 30, 46, 1246], size=samples)\n",
    "\n",
    "# posterior predictive\n",
    "lotus_pp = np.array([np.random.multinomial(162, posterior) for posterior in lotus])\n",
    "toysmith_pp = np.array([np.random.multinomial(410, posterior) for posterior in toysmith])\n",
    "\n",
    "# pp average \n",
    "theta_lotus_pp = lotus_pp.mean(axis = 0)/ lotus_pp.mean(axis = 0).sum()\n",
    "theta_toysmith_pp = toysmith_pp.mean(axis = 0)/toysmith_pp.mean(axis = 0).sum()\n",
    "\n",
    "print (\"posterior predictive sample average of theta estimate for Lotus is \", theta_lotus_pp)\n",
    "print (\"posterior predictive sample average of theta estimate for ToySmith is \", theta_toysmith_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "The posterior predictive sample average for estimating theta is quite close to MAP and posterior mean estimate of theta, as it depends on prior belief we feed into the empirical samples when we generating posterior samples. It incorporates our prior belief, thus it makes sense to see the $\\theta$ estimates are quite heavily centered around rating 1 stars and rating 5 stars, with little possiblity on 2-4 stars. It looks quite different from our MLE estimate, just as MAP and mean posterior estimate do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **(Model Evaluation)** Compute the 95% credible interval of $\\theta$ for each product (*Hint: compute the 95% credible interval for each $\\theta_i$, $i=1, \\ldots, 5$*). For which product is the posterior mean and MAP estimate more reliable and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus 95% credible interval of theta is from  [0.44533986 0.00130801 0.00252421 0.00898588 0.49089776]  to  [0.48730559 0.00601042 0.00847079 0.01864724 0.53299793] \n",
      "\n",
      "ToySmith 95% credible interval of theta is from  [0.41808619 0.00978988 0.00841818 0.01400778 0.49635283]  to  [0.45789355 0.01917679 0.01721684 0.0248804  0.5364186 ]\n"
     ]
    }
   ],
   "source": [
    "samples = 100000\n",
    "\n",
    "# lotus posterior samples\n",
    "lotus = np.random.dirichlet([1010, 7, 11, 29, 1109], size=samples)\n",
    "\n",
    "# toysmith posterior samples\n",
    "toysmith = np.random.dirichlet([1057, 34, 30, 46, 1246], size=samples)\n",
    "\n",
    "#############Lotus#################\n",
    "# Compute the 97.5 th percentile of the posterior \n",
    "pp_upper_lotus = np.percentile(lotus, 97.5, axis=0)\n",
    "\n",
    "# Compute the 2.5 th percentile of the posterior \n",
    "pp_lower_lotus = np.percentile(lotus, 2.5, axis=0)\n",
    "\n",
    "# Compute the 50 th percentile of the posterior\n",
    "# pp_mean_lotus = np.mean(lotus, axis=0)\n",
    "\n",
    "#############Toysmith#################\n",
    "# Compute the 97.5 th percentile of the posterior \n",
    "pp_upper_toysmith = np.percentile(toysmith, 97.5, axis=0)\n",
    "\n",
    "# Compute the 2.5 th percentile of the posterior \n",
    "pp_lower_toysmith = np.percentile(toysmith, 2.5, axis=0)\n",
    "\n",
    "# Compute the 50 th percentile of the posterior\n",
    "# pp_mean_toysmith = np.mean(toysmith, axis=0)\n",
    "\n",
    "print(\"Lotus 95% credible interval of theta is from \", pp_lower_lotus, \" to \", pp_upper_lotus,'\\n')\n",
    "print(\"ToySmith 95% credible interval of theta is from \", pp_lower_toysmith, \" to \", pp_upper_toysmith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width of 95% CI for Lotus World is  [0.04196574 0.0047024  0.00594659 0.00966136 0.04210017]\n",
      "Width of 95% CI for ToySmith is  [0.03980737 0.00938692 0.00879866 0.01087262 0.04006577]\n"
     ]
    }
   ],
   "source": [
    "width_lotus = pp_upper_lotus - pp_lower_lotus\n",
    "width_toysmith = pp_upper_toysmith - pp_lower_toysmith\n",
    "print (\"Width of 95% CI for Lotus World is \", width_lotus)\n",
    "print (\"Width of 95% CI for ToySmith is \", width_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "The 95% interval and width at each $\\theta$ looks close for both product retail store. Looking at the width of CI interval, both product also looks close to each other. It is indecisive to say one product is superior to the other or more certain with its estimates. \n",
    "    \n",
    "Regarding MAP and posterior mean estimates, none of them is quite reliable for both product, as they are both doing point estimation, given the similar confidence interval for both product, it would be better to keep posterior distribution instead of giving a point estimation. It could be misleading to use point estimate, or even use them as a criteria to determine which retail store is superior to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III: Broader Impact Analysis\n",
    "\n",
    "Starting in 2020, major machine learning conferences are beginning to ask authors as well as reviewers to explicitly consider the broader impact of new machine learning methods. To properly evaluate the potential good or harm that a piece of technology (AI or not) can do to the general public, we need to be aware that no technology is deployed in ideal conditions or in perfectly neutral contexts. In order to assess the potential broader impact of technology, we need to analyze the social systems/institutions of which these technologies will become a part.\n",
    "\n",
    "To help you analyze the broader impact of your technology, begin by considering the following questions:\n",
    "\n",
    "I. Identify the relevant socio-technical systems\n",
    "  - In what social, political, economic system could the tech be deployed?\n",
    "  - How would the tech be used in these systems (what role will it take in the decision making processes)?<br><br>\n",
    "  \n",
    "II. Identify the stakeholders\n",
    "  - Who are the users?\n",
    "  - Who are the affected communities (are these the users)?\n",
    "  \n",
    "    ***Hint:*** users are typically decision makers who will use the technology as decision aids (e.g. doctors), whereas affected communities may be folks who are impacted by these decisions but who are not represented in the decision making process (e.g. patients).<br><br>\n",
    "    \n",
    "III. What types of harm can this tech do?\n",
    "  - What kinds of failures can this tech have?\n",
    "  - What kinds of direct harm can these failures cause?\n",
    "  - What kinds of harm can the socio-technical system cause?\n",
    "  \n",
    "    ***Hint:*** many technical innovations have niche applications, they may sit in a long chain of decision making in a complex system. As such, it may seem, at first glance, that these technologies have no immediate real-life impact. In these cases, it’s helpful to think about the impact of the entire system and then think about how the proposed innovations aid, hamper or change the goals or outcomes of this system.<br><br>\n",
    "    \n",
    "IV. What types of good can this tech do?\n",
    "  - What kinds of needs do these users/communities have?\n",
    "  - What kinds of constraints do these users/communities have?\n",
    "\n",
    "\n",
    "1. **(Impact)** Analyze the broader impact of models of product popularity/quality when used for ranking. Specifically, focus on anticipating ways these models can interact with other components of the decision systems in which they will be deployed, identifying end-users, affected communities as well as anticipating the effects (positive and negative) on affected communities (in particular, does the model have the same effect on all subpopulations in the affected communities?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "* *End User*: E-Commerce Websites, Third Party Platforms\n",
    "* *Affected Communities*: Sellers (companies who list products) and Buyers (customers who purchase and browse in the website)\n",
    "    \n",
    "In this case, there will be huge social and commercial broad impact of models of product popularity and quality when used for ranking. Search and ranking, are two key algorithms and models that drive the traffic for both its buyers and sellers on the platform. Ranking in an ecommerce website could be crucial to sellers' business, it could result in fierce SEO competition and optimization that possibly be done in order to raise their store's ranking to gain more traffics, page views, daily active users and potential deep converted buyers. In other words, a rank model use product popularity and quality place these two features into key position for seller to gain its traffic, and ultimately acheive success monetization, business value, and sustainable development. \n",
    "\n",
    "Thus, Model developers also need to be part of this conversation: to help anticipate and assess the social impact of its applications, and promote initiatives to steer research and society in beneficial directions.\n",
    "\n",
    "The end user of this model will be E-Commerce Websites, Third Party Platforms; and affected communities will be sellers and buyers in that website. There is clearly imbalance of power structure between end user and affected community. In real life, end users will use this model to rank products, display listings, inform their decision making, while affected community could only passively accept end users' rankings. Buyers and sellers who are impacted by these decisions may not be represented in the decision making process. \n",
    "\n",
    "The model deployed may not have the same effect on all subpopulations in the affected communities of sellers. This definitely rewards those old, reputatble sellers with high quality and high number of comments. However, it could also negatively impact new joiners who may not have any comments either in good quality or low quality. For those subgroup of seller users, they may encounter 'cold start' problem given the sparsity of their data. This could potentially encourage fraud behaviours to purchase spamming comments, or create fake transacations to trick the system in order to get higher ranking, which may also create extra burden for the company to defraud and detect spamming content. \n",
    "    \n",
    "In addition, it may even lead to a scenario where those old reputable stores always ranked at top when some keywords are being searched by users, and thus drive all user traffic into their stores, resulting in a vicious cycle that may eventually result in monopoly or oligopoly market. It would make the e-commerce platform become a dead ocean where new entrants have high barriers to enter, which may be a negative ecosystem ultimately.  \n",
    "    \n",
    "On the other side, users could also be misled by rankings if the models of product popularity and quality is not fair. If there exists too many fraudlent comments in order to raise sellers' ranking and they are not cleaned up, users would have a hard time judge it and be misled by ranking result as well. They may eventually purchased something that are not so ideal. \n",
    "    \n",
    "Therefore, this model could be deployed, but it should be deployed with careful monitoring and a more considerable weightage scores designed to place on popularity/quality features that are used for ranking. Other features should also be considered and bring into decision making. \n",
    "    \n",
    "The goal of end user is certainly sort products and make a ranking that is more relevant and helpful to users, while the goal of sellers are to sell more products, and the goal of buyers are to purchase ideal products.  The relationship between end-users and affected communities is thus important to think about because the interests/goals of end users and the affected community may not always align. And when they do not, as a developer of algorithm and model, we need to be careful about deploying model, and has to make a decision about to whose interest are developers obligated. In this case, we need to take care of the entire e-commerce system while at the same time bring more benefits to platform, buyers and sellers as a whole, so that it would be a win-win strategy. Technology is no longer value neutral but somehow value laden in this case. End user should handle the output with care and potentially improve the model with more relevant features to create a healthier competition within the ecosystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Comparison of Ranking Systems)** \n",
    "\n",
    "  Recall that on Amazon, the first customer review statistic displayed for a product is the average rating. What is potentially problemmatic with ranking products based on the average customer rating?\n",
    "\n",
    "  Which point estimate (MAP, MLE, posterior mean or posterior predictive estimate) of $\\theta$, if any, would you feel choose to rank the two Amazon products? Why? \n",
    "\n",
    "  *Hint: think about which of these estimates are equivalent (if any). If they are not equivalent, what are the special properties of each estimate? What aspect of the data or the model is each estimate good at capturing?*\n",
    "  \n",
    "   **Note:** we're not looking for \"the correct answer\" here. We are looking for a sound decision based on a statistically correct interpretation of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00007B'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "Using average customer rating as review statistics displayed on products could be problemmatic due to the following factors: \n",
    "1. When data is sparse and there's not so many reviews, it could be severely biased towards some early ratings. The variance could also be really high when data size is small, for example a high rating could effectively raise your ranking. \n",
    "2. For new entrants into the business, they may encounter cold start problem with sparse data, or zero reviews, so that their product will also be ranked at bottom. It would be unhealthy and too high entrance barrier for new joiners into the ecosystem. Maybe some booster factor should be take into consideration to deal with those cold start problem. \n",
    "3. There could be possibly fraud transactions in order to trick the system in order to raise the ratings.\n",
    "4. Competitors could also give a large number of low ratings to affect your ratings.\n",
    "5. People may tend to leave comments only if they have strong opinions, for example, when the product is very good, or very bad. If this prior belief is relatively true, then with large number of reviews, all reviews will eventually converge to a similar distribution with high weightage on 1 stars and 5 stars as what we see in problem 2.\n",
    "    \n",
    "I would feel better to choose MLE estimation to rank two Amazon products. It tends to be more close to real data but not bring in any prior information. \n",
    "    \n",
    "MAP and posterior mean point estimate are essentially MLE estimate with regularization. And MAP and posterior mean converge to MLE when sample size gets large. The difference is that they could incorporate a prior belief into model. As what we experiment before, the setting of priors affect the point estimates in posterior prediction a lot. If we hold strong prior towards certain ratings, it will vary MAP and posterior mean estimate a lot. However, for ranking of two products, I am not feeling confident to bring strong prior belief into modeling. When we set prior to carry no information, i.e. uniform alpha in Dirichelet distribution example, the two point estimates are closer to MLE. \n",
    "    \n",
    "MAP capture the mode and majority population information, while posterior mean captures average mean information in population. Besides, as what we just discussed, regarding MAP and posterior mean estimates, none of them is quite reliable for both products, as they are both doing point estimation, it would be better to keep posterior distribution instead of giving a point estimation. It could be misleading to use point estimate, or even use them as a criteria to determine which retail store is superior to the other. Thus, to sum up, I would feel more comfortable to choose to use MLE estimates in this case for ranking two products. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
